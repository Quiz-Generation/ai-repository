"""
ğŸ“ Text Helper
"""
from typing import List, Dict, Any, Optional
from datetime import datetime
from ..models.document_model import DocumentChunk
from ..core.config import settings


class TextHelper:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""

    def __init__(self):
        self.chunk_size = settings.CHUNK_SIZE
        self.chunk_overlap = settings.CHUNK_OVERLAP

    def split_text_into_chunks(
        self,
        text: str,
        document_id: str = "",
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None
    ) -> List[DocumentChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # ë§¤ê°œë³€ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì •
        chunk_size = chunk_size or self.chunk_size
        chunk_overlap = chunk_overlap or self.chunk_overlap

        chunks = []

        # ê°„ë‹¨í•œ ì²­í‚¹ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„í•  í•„ìš”)
        text_length = len(text)
        start_index = 0
        chunk_index = 0

        while start_index < text_length:
            end_index = min(start_index + chunk_size, text_length)

            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end_index < text_length:
                # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ì°¾ê¸°
                last_sentence_end = text.rfind('.', start_index, end_index)
                last_newline = text.rfind('\n', start_index, end_index)

                boundary = max(last_sentence_end, last_newline)
                if boundary > start_index:
                    end_index = boundary + 1

            chunk_content = text[start_index:end_index].strip()

            if chunk_content:
                chunk = DocumentChunk(
                    id="",  # __post_init__ì—ì„œ ìë™ ìƒì„±
                    document_id=document_id,
                    chunk_index=chunk_index,
                    content=chunk_content,
                    start_index=start_index,
                    end_index=end_index,
                    metadata={
                        "length": len(chunk_content),
                        "has_overlap": chunk_index > 0
                    },
                    created_at=datetime.now()
                )
                chunks.append(chunk)
                chunk_index += 1

            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  ê³„ì‚° (ì˜¤ë²„ë© ê³ ë ¤)
            start_index = max(0, end_index - chunk_overlap)

            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start_index >= end_index:
                break

        return chunks

    def split_text_simple(
        self,
        text: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœ ë¬¸ìì—´ ì²­í¬ë¡œ ë¶„í•  (DocumentChunk ì—†ì´)"""
        chunks = []
        text_length = len(text)
        start_index = 0

        while start_index < text_length:
            end_index = min(start_index + chunk_size, text_length)

            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end_index < text_length:
                last_sentence_end = text.rfind('.', start_index, end_index)
                last_newline = text.rfind('\n', start_index, end_index)
                boundary = max(last_sentence_end, last_newline)
                if boundary > start_index:
                    end_index = boundary + 1

            chunk_content = text[start_index:end_index].strip()
            if chunk_content:
                chunks.append(chunk_content)

            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  ê³„ì‚°
            start_index = max(0, end_index - chunk_overlap)
            if start_index >= end_index:
                break

        return chunks

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = ' '.join(text.split())

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•„ìš”ì‹œ)
        # text = re.sub(r'[^\w\sê°€-í£]', ' ', text)

        return text.strip()

    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # TODO: ì‹¤ì œ ì„ë² ë”© ìƒì„± êµ¬í˜„
        # from sentence_transformers import SentenceTransformer
        # model = SentenceTransformer(settings.EMBEDDING_MODEL)
        # return model.encode(texts).tolist()

        # ì„ì‹œë¡œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
        return [[0.1] * 384 for _ in texts]