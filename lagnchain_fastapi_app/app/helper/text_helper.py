"""
ğŸ“ Text Helper
"""
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from ..models.document_model import DocumentChunk
from ..core.config import settings

logger = logging.getLogger(__name__)


class TextHelper:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""

    def __init__(self):
        self.chunk_size = settings.CHUNK_SIZE
        self.chunk_overlap = settings.CHUNK_OVERLAP

    def split_text_into_chunks(
        self,
        text: str,
        document_id: str = "",
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None
    ) -> List[DocumentChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        # ë§¤ê°œë³€ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì •
        chunk_size = chunk_size or self.chunk_size
        chunk_overlap = chunk_overlap or self.chunk_overlap

        chunks = []

        # ê°„ë‹¨í•œ ì²­í‚¹ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„í•  í•„ìš”)
        text_length = len(text)
        start_index = 0
        chunk_index = 0

        while start_index < text_length:
            end_index = min(start_index + chunk_size, text_length)

            # ì˜¤ë²„ë©ì„ ìœ„í•´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end_index < text_length:
                # ë§ˆì§€ë§‰ ë§ˆì¹¨í‘œë‚˜ ì¤„ë°”ê¿ˆ ì°¾ê¸°
                last_sentence_end = text.rfind('.', start_index, end_index)
                last_newline = text.rfind('\n', start_index, end_index)

                boundary = max(last_sentence_end, last_newline)
                if boundary > start_index:
                    end_index = boundary + 1

            chunk_content = text[start_index:end_index].strip()

            if chunk_content:
                chunk = DocumentChunk(
                    id="",  # __post_init__ì—ì„œ ìë™ ìƒì„±
                    document_id=document_id,
                    chunk_index=chunk_index,
                    content=chunk_content,
                    start_index=start_index,
                    end_index=end_index,
                    metadata={
                        "length": len(chunk_content),
                        "has_overlap": chunk_index > 0
                    },
                    created_at=datetime.now()
                )
                chunks.append(chunk)
                chunk_index += 1

            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  ê³„ì‚° (ì˜¤ë²„ë© ê³ ë ¤)
            start_index = max(0, end_index - chunk_overlap)

            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if start_index >= end_index:
                break

        return chunks

    def split_text_simple(
        self,
        text: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœ ë¬¸ìì—´ ì²­í¬ë¡œ ë¶„í•  (DocumentChunk ì—†ì´)"""
        try:
            logger.info(f"STEP5a í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘: í…ìŠ¤íŠ¸ ê¸¸ì´={len(text)}, ì²­í¬í¬ê¸°={chunk_size}, ì˜¤ë²„ë©={chunk_overlap}")

            if not text or not text.strip():
                logger.warning("WARNING ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
                return []

            chunks = []
            text_length = len(text)
            start_index = 0
            loop_count = 0  # ë¬´í•œë£¨í”„ ë°©ì§€ìš© ì¹´ìš´í„°

            while start_index < text_length and loop_count < 1000:  # ìµœëŒ€ 1000ë²ˆ ë°˜ë³µ ì œí•œ
                loop_count += 1
                end_index = min(start_index + chunk_size, text_length)

                # ì˜¤ë²„ë©ì„ ìœ„í•´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
                if end_index < text_length:
                    last_sentence_end = text.rfind('.', start_index, end_index)
                    last_newline = text.rfind('\n', start_index, end_index)
                    boundary = max(last_sentence_end, last_newline)
                    if boundary > start_index:
                        end_index = boundary + 1

                chunk_content = text[start_index:end_index].strip()
                if chunk_content:
                    chunks.append(chunk_content)
                    logger.debug(f"STEP5a ì²­í¬ {len(chunks)}: ì‹œì‘={start_index}, ë={end_index}, ê¸¸ì´={len(chunk_content)}")

                # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  ê³„ì‚°
                next_start = max(start_index + 1, end_index - chunk_overlap)  # ìµœì†Œ 1ê¸€ìì”© ì§„í–‰

                if next_start >= text_length:  # í…ìŠ¤íŠ¸ ëì— ë„ë‹¬
                    break

                if next_start <= start_index:  # ì§„í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°
                    logger.warning(f"WARNING ì²­í‚¹ì—ì„œ ì§„í–‰ë˜ì§€ ì•ŠìŒ: start_index={start_index}, next_start={next_start}")
                    break

                start_index = next_start

            if loop_count >= 1000:
                logger.warning(f"WARNING ì²­í‚¹ì´ 1000ê°œ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")

            logger.info(f"STEP5b í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks

        except Exception as e:
            logger.error(f"ERROR í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return []

    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = ' '.join(text.split())

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•„ìš”ì‹œ)
        # text = re.sub(r'[^\w\sê°€-í£]', ' ', text)

        return text.strip()

    @staticmethod
    def create_text_chunks(
        text: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List[str]:
        """
        ğŸ“ ì •ì  ë©”ì„œë“œ: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë²¡í„° DBìš©)
        - sentence-transformersì™€ ë²¡í„° DBì—ì„œ ì‚¬ìš©
        """
        try:
            logger.info(f"STEP_CHUNK í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘: ê¸¸ì´={len(text)}, ì²­í¬í¬ê¸°={chunk_size}, ì˜¤ë²„ë©={chunk_overlap}")

            if not text or not text.strip():
                logger.warning("WARNING ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
                return []

            chunks = []
            text_length = len(text)
            start_index = 0
            loop_count = 0  # ë¬´í•œë£¨í”„ ë°©ì§€ìš© ì¹´ìš´í„°

            while start_index < text_length and loop_count < 1000:  # ìµœëŒ€ 1000ë²ˆ ë°˜ë³µ ì œí•œ
                loop_count += 1
                end_index = min(start_index + chunk_size, text_length)

                # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„ (ë” ë‚˜ì€ ì²­í‚¹ì„ ìœ„í•´)
                if end_index < text_length:
                    # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ, ê³µë°± ìˆœì„œë¡œ ê²½ê³„ ì°¾ê¸°
                    boundaries = [
                        text.rfind('.', start_index, end_index),
                        text.rfind('\n', start_index, end_index),
                        text.rfind(' ', start_index, end_index)
                    ]

                    best_boundary = max([b for b in boundaries if b > start_index], default=-1)
                    if best_boundary > start_index:
                        end_index = best_boundary + 1

                chunk_content = text[start_index:end_index].strip()
                if chunk_content:
                    chunks.append(chunk_content)
                    logger.debug(f"STEP_CHUNK ì²­í¬ {len(chunks)}: ì‹œì‘={start_index}, ë={end_index}, ê¸¸ì´={len(chunk_content)}")

                # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  ê³„ì‚° (ì˜¤ë²„ë© ê³ ë ¤)
                next_start = max(start_index + 1, end_index - chunk_overlap)

                if next_start >= text_length or next_start <= start_index:
                    break

                start_index = next_start

            if loop_count >= 1000:
                logger.warning(f"WARNING ì²­í‚¹ì´ 1000ê°œ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨")

            logger.info(f"SUCCESS í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks

        except Exception as e:
            logger.error(f"ERROR í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            return [text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        # TODO: ì‹¤ì œ ì„ë² ë”© ìƒì„± êµ¬í˜„
        # from sentence_transformers import SentenceTransformer
        # model = SentenceTransformer(settings.EMBEDDING_MODEL)
        # return model.encode(texts).tolist()

        # ì„ì‹œë¡œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
        return [[0.1] * 384 for _ in texts]