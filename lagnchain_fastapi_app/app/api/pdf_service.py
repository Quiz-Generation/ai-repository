"""
PDF ì—…ë¡œë“œ â†’ ë²¡í„° DB ì €ì¥ API ë¼ìš°í„°

í•µì‹¬ ê¸°ëŠ¥:
- POST /pdf/upload: PDF íŒŒì¼ ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥ â†’ document_id ë°˜í™˜
- GET /pdf/documents: ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
- GET /pdf/documents/{document_id}: íŠ¹ì • ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
- GET /pdf/search: ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰
- GET /pdf/search/{document_id}: íŠ¹ì • ë¬¸ì„œì—ì„œ ê²€ìƒ‰
- GET /pdf/health: ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Query, Path
from fastapi.responses import JSONResponse
import logging
import tempfile
import os
import time
from datetime import datetime

# PDF ì¶”ì¶œìš©
try:
    import fitz  # PyMuPDF
    HAS_PYMUPDF = True
except ImportError:
    HAS_PYMUPDF = False

# ë²¡í„° ì„œë¹„ìŠ¤ import (ìƒëŒ€ ê²½ë¡œë¡œ ë³€ê²½)
from ..services.vector_service import get_global_vector_service

# ğŸ”¥ ë™ì  PDF ì¶”ì¶œ ì‹œìŠ¤í…œ import ì¶”ê°€
from ..services.dynamic_pdf import DynamicPDFService
from ..schemas.dynamic_pdf import Priority

# Swagger ë¬¸ì„œ ì„¤ëª… import
from ..docs.pdf_service import (
    desc_upload_pdf,
    desc_get_documents,
    desc_get_document_info,
    desc_search_all_documents,
    desc_search_in_document,
    desc_health_check,
    desc_switch_database,
    desc_get_stats
)

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/pdf", tags=["PDF Vector"])

# ì „ì—­ ë²¡í„° ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (WEAVIATE ê¸°ë³¸ ì‚¬ìš©) - ì‹±ê¸€í†¤ ì‚¬ìš©
vector_service = get_global_vector_service()

# ğŸ”¥ ë™ì  PDF ì¶”ì¶œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dynamic_pdf_service = DynamicPDFService()


@router.get("/health", description=desc_health_check)
async def health_check() -> JSONResponse:
    """ë²¡í„° DB ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        stats = vector_service.get_stats()
        return JSONResponse(
            status_code=200,
            content={
                "status": "healthy",
                "service": "PDF Vector Service (ë™ì  ì¶”ì¶œê¸° ì§€ì›)",
                "vector_db": stats["db_type"],
                "total_documents": stats["total_documents"],
                "total_uploaded_files": stats["total_uploaded_files"],
                "supported_dbs": stats["supported_dbs"],
                # ğŸ”¥ ë™ì  ì¶”ì¶œê¸° ì •ë³´ ì¶”ê°€
                "extraction_system": {
                    "type": "smart_auto_optimization",
                    "available_extractors": ["pdfminer", "pdfplumber", "pymupdf"],
                    "default_mode": "auto",
                    "auto_optimization": True,
                    "manual_priorities": ["speed", "quality", "balanced"],
                    "smart_features": [
                        "íŒŒì¼ í¬ê¸° ê¸°ë°˜ ìë™ ìš°ì„ ìˆœìœ„ ê²°ì •",
                        "ë‚´ìš© ìœ í˜• ê¸°ë°˜ ì¶”ì¶œê¸° ì„ íƒ",
                        "íŒŒì¼ëª… ê¸°ë°˜ fallback ë¶„ì„",
                        "ë‹¤ì¤‘ í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"
                    ]
                },
                "endpoints": [
                    "POST /pdf/upload (ìŠ¤ë§ˆíŠ¸ ìë™ ìµœì í™”)",
                    "POST /pdf/analyze (ì¶”ì¶œê¸° ì¶”ì²œ ë¶„ì„)",
                    "GET /pdf/documents",
                    "GET /pdf/documents/{document_id}",
                    "GET /pdf/search",
                    "GET /pdf/search/{document_id}",
                    "GET /pdf/health",
                    "POST /pdf/switch-db"
                ]
            }
        )
    except Exception as e:
        return JSONResponse(
            status_code=503,
            content={
                "status": "unhealthy",
                "error": str(e)
            }
        )


@router.post("/upload", description=desc_upload_pdf)
async def upload_pdf(
    file: UploadFile = File(...),
    priority: str = Query("auto", description="ì¶”ì¶œ ìš°ì„ ìˆœìœ„: auto(ìë™ìµœì í™”), speed, quality, balanced")
) -> JSONResponse:
    """ğŸ“¤ PDF íŒŒì¼ ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥ â†’ document_id ë°˜í™˜ (ìŠ¤ë§ˆíŠ¸ ìë™ ìµœì í™”)"""

    upload_start_time = time.time()

    if not file.filename or not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤")

    filename = file.filename
    logger.info(f"PDF ì—…ë¡œë“œ ì‹œì‘: {filename} (ìš°ì„ ìˆœìœ„: {priority})")

    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name

        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")

        # ğŸ§  ìŠ¤ë§ˆíŠ¸ ìš°ì„ ìˆœìœ„ ê²°ì •
        if priority == "auto":
            logger.info("ìë™ ìµœì í™” ëª¨ë“œ: íŒŒì¼ ë¶„ì„ ì¤‘...")
            recommendations = dynamic_pdf_service.get_extractor_recommendations(temp_path)

            # íŒŒì¼ í¬ê¸°ì™€ ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ìµœì  ìš°ì„ ìˆœìœ„ ìë™ ê²°ì •
            content_type = recommendations["file_info"]["content_type"]
            size_mb = recommendations["file_info"]["size_mb"]

            if size_mb > 10:
                optimal_priority = "speed"
                reason = f"ëŒ€ìš©ëŸ‰ íŒŒì¼({size_mb:.1f}MB) â†’ ì†ë„ ìš°ì„ "
            elif content_type in ["korean", "mixed"]:
                optimal_priority = "quality"
                reason = f"í•œê¸€ ë¬¸ì„œ â†’ í’ˆì§ˆ ìš°ì„ "
            elif size_mb > 5:
                optimal_priority = "speed"
                reason = f"ì¤‘ëŒ€ìš©ëŸ‰ íŒŒì¼({size_mb:.1f}MB) â†’ ì†ë„ ìš°ì„ "
            else:
                optimal_priority = "balanced"
                reason = f"ì†Œìš©ëŸ‰ íŒŒì¼({size_mb:.1f}MB) â†’ ê· í˜• ëª¨ë“œ"

            logger.info(f"ìë™ ê²°ì •: {optimal_priority} ({reason})")
            extraction_priority = Priority(optimal_priority)
            auto_selected = True
        else:
            # ì‚¬ìš©ì ì§€ì • ìš°ì„ ìˆœìœ„ ê²€ì¦
            try:
                extraction_priority = Priority(priority.lower())
                auto_selected = False
                reason = f"ì‚¬ìš©ì ì§€ì •: {priority}"
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"ì˜ëª»ëœ ìš°ì„ ìˆœìœ„: {priority}. ì‚¬ìš© ê°€ëŠ¥: auto, speed, quality, balanced"
                )

        try:
            # ë™ì  PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœì í™”ëœ ìš°ì„ ìˆœìœ„ ì‚¬ìš©)
            extraction_result = dynamic_pdf_service.extract_text(temp_path, extraction_priority)

            if not extraction_result.success:
                logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result.error}")
                raise HTTPException(status_code=400, detail=f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {extraction_result.error}")

            pdf_text = extraction_result.text

            if len(pdf_text.strip()) < 100:
                logger.warning(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•¨: {len(pdf_text)}ì")
                raise HTTPException(status_code=400, detail="PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ë²¡í„° ì €ì¥
            logger.info(f"ë²¡í„° ì €ì¥ ì‹œì‘...")
            vector_start = time.time()
            result = vector_service.process_pdf_text(pdf_text, filename)
            vector_time = time.time() - vector_start

            # ë²¡í„° ì €ì¥ ê²°ê³¼ ê²€ì¦ (ìƒˆë¡œìš´ í˜•ì‹)
            if not result.get("document_id"):
                logger.error(f"ë²¡í„° ì €ì¥ ì‹¤íŒ¨: document_idê°€ ì—†ìŒ")
                raise HTTPException(status_code=500, detail="ë²¡í„° ì €ì¥ ì‹¤íŒ¨: ë¬¸ì„œ ID ìƒì„± ì˜¤ë¥˜")

            total_time = time.time() - upload_start_time
            logger.info(f"ì—…ë¡œë“œ ì™„ë£Œ: ì´ {total_time:.2f}ì´ˆ (ì¶”ì¶œ: {extraction_result.extraction_time:.2f}ì´ˆ, ë²¡í„°í™”: {vector_time:.2f}ì´ˆ)")

            return JSONResponse(
                status_code=200,
                content={
                    "message": "PDF ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥ ì„±ê³µ (ìŠ¤ë§ˆíŠ¸ ìë™ ìµœì í™”)",
                    "document_id": result["document_id"],  # ğŸ”‘ RAGìš© ë¬¸ì„œ ID
                    "filename": filename,
                    "file_size": len(content),
                    "text_length": len(pdf_text),
                    "total_chunks": result["total_chunks"],
                    "stored_chunks": result["stored_chunks"],
                    "db_type": vector_service.db_type,
                    "upload_timestamp": datetime.now().isoformat(),
                    # ğŸ§  ìŠ¤ë§ˆíŠ¸ ìµœì í™” ì •ë³´
                    "optimization_info": {
                        "priority_mode": "auto" if auto_selected else "manual",
                        "selected_priority": extraction_priority.value,
                        "selection_reason": reason,
                        "extractor_used": extraction_result.extractor_used,
                        "content_type": extraction_result.content_type,
                        "auto_optimized": auto_selected
                    },
                    # ë™ì  ì¶”ì¶œ ì •ë³´
                    "extraction_info": {
                        "extractor_used": extraction_result.extractor_used,
                        "content_type": extraction_result.content_type,
                        "priority": extraction_result.priority,
                        "extraction_time": extraction_result.extraction_time,
                        "speed_mbps": extraction_result.speed_mbps,
                        "auto_selected": extraction_result.metadata.get("auto_selected", True),
                        "selection_reason": extraction_result.metadata.get("selection_reason", "")
                    },
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                    "performance": {
                        "total_time": round(total_time, 3),
                        "extraction_time": extraction_result.extraction_time,
                        "vectorization_time": round(vector_time, 3),
                        "extraction_speed_mbps": extraction_result.speed_mbps
                    },
                    "note": "ğŸ§  ìŠ¤ë§ˆíŠ¸ ìë™ ìµœì í™”ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. document_idë¥¼ ì €ì¥í•˜ì—¬ RAG í€´ì¦ˆ ìƒì„±ì— ì‚¬ìš©í•˜ì„¸ìš”."
                }
            )

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    except HTTPException:
        raise
    except Exception as e:
        error_time = time.time() - upload_start_time
        logger.error(f"PDF ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)} ({error_time:.2f}ì´ˆ)")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@router.get("/documents", description=desc_get_documents)
async def get_document_list() -> JSONResponse:
    """ğŸ“‹ ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (RAGìš©)"""
    try:
        documents = vector_service.get_document_list()

        # RAGìš© ì •ë³´ ì¶”ê°€
        for doc in documents:
            doc["available_for_rag"] = True
            doc["recommended_for_quiz"] = doc["chunk_count"] >= 5  # 5ê°œ ì´ìƒ ì²­í¬ë©´ í€´ì¦ˆ ìƒì„± ê¶Œì¥

        return JSONResponse(
            status_code=200,
            content={
                "message": "ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì„±ê³µ",
                "total_documents": len(documents),
                "db_type": vector_service.db_type,
                "documents": documents,
                "note": "document_idë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ë¬¸ì„œë¡œ RAG í€´ì¦ˆë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            }
        )
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")


@router.get("/documents/{document_id}", description=desc_get_document_info)
async def get_document_info(
    document_id: str = Path(..., description="ë¬¸ì„œ ID")
) -> JSONResponse:
    """ğŸ“„ íŠ¹ì • ë¬¸ì„œ ì •ë³´ ì¡°íšŒ (RAGìš© ìƒì„¸ ì •ë³´)"""
    try:
        document_info = vector_service.get_document_info(document_id)

        if not document_info:
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}")

        # RAGìš© ì¶”ê°€ ì •ë³´
        document_info["rag_ready"] = True
        document_info["chunk_size_avg"] = document_info["total_chars"] // document_info["chunk_count"]
        document_info["quiz_generation_score"] = min(10, document_info["chunk_count"] * 2)  # ì ìˆ˜ ê³„ì‚°

        return JSONResponse(
            status_code=200,
            content={
                "message": "ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì„±ê³µ",
                "document": document_info,
                "db_type": vector_service.db_type,
                "rag_info": {
                    "can_generate_quiz": document_info["chunk_count"] >= 3,
                    "recommended_questions": min(10, document_info["chunk_count"] // 2),
                    "content_quality": "high" if document_info["chunk_count"] >= 10 else "medium"
                }
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")


@router.get("/search", description=desc_search_all_documents)
async def search_all_documents(
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Query(5, ge=1, le=20, description="ê²°ê³¼ ê°œìˆ˜")
) -> JSONResponse:
    """ğŸ” ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰"""
    if not query.strip():
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    try:
        results = vector_service.search_documents(query, top_k)

        # ê²°ê³¼ ì •ë¦¬
        formatted_results = []
        for result in results:
            formatted_results.append({
                "doc_id": result["doc_id"],
                "document_id": result["metadata"].get("document_id", ""),
                "source_filename": result["metadata"].get("source", ""),
                "text_preview": result["text"][:200] + "..." if len(result["text"]) > 200 else result["text"],
                "similarity": round(result["similarity"], 4),
                "chunk_index": result["metadata"].get("chunk_index", 0)
            })

        return JSONResponse(
            status_code=200,
            content={
                "message": "ì „ì²´ ê²€ìƒ‰ ì™„ë£Œ",
                "query": query,
                "total_results": len(formatted_results),
                "db_type": vector_service.db_type,
                "results": formatted_results
            }
        )

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


@router.get("/search/{document_id}", description=desc_search_in_document)
async def search_in_document(
    document_id: str = Path(..., description="ë¬¸ì„œ ID"),
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Query(5, ge=1, le=10, description="ê²°ê³¼ ê°œìˆ˜")
) -> JSONResponse:
    """ğŸ¯ íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰ (RAGìš© ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ)"""
    if not query.strip():
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    try:
        # ë¬¸ì„œ ì¡´ì¬ í™•ì¸
        document_info = vector_service.get_document_info(document_id)
        if not document_info:
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}")

        # íŠ¹ì • ë¬¸ì„œì—ì„œ ê²€ìƒ‰
        results = vector_service.search_in_document(query, document_id, top_k)

        # RAGìš© ê²°ê³¼ ì •ë¦¬
        formatted_results = []
        full_context = ""

        for result in results:
            formatted_result = {
                "doc_id": result["doc_id"],
                "text_preview": result["text"][:200] + "..." if len(result["text"]) > 200 else result["text"],
                "full_text": result["text"],  # RAG ì»¨í…ìŠ¤íŠ¸ìš©
                "similarity": round(result["similarity"], 4),
                "chunk_index": result["metadata"].get("chunk_index", 0)
            }
            formatted_results.append(formatted_result)
            full_context += result["text"] + "\n\n"

        return JSONResponse(
            status_code=200,
            content={
                "message": f"ë¬¸ì„œ ë‚´ ê²€ìƒ‰ ì™„ë£Œ",
                "document_id": document_id,
                "document_filename": document_info["source_filename"],
                "query": query,
                "total_results": len(formatted_results),
                "db_type": vector_service.db_type,
                "results": formatted_results,
                "rag_context": {
                    "combined_text": full_context.strip(),
                    "context_length": len(full_context),
                    "ready_for_rag": len(full_context) > 100
                }
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ë‚´ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ë‚´ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


@router.post("/switch-db", description=desc_switch_database)
async def switch_database(db_type: str) -> JSONResponse:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½"""
    try:
        success = vector_service.switch_database(db_type)

        if not success:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {db_type}. ì§€ì› íƒ€ì…: {vector_service.get_stats()['supported_dbs']}"
            )

        return JSONResponse(
            status_code=200,
            content={
                "message": f"ë°ì´í„°ë² ì´ìŠ¤ê°€ {db_type}ìœ¼ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤",
                "previous_db": vector_service.db_type,
                "current_db": db_type,
                "total_documents": 0  # ìƒˆ DBì´ë¯€ë¡œ 0
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"DB ë³€ê²½ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"DB ë³€ê²½ ì˜¤ë¥˜: {str(e)}")


@router.get("/stats", description=desc_get_stats)
async def get_stats() -> JSONResponse:
    """ë²¡í„° DB í†µê³„"""
    try:
        stats = vector_service.get_stats()
        return JSONResponse(
            status_code=200,
            content=stats
        )
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")


@router.post("/analyze", description="PDF íŒŒì¼ ë¶„ì„ ë° ì¶”ì¶œê¸° ì¶”ì²œ")
async def analyze_pdf(file: UploadFile = File(...)) -> JSONResponse:
    """ğŸ” PDF íŒŒì¼ ë¶„ì„ ë° ìµœì  ì¶”ì¶œê¸° ì¶”ì²œ (ì—…ë¡œë“œ ì „ ë¯¸ë¦¬ë³´ê¸°)"""

    analyze_start = time.time()

    if not file.filename or not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤")

    filename = file.filename
    logger.info(f"PDF ë¶„ì„ ìš”ì²­: {filename}")

    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name

        file_size_mb = len(content) / (1024 * 1024)
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")

        try:
            # íŒŒì¼ ë¶„ì„ ë° ì¶”ì²œ
            recommendations = dynamic_pdf_service.get_extractor_recommendations(temp_path)

            analyze_time = time.time() - analyze_start
            logger.info(f"ë¶„ì„ ì™„ë£Œ: {analyze_time:.2f}ì´ˆ")

            return JSONResponse(
                status_code=200,
                content={
                    "message": "PDF íŒŒì¼ ë¶„ì„ ì™„ë£Œ",
                    "filename": filename,
                    "file_size": len(content),
                    "analysis_time": round(analyze_time, 3),
                    "analysis": recommendations,
                    "usage_tip": "ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ /pdf/upload APIì—ì„œ priority íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”"
                }
            )

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    except Exception as e:
        error_time = time.time() - analyze_start
        logger.error(f"PDF ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)} ({error_time:.2f}ì´ˆ)")
        raise HTTPException(status_code=500, detail=f"PDF ë¶„ì„ ì˜¤ë¥˜: {str(e)}")