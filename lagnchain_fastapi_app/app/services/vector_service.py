#!/usr/bin/env python3
"""
ğŸ”¥ ì‹¤ì œ ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤
- ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰
- ìë™ ì„ë² ë”© ìƒì„±
- ì˜êµ¬ ì €ì¥
"""
import hashlib
import numpy as np
import json
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pathlib import Path
import uuid
from datetime import datetime
import logging

# ğŸ”¥ ì‹¤ì œ ë²¡í„°DB ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import chromadb
    from chromadb.config import Settings
    HAS_CHROMADB = True
except ImportError:
    HAS_CHROMADB = False

try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False

logger = logging.getLogger(__name__)


class VectorDatabase(ABC):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì €ì¥"""
        pass

    @abstractmethod
    def search_similar(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        pass

    @abstractmethod
    def count_documents(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        pass

    @abstractmethod
    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤(íŒŒì¼)ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        pass

    @abstractmethod
    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        pass

    @abstractmethod
    def save_to_disk(self) -> bool:
        """ë””ìŠ¤í¬ì— ì €ì¥"""
        pass

    @abstractmethod
    def load_from_disk(self) -> bool:
        """ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ"""
        pass


class RealChromaDB(VectorDatabase):
    """ğŸ”¥ ì‹¤ì œ ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤"""

    def __init__(self, data_dir: str = "./vector_data"):
        if not HAS_CHROMADB:
            raise ImportError("ChromaDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install chromadb")

        self.name = "real_chromadb"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì˜êµ¬ ì €ì¥)
        self.client = chromadb.PersistentClient(
            path=str(self.data_dir / "chroma_db"),
            settings=Settings(anonymized_telemetry=False)
        )

        # ì»¬ë ‰ì…˜ ìƒì„±/ê°€ì ¸ì˜¤ê¸°
        self.collection = self.client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
        )

        logger.info(f"ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {self.collection.count()}ê°œ ë¬¸ì„œ ë¡œë“œë¨")

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œë¥¼ ChromaDBì— ì €ì¥"""
        try:
            # ChromaDBëŠ” ìë™ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•˜ë¯€ë¡œ vectorëŠ” ë¬´ì‹œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©
            self.collection.add(
                documents=[text],
                metadatas=[metadata],
                ids=[doc_id]
            )
            return True
        except Exception as e:
            logger.error(f"ChromaDB ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def search_similar(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ì‚¬ìš©)"""
        try:
            # í•„í„° ì„¤ì •
            where_filter = {}
            if document_id:
                where_filter["document_id"] = document_id

            # ChromaDB ê²€ìƒ‰
            results = self.collection.query(
                query_texts=[query],
                n_results=top_k,
                where=where_filter if where_filter else None
            )

            # ê²°ê³¼ ë³€í™˜
            formatted_results = []
            if (results.get("documents") and
                results["documents"] and
                len(results["documents"]) > 0 and
                results["documents"][0]):

                docs = results["documents"][0]
                ids = results.get("ids", [[]])[0] if results.get("ids") else []
                metadatas = results.get("metadatas", [[]])[0] if results.get("metadatas") else []
                distances = results.get("distances", [[]])[0] if results.get("distances") else []

                for i, doc in enumerate(docs):
                    formatted_results.append({
                        "doc_id": ids[i] if i < len(ids) else f"unknown_{i}",
                        "text": doc,
                        "metadata": metadatas[i] if i < len(metadatas) else {},
                        "similarity": 1.0 - distances[i] if i < len(distances) else 0.5
                    })

            return formatted_results

        except Exception as e:
            logger.error(f"ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def count_documents(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        try:
            return self.collection.count()
        except Exception:
            return 0

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        try:
            results = self.collection.get(
                where={"source": source_name}
            )

            docs = []
            documents = results.get("documents", [])
            ids = results.get("ids", [])
            metadatas = results.get("metadatas", [])

            if documents:
                for i, doc in enumerate(documents):
                    docs.append({
                        "doc_id": ids[i] if i < len(ids) else f"unknown_{i}",
                        "text": doc,
                        "metadata": metadatas[i] if i < len(metadatas) else {}
                    })
            return docs
        except Exception as e:
            logger.error(f"ì†ŒìŠ¤ë³„ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
            all_docs = self.collection.get()

            sources = {}
            metadatas = all_docs.get("metadatas", [])
            documents = all_docs.get("documents", [])

            if metadatas:
                for i, metadata in enumerate(metadatas):
                    if not metadata:
                        continue

                    source = metadata.get("source")
                    document_id = metadata.get("document_id")

                    if source and document_id:
                        if document_id not in sources:
                            sources[document_id] = {
                                "document_id": document_id,
                                "source_filename": source,
                                "chunk_count": 0,
                                "upload_timestamp": metadata.get("upload_timestamp", ""),
                                "total_chars": 0
                            }
                        sources[document_id]["chunk_count"] += 1
                        if documents and i < len(documents):
                            sources[document_id]["total_chars"] += len(documents[i])

            return list(sources.values())
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def save_to_disk(self) -> bool:
        """ChromaDBëŠ” ìë™ìœ¼ë¡œ ì˜êµ¬ ì €ì¥ë¨"""
        return True

    def load_from_disk(self) -> bool:
        """ChromaDBëŠ” ìë™ìœ¼ë¡œ ë¡œë“œë¨"""
        return True


class FallbackChromaDB(VectorDatabase):
    """ChromaDBê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” fallback (ê¸°ì¡´ JSON ë°©ì‹)"""

    def __init__(self, data_dir: str = "./vector_data"):
        self.documents = {}
        self.name = "fallback_json"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.data_file = self.data_dir / "fallback_documents.json"

        # ì„ë² ë”© ëª¨ë¸ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        self.embedder = None
        if HAS_SENTENCE_TRANSFORMERS:
            try:
                self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                logger.info("SentenceTransformer ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")

        # ì‹œì‘ì‹œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self.load_from_disk()

    def _embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
        if self.embedder:
            return self.embedder.encode(text).tolist()
        else:
            # fallback: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„° (ë§¤ìš° ë‹¨ìˆœí•¨)
            hash_val = hashlib.md5(text.encode()).hexdigest()
            return [float(ord(c)) / 255.0 for c in hash_val[:384]]  # 384ì°¨ì›

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        try:
            # ë²¡í„°ê°€ ì—†ìœ¼ë©´ ìƒì„±
            if not vector:
                vector = self._embed_text(text)

            self.documents[doc_id] = {
                "text": text,
                "vector": vector,
                "metadata": metadata
            }
            return True
        except Exception as e:
            logger.error(f"Fallback ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def search_similar(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            if isinstance(query, str):
                query_vector = self._embed_text(query)
            else:
                query_vector = query

            results = []
            for doc_id, doc_data in self.documents.items():
                # íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§
                if document_id and not doc_data["metadata"].get("document_id") == document_id:
                    continue

                doc_vector = doc_data["vector"]
                similarity = np.dot(query_vector, doc_vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
                )

                results.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"],
                    "similarity": float(similarity)
                })

            results.sort(key=lambda x: x["similarity"], reverse=True)
            return results[:top_k]
        except Exception as e:
            logger.error(f"Fallback ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def count_documents(self) -> int:
        return len(self.documents)

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        docs = []
        for doc_id, doc_data in self.documents.items():
            if doc_data["metadata"].get("source") == source_name:
                docs.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"]
                })
        return docs

    def list_document_sources(self) -> List[Dict[str, Any]]:
        sources = {}
        for doc_id, doc_data in self.documents.items():
            source = doc_data["metadata"].get("source")
            document_id = doc_data["metadata"].get("document_id")

            if source and document_id:
                if document_id not in sources:
                    sources[document_id] = {
                        "document_id": document_id,
                        "source_filename": source,
                        "chunk_count": 0,
                        "upload_timestamp": doc_data["metadata"].get("upload_timestamp", ""),
                        "total_chars": 0
                    }
                sources[document_id]["chunk_count"] += 1
                sources[document_id]["total_chars"] += len(doc_data["text"])

        return list(sources.values())

    def save_to_disk(self) -> bool:
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            logger.error(f"Fallback ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_from_disk(self) -> bool:
        try:
            if self.data_file.exists():
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                logger.info(f"Fallback DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            logger.error(f"Fallback ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.documents = {}
            return False


class VectorDBFactory:
    """ğŸ”¥ ë²¡í„° DB íŒ©í† ë¦¬ í´ë˜ìŠ¤ - ì‹¤ì œ ChromaDB ìš°ì„  ì‚¬ìš©"""

    _db_types = {
        "chromadb": RealChromaDB,
        "fallback": FallbackChromaDB
    }

    @classmethod
    def create_vector_db(cls, db_type: str = "chromadb") -> VectorDatabase:
        """ë²¡í„° DB ìƒì„± - ChromaDB ìš°ì„ , ì‹¤íŒ¨ì‹œ fallback"""

        # ChromaDB ìš°ì„  ì‹œë„
        if db_type == "chromadb" or db_type == "real_chromadb":
            try:
                return RealChromaDB()
            except ImportError as e:
                logger.warning(f"ChromaDB ì‚¬ìš© ë¶ˆê°€, fallback ì‚¬ìš©: {e}")
                return FallbackChromaDB()

        # Fallback ì§ì ‘ ìš”ì²­
        if db_type == "fallback":
            return FallbackChromaDB()

        # ê¸°ë³¸ê°’ì€ ChromaDB ì‹œë„
        try:
            return RealChromaDB()
        except ImportError:
            logger.warning("ChromaDB ì—†ìŒ, fallback ì‚¬ìš©")
            return FallbackChromaDB()

    @classmethod
    def get_supported_types(cls) -> List[str]:
        return list(cls._db_types.keys())


class TextEmbedder:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ê¸° (ChromaDBê°€ ìë™ ì²˜ë¦¬í•˜ë¯€ë¡œ í•„ìš”ì‹œë§Œ ì‚¬ìš©)"""

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (fallbackìš©)"""
        if HAS_SENTENCE_TRANSFORMERS:
            model = SentenceTransformer('all-MiniLM-L6-v2')
            return model.encode(text).tolist()
        else:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„°
            hash_val = hashlib.md5(text.encode()).hexdigest()
            return [float(ord(c)) / 255.0 for c in hash_val[:384]]


class TextChunker:
    """í…ìŠ¤íŠ¸ ì²­í‚¹ í´ë˜ìŠ¤"""

    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹"""
        if len(text) <= self.chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
            end = start + self.chunk_size

            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë¼ë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                # ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ ë ì°¾ê¸°
                sentence_ends = ['.', '!', '?', '\n']
                best_end = end

                for i in range(end - 100, end + 100):
                    if i < len(text) and text[i] in sentence_ends:
                        best_end = i + 1
                        break

                end = best_end

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # ë‹¤ìŒ ì‹œì‘ì  ê³„ì‚° (ì˜¤ë²„ë© ì ìš©)
            start = end - self.overlap

        return chunks


class PDFVectorService:
    """ğŸ”¥ ì‹¤ì œ ChromaDB ê¸°ë°˜ PDF ë²¡í„° ì„œë¹„ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""

    _instance = None
    _initialized = False

    def __new__(cls, db_type: str = "chromadb"):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, db_type: str = "chromadb"):
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ë‹¤ì‹œ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        if self._initialized:
            return

        self.vector_db = VectorDBFactory.create_vector_db(db_type)
        self.text_chunker = TextChunker()
        self.embedder = TextEmbedder()  # fallbackìš©

        logger.info(f"ğŸ”¥ PDFVectorService ì´ˆê¸°í™” ì™„ë£Œ: {self.vector_db.name} ì‚¬ìš©")
        self._initialized = True

    def process_pdf_text(self, pdf_text: str, source_name: str) -> Dict[str, Any]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥"""
        try:
            # ê³ ìœ í•œ ë¬¸ì„œ ID ìƒì„±
            document_id = str(uuid.uuid4())
            upload_timestamp = datetime.now().isoformat()

            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.text_chunker.chunk_text(pdf_text)
            logger.info(f"PDF ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            stored_count = 0
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ìŠ¤í‚µ
                    continue

                # ì²­í¬ ID ìƒì„±
                chunk_id = f"{document_id}_{i}"

                # ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "document_id": document_id,
                    "source": source_name,
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "upload_timestamp": upload_timestamp,
                    "text_length": len(chunk)
                }

                # ChromaDBëŠ” ìë™ ì„ë² ë”© ìƒì„±í•˜ë¯€ë¡œ ë¹ˆ ë²¡í„° ì „ë‹¬
                success = self.vector_db.store_document(
                    doc_id=chunk_id,
                    text=chunk,
                    vector=[],  # ChromaDBê°€ ìë™ ìƒì„±
                    metadata=metadata
                )

                if success:
                    stored_count += 1

            # ì €ì¥ (ChromaDBëŠ” ìë™ ì €ì¥)
            self.vector_db.save_to_disk()

            result = {
                "document_id": document_id,
                "source_name": source_name,
                "total_chunks": len(chunks),
                "stored_chunks": stored_count,
                "upload_timestamp": upload_timestamp,
                "success": stored_count > 0
            }

            logger.info(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {stored_count}/{len(chunks)}ê°œ ì²­í¬ ì €ì¥ë¨")
            return result

        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "document_id": None,
                "source_name": source_name,
                "error": str(e),
                "success": False
            }

    def search_documents(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        return self.vector_db.search_similar(query, top_k, document_id)

    def search_in_document(self, query: str, document_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œ ê²€ìƒ‰"""
        return self.vector_db.search_similar(query, top_k, document_id)

    def get_document_list(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡"""
        return self.vector_db.list_document_sources()

    def get_document_info(self, document_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ì •ë³´"""
        sources = self.vector_db.list_document_sources()
        for source in sources:
            if source.get("document_id") == document_id:
                return source
        return None

    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´"""
        try:
            total_docs = self.vector_db.count_documents()
            sources = self.vector_db.list_document_sources()

            return {
                "total_chunks": total_docs,
                "total_documents": len(sources),
                "database_type": self.vector_db.name,
                "sources": sources
            }
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}

    def switch_database(self, new_db_type: str) -> bool:
        """ë²¡í„° DB ë³€ê²½"""
        try:
            old_db = self.vector_db
            self.vector_db = VectorDBFactory.create_vector_db(new_db_type)
            logger.info(f"ë²¡í„° DB ë³€ê²½: {old_db.name} -> {self.vector_db.name}")
            return True
        except Exception as e:
            logger.error(f"ë²¡í„° DB ë³€ê²½ ì‹¤íŒ¨: {e}")
            return False

    def force_save(self) -> bool:
        """ê°•ì œ ì €ì¥"""
        try:
            return self.vector_db.save_to_disk()
        except Exception as e:
            logger.error(f"ê°•ì œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
def get_global_vector_service() -> PDFVectorService:
    """ì „ì—­ ë²¡í„° ì„œë¹„ìŠ¤ ë°˜í™˜"""
    return PDFVectorService()


if __name__ == "__main__":
    print("ğŸ”¥ Real ChromaDB Vector Service")
    print("âœ… High-performance vector search")
    print("âœ… Automatic embedding generation")
    print("âœ… Persistent storage")
    print("âœ… Scalable and production-ready")

    # ì‚¬ìš© ì˜ˆì‹œ
    service = PDFVectorService()
    print(f"í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë²¡í„°DB: {service.vector_db.name}")
    print(f"ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {service.vector_db.count_documents()}")