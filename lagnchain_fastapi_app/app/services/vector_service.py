#!/usr/bin/env python3
"""
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ (íŒ©í† ë¦¬ íŒ¨í„´)
"""
import hashlib
import numpy as np
import json
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pathlib import Path
import uuid
from datetime import datetime


class VectorDatabase(ABC):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì €ì¥"""
        pass

    @abstractmethod
    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        pass

    @abstractmethod
    def count_documents(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        pass

    @abstractmethod
    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤(íŒŒì¼)ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        pass

    @abstractmethod
    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        pass

    @abstractmethod
    def save_to_disk(self) -> bool:
        """ë””ìŠ¤í¬ì— ì €ì¥"""
        pass

    @abstractmethod
    def load_from_disk(self) -> bool:
        """ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ"""
        pass


class WeaviateDB(VectorDatabase):
    """Weaviate ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (íŒŒì¼ ì§€ì†ì„± í¬í•¨)"""

    def __init__(self, data_dir: str = "./vector_data"):
        self.documents = {}
        self.name = "weaviate"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.data_file = self.data_dir / "weaviate_documents.json"

        # ì‹œì‘ì‹œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self.load_from_disk()

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        try:
            self.documents[doc_id] = {
                "text": text,
                "vector": vector,
                "metadata": metadata
            }
            # ë§¤ë²ˆ ì €ì¥í•˜ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë¯€ë¡œ, ì¼ì • ê°„ê²©ì´ë‚˜ ì¢…ë£Œì‹œì—ë§Œ ì €ì¥
            return True
        except Exception:
            return False

    def save_to_disk(self) -> bool:
        """ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_from_disk(self) -> bool:
        """ë””ìŠ¤í¬ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.data_file.exists():
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                print(f"ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            print(f"ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.documents = {}
            return False

    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        results = []

        for doc_id, doc_data in self.documents.items():
            # íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§
            if document_id and not doc_data["metadata"].get("document_id") == document_id:
                continue

            doc_vector = doc_data["vector"]
            similarity = np.dot(query_vector, doc_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
            )

            results.append({
                "doc_id": doc_id,
                "text": doc_data["text"],
                "metadata": doc_data["metadata"],
                "similarity": float(similarity)
            })

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]

    def count_documents(self) -> int:
        return len(self.documents)

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        docs = []
        for doc_id, doc_data in self.documents.items():
            if doc_data["metadata"].get("source") == source_name:
                docs.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"]
                })
        return docs

    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = {}
        for doc_id, doc_data in self.documents.items():
            source = doc_data["metadata"].get("source")
            document_id = doc_data["metadata"].get("document_id")

            if source and document_id:
                if document_id not in sources:
                    sources[document_id] = {
                        "document_id": document_id,
                        "source_filename": source,
                        "chunk_count": 0,
                        "upload_timestamp": doc_data["metadata"].get("upload_timestamp", ""),
                        "total_chars": 0
                    }
                sources[document_id]["chunk_count"] += 1
                sources[document_id]["total_chars"] += len(doc_data["text"])

        return list(sources.values())


class ChromaDB(VectorDatabase):
    """Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (íŒŒì¼ ì§€ì†ì„± í¬í•¨)"""

    def __init__(self, data_dir: str = "./vector_data"):
        self.documents = {}
        self.name = "chroma"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.data_file = self.data_dir / "chroma_documents.json"

        # ì‹œì‘ì‹œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self.load_from_disk()

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        try:
            self.documents[doc_id] = {
                "text": text,
                "vector": vector,
                "metadata": metadata
            }
            return True
        except Exception:
            return False

    def save_to_disk(self) -> bool:
        """ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(self.documents, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def load_from_disk(self) -> bool:
        """ë””ìŠ¤í¬ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.data_file.exists():
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                print(f"ë²¡í„° DB ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            return True
        except Exception as e:
            print(f"ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.documents = {}
            return False

    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        results = []

        for doc_id, doc_data in self.documents.items():
            # íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§
            if document_id and not doc_data["metadata"].get("document_id") == document_id:
                continue

            doc_vector = doc_data["vector"]
            similarity = np.dot(query_vector, doc_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
            )

            results.append({
                "doc_id": doc_id,
                "text": doc_data["text"],
                "metadata": doc_data["metadata"],
                "similarity": float(similarity)
            })

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]

    def count_documents(self) -> int:
        return len(self.documents)

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        docs = []
        for doc_id, doc_data in self.documents.items():
            if doc_data["metadata"].get("source") == source_name:
                docs.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"]
                })
        return docs

    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = {}
        for doc_id, doc_data in self.documents.items():
            source = doc_data["metadata"].get("source")
            document_id = doc_data["metadata"].get("document_id")

            if source and document_id:
                if document_id not in sources:
                    sources[document_id] = {
                        "document_id": document_id,
                        "source_filename": source,
                        "chunk_count": 0,
                        "upload_timestamp": doc_data["metadata"].get("upload_timestamp", ""),
                        "total_chars": 0
                    }
                sources[document_id]["chunk_count"] += 1
                sources[document_id]["total_chars"] += len(doc_data["text"])

        return list(sources.values())


class VectorDBFactory:
    """ë²¡í„° DB íŒ©í† ë¦¬ í´ë˜ìŠ¤"""

    _db_types = {
        "weaviate": WeaviateDB,
        "chroma": ChromaDB,
    }

    @classmethod
    def create_vector_db(cls, db_type: str = "weaviate") -> VectorDatabase:
        """ë²¡í„° DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if db_type not in cls._db_types:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {db_type}")

        return cls._db_types[db_type]()

    @classmethod
    def get_supported_types(cls) -> List[str]:
        """ì§€ì›í•˜ëŠ” DB íƒ€ì… ëª©ë¡"""
        return list(cls._db_types.keys())


class TextEmbedder:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© í´ë˜ìŠ¤"""

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
        text_hash = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(text_hash % (2**32))
        return np.random.normal(0, 1, 384).tolist()


class TextChunker:
    """í…ìŠ¤íŠ¸ ì²­í‚¹ í´ë˜ìŠ¤"""

    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if len(text) <= self.chunk_size:
            return [text.strip()]

        chunks = []
        start = 0

        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end].strip()

            if len(chunk) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                chunks.append(chunk)

            start = end - self.overlap
            if start >= len(text):
                break

        return chunks


class PDFVectorService:
    """PDF â†’ ë²¡í„° ì €ì¥ ë©”ì¸ ì„œë¹„ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""

    _instance = None
    _initialized = False

    def __new__(cls, db_type: str = "weaviate"):
        if cls._instance is None:
            cls._instance = super(PDFVectorService, cls).__new__(cls)
        return cls._instance

    def __init__(self, db_type: str = "weaviate"):
        # ì´ë¯¸ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ë©´ ë‹¤ì‹œ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ
        if PDFVectorService._initialized:
            return

        self.vector_db = VectorDBFactory.create_vector_db(db_type)
        self.embedder = TextEmbedder()
        self.chunker = TextChunker()
        self.db_type = db_type
        PDFVectorService._initialized = True
        print(f"ë²¡í„° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {db_type} DB, ê¸°ì¡´ {self.vector_db.count_documents()}ê°œ ë¬¸ì„œ ë¡œë“œë¨")

    def process_pdf_text(self, pdf_text: str, source_name: str) -> Dict[str, Any]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥"""

        # ê³ ìœ  ë¬¸ì„œ ID ìƒì„±
        document_id = str(uuid.uuid4())

        # í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = self.chunker.chunk_text(pdf_text)

        stored_count = 0
        failed_count = 0

        # ê° ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
                continue

            # ì²­í¬ ID ìƒì„±
            chunk_id = f"{document_id}_chunk_{i}"

            # ë²¡í„° ì„ë² ë”©
            vector = self.embedder.embed_text(chunk)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "document_id": document_id,
                "source": source_name,
                "chunk_index": i,
                "chunk_id": chunk_id,
                "upload_timestamp": datetime.now().isoformat(),
                "text_length": len(chunk)
            }

            # ë²¡í„° DBì— ì €ì¥
            if self.vector_db.store_document(chunk_id, chunk, vector, metadata):
                stored_count += 1
            else:
                failed_count += 1

        # ğŸ”¥ ì¤‘ìš”: ì €ì¥ í›„ ë””ìŠ¤í¬ì— ì˜êµ¬ ì €ì¥
        self.vector_db.save_to_disk()

        return {
            "document_id": document_id,
            "source_filename": source_name,
            "total_chunks": len(chunks),
            "stored_chunks": stored_count,
            "failed_chunks": failed_count,
            "text_length": len(pdf_text)
        }

    def search_documents(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§ ê°€ëŠ¥)"""
        query_vector = self.embedder.embed_text(query)
        return self.vector_db.search_similar(query_vector, top_k, document_id)

    def search_in_document(self, query: str, document_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰"""
        return self.search_documents(query, top_k, document_id)

    def get_document_list(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        return self.vector_db.list_document_sources()

    def get_document_info(self, document_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ì •ë³´ ì¡°íšŒ"""
        sources = self.get_document_list()
        for source in sources:
            if source["document_id"] == document_id:
                return source
        return None

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„"""
        document_sources = self.get_document_list()
        return {
            "total_documents": self.vector_db.count_documents(),
            "total_uploaded_files": len(document_sources),
            "db_type": self.db_type,
            "supported_dbs": VectorDBFactory.get_supported_types(),
            "uploaded_files": document_sources
        }

    def switch_database(self, new_db_type: str) -> bool:
        """ë²¡í„° DB ë³€ê²½ (íŒ©í† ë¦¬ íŒ¨í„´ í™œìš©)"""
        try:
            # ê¸°ì¡´ ë°ì´í„° ì €ì¥
            self.vector_db.save_to_disk()

            # ìƒˆ DBë¡œ ì „í™˜
            self.vector_db = VectorDBFactory.create_vector_db(new_db_type)
            self.db_type = new_db_type
            return True
        except ValueError:
            return False

    def force_save(self) -> bool:
        """ê°•ì œë¡œ ë””ìŠ¤í¬ì— ì €ì¥"""
        return self.vector_db.save_to_disk()


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_global_vector_service = None

def get_global_vector_service() -> PDFVectorService:
    """ì „ì—­ ë²¡í„° ì„œë¹„ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _global_vector_service

    if _global_vector_service is None:
        _global_vector_service = PDFVectorService()
        print("âœ… ì „ì—­ ë²¡í„° ì„œë¹„ìŠ¤ ìƒì„± ì™„ë£Œ")

    return _global_vector_service