#!/usr/bin/env python3
"""
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ (íŒ©í† ë¦¬ íŒ¨í„´)
"""
import hashlib
import numpy as np
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from pathlib import Path
import uuid
from datetime import datetime


class VectorDatabase(ABC):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        """ë¬¸ì„œ ì €ì¥"""
        pass

    @abstractmethod
    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        pass

    @abstractmethod
    def count_documents(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ìˆ˜"""
        pass

    @abstractmethod
    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤(íŒŒì¼)ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        pass

    @abstractmethod
    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        pass


class WeaviateDB(VectorDatabase):
    """Weaviate ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (ì‹œë®¬ë ˆì´ì…˜)"""

    def __init__(self):
        self.documents = {}
        self.name = "weaviate"

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        try:
            self.documents[doc_id] = {
                "text": text,
                "vector": vector,
                "metadata": metadata
            }
            return True
        except Exception:
            return False

    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        results = []

        for doc_id, doc_data in self.documents.items():
            # íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§
            if document_id and not doc_data["metadata"].get("document_id") == document_id:
                continue

            doc_vector = doc_data["vector"]
            similarity = np.dot(query_vector, doc_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
            )

            results.append({
                "doc_id": doc_id,
                "text": doc_data["text"],
                "metadata": doc_data["metadata"],
                "similarity": float(similarity)
            })

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]

    def count_documents(self) -> int:
        return len(self.documents)

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        docs = []
        for doc_id, doc_data in self.documents.items():
            if doc_data["metadata"].get("source") == source_name:
                docs.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"]
                })
        return docs

    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = {}
        for doc_id, doc_data in self.documents.items():
            source = doc_data["metadata"].get("source")
            document_id = doc_data["metadata"].get("document_id")

            if source and document_id:
                if document_id not in sources:
                    sources[document_id] = {
                        "document_id": document_id,
                        "source_filename": source,
                        "chunk_count": 0,
                        "upload_timestamp": doc_data["metadata"].get("upload_timestamp", ""),
                        "total_chars": 0
                    }
                sources[document_id]["chunk_count"] += 1
                sources[document_id]["total_chars"] += len(doc_data["text"])

        return list(sources.values())


class ChromaDB(VectorDatabase):
    """Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (ê°œë°œìš©)"""

    def __init__(self):
        self.documents = {}
        self.name = "chroma"

    def store_document(self, doc_id: str, text: str, vector: List[float], metadata: Dict[str, Any]) -> bool:
        try:
            self.documents[doc_id] = {
                "text": text,
                "vector": vector,
                "metadata": metadata
            }
            return True
        except Exception:
            return False

    def search_similar(self, query_vector: List[float], top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        results = []

        for doc_id, doc_data in self.documents.items():
            # íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§
            if document_id and not doc_data["metadata"].get("document_id") == document_id:
                continue

            doc_vector = doc_data["vector"]
            similarity = np.dot(query_vector, doc_vector) / (
                np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
            )

            results.append({
                "doc_id": doc_id,
                "text": doc_data["text"],
                "metadata": doc_data["metadata"],
                "similarity": float(similarity)
            })

        results.sort(key=lambda x: x["similarity"], reverse=True)
        return results[:top_k]

    def count_documents(self) -> int:
        return len(self.documents)

    def get_documents_by_source(self, source_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì†ŒìŠ¤ íŒŒì¼ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        docs = []
        for doc_id, doc_data in self.documents.items():
            if doc_data["metadata"].get("source") == source_name:
                docs.append({
                    "doc_id": doc_id,
                    "text": doc_data["text"],
                    "metadata": doc_data["metadata"]
                })
        return docs

    def list_document_sources(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ì†ŒìŠ¤ ëª©ë¡ ì¡°íšŒ"""
        sources = {}
        for doc_id, doc_data in self.documents.items():
            source = doc_data["metadata"].get("source")
            document_id = doc_data["metadata"].get("document_id")

            if source and document_id:
                if document_id not in sources:
                    sources[document_id] = {
                        "document_id": document_id,
                        "source_filename": source,
                        "chunk_count": 0,
                        "upload_timestamp": doc_data["metadata"].get("upload_timestamp", ""),
                        "total_chars": 0
                    }
                sources[document_id]["chunk_count"] += 1
                sources[document_id]["total_chars"] += len(doc_data["text"])

        return list(sources.values())


class VectorDBFactory:
    """ë²¡í„° DB íŒ©í† ë¦¬ í´ë˜ìŠ¤"""

    _db_types = {
        "weaviate": WeaviateDB,
        "chroma": ChromaDB,
    }

    @classmethod
    def create_vector_db(cls, db_type: str = "weaviate") -> VectorDatabase:
        """ë²¡í„° DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if db_type not in cls._db_types:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB íƒ€ì…: {db_type}")

        return cls._db_types[db_type]()

    @classmethod
    def get_supported_types(cls) -> List[str]:
        """ì§€ì›í•˜ëŠ” DB íƒ€ì… ëª©ë¡"""
        return list(cls._db_types.keys())


class TextEmbedder:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© í´ë˜ìŠ¤"""

    def embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ 384ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜"""
        text_hash = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        np.random.seed(text_hash % (2**32))
        return np.random.normal(0, 1, 384).tolist()


class TextChunker:
    """í…ìŠ¤íŠ¸ ì²­í‚¹ í´ë˜ìŠ¤"""

    def __init__(self, chunk_size: int = 1000, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if len(text) <= self.chunk_size:
            return [text.strip()]

        chunks = []
        start = 0

        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end].strip()

            if len(chunk) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                chunks.append(chunk)

            start = end - self.overlap
            if start >= len(text):
                break

        return chunks


class PDFVectorService:
    """PDF â†’ ë²¡í„° ì €ì¥ ë©”ì¸ ì„œë¹„ìŠ¤"""

    def __init__(self, db_type: str = "weaviate"):
        self.vector_db = VectorDBFactory.create_vector_db(db_type)
        self.embedder = TextEmbedder()
        self.chunker = TextChunker()
        self.db_type = db_type

    def process_pdf_text(self, pdf_text: str, source_name: str) -> Dict[str, Any]:
        """PDF í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥"""
        # ê³ ìœ í•œ ë¬¸ì„œ ID ìƒì„±
        document_id = str(uuid.uuid4())
        upload_timestamp = datetime.now().isoformat()

        # 1. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = self.chunker.chunk_text(pdf_text)

        if not chunks:
            return {
                "success": False,
                "error": "ìœ íš¨í•œ ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
            }

        # 2. ì²­í¬ë³„ ë²¡í„° ì €ì¥
        stored_count = 0

        for i, chunk in enumerate(chunks):
            doc_id = f"{document_id}_chunk_{i}"

            # ë²¡í„° ìƒì„±
            vector = self.embedder.embed_text(chunk)

            # ë©”íƒ€ë°ì´í„° ìƒì„± (document_id ì¶”ê°€)
            metadata = {
                "document_id": document_id,        # ğŸ†• ë¬¸ì„œ ì‹ë³„ì
                "source": source_name,
                "chunk_index": i,
                "chunk_size": len(chunk),
                "total_chunks": len(chunks),
                "db_type": self.db_type,
                "upload_timestamp": upload_timestamp  # ğŸ†• ì—…ë¡œë“œ ì‹œê°„
            }

            # ë²¡í„° DBì— ì €ì¥
            if self.vector_db.store_document(doc_id, chunk, vector, metadata):
                stored_count += 1

        return {
            "success": True,
            "document_id": document_id,           # ğŸ†• ì‚¬ìš©ìì—ê²Œ ë°˜í™˜
            "total_chunks": len(chunks),
            "stored_chunks": stored_count,
            "db_type": self.db_type,
            "source": source_name,
            "upload_timestamp": upload_timestamp
        }

    def search_documents(self, query: str, top_k: int = 5, document_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (íŠ¹ì • ë¬¸ì„œ IDë¡œ í•„í„°ë§ ê°€ëŠ¥)"""
        query_vector = self.embedder.embed_text(query)
        return self.vector_db.search_similar(query_vector, top_k, document_id)

    def search_in_document(self, query: str, document_id: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œë§Œ ê²€ìƒ‰"""
        return self.search_documents(query, top_k, document_id)

    def get_document_list(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        return self.vector_db.list_document_sources()

    def get_document_info(self, document_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ë¬¸ì„œ ì •ë³´ ì¡°íšŒ"""
        sources = self.get_document_list()
        for source in sources:
            if source["document_id"] == document_id:
                return source
        return None

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„"""
        document_sources = self.get_document_list()
        return {
            "total_documents": self.vector_db.count_documents(),
            "total_uploaded_files": len(document_sources),
            "db_type": self.db_type,
            "supported_dbs": VectorDBFactory.get_supported_types(),
            "uploaded_files": document_sources
        }

    def switch_database(self, new_db_type: str) -> bool:
        """ë²¡í„° DB ë³€ê²½ (íŒ©í† ë¦¬ íŒ¨í„´ í™œìš©)"""
        try:
            self.vector_db = VectorDBFactory.create_vector_db(new_db_type)
            self.db_type = new_db_type
            return True
        except ValueError:
            return False