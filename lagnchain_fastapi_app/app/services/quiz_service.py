"""
PDF ê¸°ë°˜ RAG í€´ì¦ˆ ìƒì„± ì„œë¹„ìŠ¤
ìµœì í™”ëœ í€´ì¦ˆ ìƒì„±ì„ ìœ„í•œ ë©”ì¸ ì„œë¹„ìŠ¤
"""
import logging
import time
import uuid
from typing import List, Dict, Any, Optional, Union
from dataclasses import asdict

from ..schemas.quiz_schema import (
    QuizRequest, QuizResponse, Question, Difficulty, QuestionType,
    RAGContext, TopicAnalysis, QuizGenerationStats
)
from ..services.llm_factory import LLMFactory, BaseLLMService, get_default_llm_service
from ..services.vector_service import PDFVectorService, get_global_vector_service

logger = logging.getLogger(__name__)


class RAGRetriever:
    """RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í´ë˜ìŠ¤"""

    def __init__(self, vector_service: PDFVectorService):
        self.vector_service = vector_service

    def retrieve_contexts_for_quiz(
        self,
        document_id: str,
        num_questions: int,
        topics: Optional[List[str]] = None
    ) -> List[RAGContext]:
        """í€´ì¦ˆ ìƒì„±ì„ ìœ„í•œ ìµœì  ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""

        logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘: {document_id} ({num_questions}ë¬¸ì œ)")

        # ë¬¸ì„œ ì •ë³´ í™•ì¸
        doc_info = self.vector_service.get_document_info(document_id)
        if not doc_info:
            raise ValueError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}")

        contexts = []

        # ì£¼ì œë³„ ê²€ìƒ‰ ë˜ëŠ” ì „ì²´ ê²€ìƒ‰
        if topics:
            # íŠ¹ì • ì£¼ì œë“¤ì— ëŒ€í•œ ê²€ìƒ‰
            for topic in topics:
                search_results = self.vector_service.search_in_document(
                    query=topic,
                    document_id=document_id,
                    top_k=max(3, num_questions // len(topics))
                )
                contexts.extend(self._convert_to_rag_contexts(search_results, topic))
        else:
            # ì „ì²´ ë¬¸ì„œì—ì„œ ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
            general_queries = [
                "ì•Œê³ ë¦¬ì¦˜", "ë°©ë²•", "ì •ì˜", "ê°œë…", "ì›ë¦¬", "ì˜ˆì‹œ", "ë¬¸ì œ", "í•´ê²°", "ê³„ì‚°", "êµ¬í˜„"
            ]

            for query in general_queries[:num_questions//2 + 1]:
                search_results = self.vector_service.search_in_document(
                    query=query,
                    document_id=document_id,
                    top_k=2
                )
                contexts.extend(self._convert_to_rag_contexts(search_results))

        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        contexts = self._deduplicate_contexts(contexts)
        contexts = self._filter_context_quality(contexts)

        # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
        contexts.sort(key=lambda x: x.similarity, reverse=True)

        logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ: {len(contexts)}ê°œ")
        return contexts[:num_questions * 2]  # ì—¬ìœ ë¶„ í™•ë³´

    def _convert_to_rag_contexts(
        self,
        search_results: List[Dict],
        topic: Optional[str] = None
    ) -> List[RAGContext]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ RAGContextë¡œ ë³€í™˜"""
        contexts = []

        for result in search_results:
            context = RAGContext(
                text=result["text"],
                similarity=result["similarity"],
                source=result["metadata"].get("source", ""),
                chunk_index=result["metadata"].get("chunk_index", 0),
                topic=topic,
                metadata=result["metadata"]
            )
            contexts.append(context)

        return contexts

    def _deduplicate_contexts(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """ì¤‘ë³µ ì»¨í…ìŠ¤íŠ¸ ì œê±°"""
        seen_texts = set()
        unique_contexts = []

        for context in contexts:
            # í…ìŠ¤íŠ¸ì˜ ì²« 100ìë¡œ ì¤‘ë³µ ì²´í¬
            text_signature = context.text[:100].strip()
            if text_signature not in seen_texts:
                seen_texts.add(text_signature)
                unique_contexts.append(context)

        return unique_contexts

    def _filter_context_quality(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í•„í„°ë§"""
        return [
            ctx for ctx in contexts
            if len(ctx.text.strip()) >= 50 and ctx.similarity >= 0.05
        ]


class TopicExtractor:
    """ì£¼ì œ ì¶”ì¶œ í´ë˜ìŠ¤"""

    def __init__(self, llm_service: BaseLLMService, vector_service: PDFVectorService):
        self.llm_service = llm_service
        self.vector_service = vector_service

    def extract_document_topics(self, document_id: str) -> List[TopicAnalysis]:
        """ë¬¸ì„œì—ì„œ ì£¼ìš” í† í”½ ì¶”ì¶œ ë° ë¶„ì„"""

        logger.info(f"ë¬¸ì„œ í† í”½ ì¶”ì¶œ ì‹œì‘: {document_id}")

        # ë¬¸ì„œì˜ ëŒ€í‘œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        sample_contexts = self.vector_service.search_in_document(
            query="ì£¼ìš” ë‚´ìš© í•µì‹¬",
            document_id=document_id,
            top_k=5
        )

        combined_text = "\n".join([ctx["text"] for ctx in sample_contexts])

        # LLMìœ¼ë¡œ í† í”½ ì¶”ì¶œ
        topics = self.llm_service.extract_topics(combined_text)

        # ê° í† í”½ë³„ ë¶„ì„
        topic_analyses = []
        for topic in topics:
            analysis = self._analyze_topic(document_id, topic)
            topic_analyses.append(analysis)

        logger.info(f"í† í”½ ì¶”ì¶œ ì™„ë£Œ: {len(topic_analyses)}ê°œ")
        return topic_analyses

    def _analyze_topic(self, document_id: str, topic: str) -> TopicAnalysis:
        """ê°œë³„ í† í”½ ë¶„ì„"""

        # í† í”½ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        search_results = self.vector_service.search_in_document(
            query=topic,
            document_id=document_id,
            top_k=3
        )

        if not search_results:
            return TopicAnalysis(
                topic=topic,
                confidence=0.1,
                keywords=[],
                context_chunks=[],
                question_potential=1
            )

        # í‰ê·  ìœ ì‚¬ë„ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        avg_similarity = sum(r["similarity"] for r in search_results) / len(search_results)

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        all_text = " ".join([r["text"] for r in search_results])
        keywords = self._extract_keywords(all_text)

        # ë¬¸ì œ ìƒì„± ê°€ëŠ¥ì„± ì ìˆ˜
        question_potential = min(10, int(avg_similarity * 10) + len(search_results))

        return TopicAnalysis(
            topic=topic,
            confidence=avg_similarity,
            keywords=keywords,
            context_chunks=[r["text"] for r in search_results],
            question_potential=question_potential
        )

    def _extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì¶”í›„ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ê°œì„  ê°€ëŠ¥)"""
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì œê±° ë° ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
        stopwords = {'ì´', 'ê·¸', 'ì €', 'ì˜', 'ë¥¼', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'ìˆë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆ˜', 'ê²ƒ'}

        words = text.split()
        keywords = []

        for word in words:
            if (len(word) >= 2 and
                word not in stopwords and
                word.replace(' ', '').isalnum()):
                keywords.append(word)

        # ë¹ˆë„ìˆ˜ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        from collections import Counter
        word_counts = Counter(keywords)
        return [word for word, count in word_counts.most_common(10)]


class QuizValidator:
    """í€´ì¦ˆ í’ˆì§ˆ ê²€ì¦ í´ë˜ìŠ¤"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

    def validate_quiz_quality(self, questions: List[Question]) -> Dict[str, Any]:
        """í€´ì¦ˆ ì „ì²´ í’ˆì§ˆ ê²€ì¦"""

        validation_result = {
            "overall_quality": "good",
            "total_questions": len(questions),
            "valid_questions": 0,
            "issues": [],
            "recommendations": []
        }

        valid_count = 0

        for i, question in enumerate(questions):
            question_dict = asdict(question)
            if self.llm_service.validate_question_quality(question_dict):
                valid_count += 1
            else:
                validation_result["issues"].append(f"ë¬¸ì œ {i+1}: í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬")

        validation_result["valid_questions"] = valid_count

        # ì „ì²´ í’ˆì§ˆ í‰ê°€
        quality_ratio = valid_count / len(questions) if questions else 0

        if quality_ratio >= 0.8:
            validation_result["overall_quality"] = "excellent"
        elif quality_ratio >= 0.6:
            validation_result["overall_quality"] = "good"
        elif quality_ratio >= 0.4:
            validation_result["overall_quality"] = "fair"
            validation_result["recommendations"].append("ë¬¸ì œ í’ˆì§ˆ ê°œì„  í•„ìš”")
        else:
            validation_result["overall_quality"] = "poor"
            validation_result["recommendations"].append("ë¬¸ì œ ì¬ìƒì„± ê¶Œì¥")

        return validation_result


class QuizService:
    """PDF ê¸°ë°˜ í€´ì¦ˆ ìƒì„± ë©”ì¸ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        vector_service: Optional[PDFVectorService] = None,
        llm_service: Optional[BaseLLMService] = None
    ):
        """í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""

        # ë²¡í„° ì„œë¹„ìŠ¤ (PDF ì„œë¹„ìŠ¤ì™€ ë™ì¼í•œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )
        if vector_service is None:
            self.vector_service = get_global_vector_service()
        else:
            self.vector_service = vector_service

        # LLM ì„œë¹„ìŠ¤ (ê¸°ë³¸: OpenAI GPT-4o-mini)
        self.llm_service = llm_service or get_default_llm_service()

        # í•˜ìœ„ ì»´í¬ë„ŒíŠ¸ë“¤
        self.rag_retriever = RAGRetriever(self.vector_service)
        self.topic_extractor = TopicExtractor(self.llm_service, self.vector_service)
        self.quiz_validator = QuizValidator(self.llm_service)

        logger.info(f"í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: LLM={self.llm_service.model_name}, VectorDB={self.vector_service.db_type}")

    def generate_quiz(self, request: QuizRequest) -> QuizResponse:
        """ë©”ì¸ í€´ì¦ˆ ìƒì„± ë©”ì„œë“œ - í† í”½ì€ í•­ìƒ ìë™ ì¶”ì¶œ"""

        start_time = time.time()
        quiz_id = str(uuid.uuid4())

        logger.info(f"í€´ì¦ˆ ìƒì„± ì‹œì‘: {request.document_id} ({request.num_questions}ë¬¸ì œ)")

        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ì¡´ì¬ í™•ì¸
            doc_info = self.vector_service.get_document_info(request.document_id)
            if not doc_info:
                raise ValueError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.document_id}")

            logger.info(f"ë¬¸ì„œ í™•ì¸ ì™„ë£Œ: {doc_info['source_filename']} ({doc_info['chunk_count']}ê°œ ì²­í¬)")

            # 2ë‹¨ê³„: í† í”½ ìë™ ì¶”ì¶œ (ì™„ì „ ìë™í™”)
            logger.info("STEP1: ë¬¸ì„œ í† í”½ ìë™ ì¶”ì¶œ ì¤‘...")
            topic_analyses = self.topic_extractor.extract_document_topics(request.document_id)
            extracted_topics = [ta.topic for ta in topic_analyses[:7]]  # ìƒìœ„ 7ê°œ í† í”½

            logger.info(f"ìë™ ì¶”ì¶œëœ í† í”½: {extracted_topics}")

            # 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            logger.info("STEP2: RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘...")
            contexts = self.rag_retriever.retrieve_contexts_for_quiz(
                document_id=request.document_id,
                num_questions=request.num_questions,
                topics=extracted_topics
            )

            if not contexts:
                raise ValueError("í€´ì¦ˆ ìƒì„±ì„ ìœ„í•œ ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ê²°í•©
            combined_context = self._combine_contexts(contexts)
            logger.info(f"ê²°í•©ëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(combined_context)}ì")

            # 5ë‹¨ê³„: ë¬¸ì œ ìœ í˜• ê²°ì •
            question_types = self._determine_question_types(request)

            # 6ë‹¨ê³„: LLMìœ¼ë¡œ í€´ì¦ˆ ìƒì„±
            logger.info("STEP3: LLM í€´ì¦ˆ ìƒì„± ì¤‘...")
            llm_result = self.llm_service.generate_quiz(
                context=combined_context,
                num_questions=request.num_questions,
                difficulty=request.difficulty.value,
                question_types=[qt.value for qt in question_types],
                topics=extracted_topics
            )

            if not llm_result.get("success", False):
                raise ValueError(f"LLM í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {llm_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

            # 7ë‹¨ê³„: ì‘ë‹µ ë°ì´í„°ë¥¼ Question ê°ì²´ë¡œ ë³€í™˜
            logger.info("STEP4: ë¬¸ì œ ë°ì´í„° ë³€í™˜ ì¤‘...")
            questions = self._convert_to_question_objects(
                llm_result["questions"],
                contexts,
                request.difficulty  # base_difficultyë¡œ ì „ë‹¬
            )

            # 8ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦
            logger.info("STEP5: ë¬¸ì œ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            validation_result = self.quiz_validator.validate_quiz_quality(questions)

            # 9ë‹¨ê³„: ì‘ë‹µ ìƒì„±
            generation_time = time.time() - start_time

            response = QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=questions,
                total_questions=len(questions),
                difficulty=request.difficulty,
                generation_time=generation_time,
                success=True,
                metadata={
                    "extracted_topics": extracted_topics,
                    "user_hint_topics": [],
                    "contexts_used": len(contexts),
                    "avg_context_similarity": sum(c.similarity for c in contexts) / len(contexts),
                    "validation_result": validation_result,
                    "llm_model": self.llm_service.model_name,
                    "document_info": doc_info,
                    "generation_stats": {
                        "context_retrieval_count": len(contexts),
                        "topic_extraction_count": len(extracted_topics),
                        "question_types_used": [qt.value for qt in question_types]
                    }
                }
            )

            logger.info(f"í€´ì¦ˆ ìƒì„± ì™„ë£Œ: {len(questions)}ë¬¸ì œ ({generation_time:.2f}ì´ˆ)")
            return response

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {str(e)} ({error_time:.2f}ì´ˆ)")

            return QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=[],
                total_questions=0,
                difficulty=request.difficulty,
                generation_time=error_time,
                success=False,
                error=str(e)
            )

    def extract_topics(self, document_id: str) -> List[str]:
        """ë¬¸ì„œ í† í”½ ì¶”ì¶œ (ì™¸ë¶€ APIìš©)"""
        try:
            topic_analyses = self.topic_extractor.extract_document_topics(document_id)
            return [ta.topic for ta in topic_analyses]
        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def validate_question_quality(self, question: Question) -> bool:
        """ê°œë³„ ë¬¸ì œ í’ˆì§ˆ ê²€ì¦ (ì™¸ë¶€ APIìš©)"""
        question_dict = asdict(question)
        return self.llm_service.validate_question_quality(question_dict)

    def retrieve_topic_contexts(self, document_id: str, topic: str) -> List[Dict]:
        """íŠ¹ì • í† í”½ì˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì™¸ë¶€ APIìš©)"""
        try:
            contexts = self.rag_retriever.retrieve_contexts_for_quiz(
                document_id=document_id,
                num_questions=5,  # ê¸°ë³¸ê°’
                topics=[topic]
            )
            return [asdict(ctx) for ctx in contexts]
        except Exception as e:
            logger.error(f"í† í”½ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def switch_llm_model(self, llm_service: BaseLLMService):
        """LLM ëª¨ë¸ êµì²´"""
        old_model = self.llm_service.model_name
        self.llm_service = llm_service

        # í•˜ìœ„ ì»´í¬ë„ŒíŠ¸ë“¤ë„ ì—…ë°ì´íŠ¸
        self.topic_extractor.llm_service = llm_service
        self.quiz_validator.llm_service = llm_service

        logger.info(f"LLM ëª¨ë¸ êµì²´: {old_model} â†’ {llm_service.model_name}")

    def _combine_contexts(self, contexts: List[RAGContext]) -> str:
        """ì—¬ëŸ¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©"""
        combined = []

        for i, context in enumerate(contexts):
            section = f"[ì„¹ì…˜ {i+1}]\n{context.text}\n"
            combined.append(section)

        return "\n".join(combined)

    def _determine_question_types(self, request: QuizRequest) -> List[QuestionType]:
        """ë¬¸ì œ ìœ í˜• ê²°ì •"""
        if request.question_types:
            return request.question_types

        # ê¸°ë³¸ ë¬¸ì œ ìœ í˜• ì¡°í•© (ë‚œì´ë„ë³„)
        if request.difficulty == Difficulty.EASY:
            return [QuestionType.MULTIPLE_CHOICE, QuestionType.TRUE_FALSE]
        elif request.difficulty == Difficulty.MEDIUM:
            return [QuestionType.MULTIPLE_CHOICE, QuestionType.SHORT_ANSWER, QuestionType.FILL_BLANK]
        else:  # HARD
            return [QuestionType.SHORT_ANSWER, QuestionType.MULTIPLE_CHOICE, QuestionType.FILL_BLANK]

    def _convert_to_question_objects(
        self,
        llm_questions: List[Dict],
        contexts: List[RAGContext],
        base_difficulty: Difficulty
    ) -> List[Question]:
        """LLM ì‘ë‹µì„ Question ê°ì²´ë¡œ ë³€í™˜ (ë¬¸ì œë³„ ë‚œì´ë„ ë‹¤ì–‘í™”)"""
        questions = []

        for i, q_data in enumerate(llm_questions):
            try:
                # ë¬¸ì œ ìœ í˜• ë³€í™˜
                question_type = QuestionType(q_data.get("question_type", "multiple_choice"))

                # ğŸ“Š ë¬¸ì œë³„ ë‚œì´ë„ ìë™ í• ë‹¹ (ë‹¤ì–‘í™”)
                if len(llm_questions) >= 3:
                    # 3ë¬¸ì œ ì´ìƒì´ë©´ ë‚œì´ë„ ë¶„ì‚°
                    if i % 3 == 0:
                        difficulty = Difficulty.EASY
                    elif i % 3 == 1:
                        difficulty = base_difficulty  # ê¸°ë³¸ ë‚œì´ë„ ìœ ì§€
                    else:
                        difficulty = Difficulty.HARD
                else:
                    # 3ë¬¸ì œ ë¯¸ë§Œì´ë©´ ê¸°ë³¸ ë‚œì´ë„ ì‚¬ìš©
                    difficulty = base_difficulty

                # ì†ŒìŠ¤ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸°
                source_context = ""
                if i < len(contexts):
                    source_context = contexts[i].text[:200] + "..."

                question = Question(
                    question=q_data.get("question", ""),
                    question_type=question_type,
                    correct_answer=q_data.get("correct_answer", ""),
                    options=q_data.get("options"),
                    explanation=q_data.get("explanation", ""),
                    difficulty=difficulty,  # ê°œë³„ ë¬¸ì œ ë‚œì´ë„
                    source_context=source_context,
                    topic=q_data.get("topic", "ì¼ë°˜"),
                    metadata={
                        "llm_generated": True,
                        "context_similarity": contexts[i].similarity if i < len(contexts) else 0,
                        "generation_order": i + 1,
                        "assigned_difficulty": difficulty.value  # í• ë‹¹ëœ ë‚œì´ë„ ì¶”ê°€
                    }
                )

                questions.append(question)

            except Exception as e:
                logger.warning(f"ë¬¸ì œ {i+1} ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        return questions


# ì „ì—­ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_default_quiz_service: Optional[QuizService] = None


def get_default_quiz_service() -> QuizService:
    """ê¸°ë³¸ í€´ì¦ˆ ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _default_quiz_service

    if _default_quiz_service is None:
        _default_quiz_service = QuizService()
        logger.info("ê¸°ë³¸ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    return _default_quiz_service


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("=== í€´ì¦ˆ ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ===")

    try:
        quiz_service = QuizService()
        print(f"í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì„±ê³µ: LLM={quiz_service.llm_service.model_name}")
    except Exception as e:
        print(f"í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")