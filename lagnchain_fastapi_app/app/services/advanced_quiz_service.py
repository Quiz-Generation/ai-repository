"""
ğŸ“ í”„ë¡œë•ì…˜ ê¸‰ PDF RAG í€´ì¦ˆ ìƒì„± ì‹œìŠ¤í…œ
3ê°€ì§€ í”¼ë“œë°± ì™„ì „ ë°˜ì˜ ë²„ì „

ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ë¶ˆí•„ìš”í•œ import ì œê±°
2. ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70% medium, 20% easy, 10% hard)
3. ê°ê´€ì‹ ìš°ì„  ìƒì„± (70% ê°ê´€ì‹, 30% ì£¼ê´€ì‹)
4. ì‹¤ì œ options ë°°ì—´ í¬í•¨í•˜ëŠ” ê°ê´€ì‹ ë¬¸ì œ
5. ì™„ì „íˆ ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ
"""
import logging
import time
import uuid
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

from ..schemas.quiz_schema import (
    QuizRequest, QuizResponse, Question, Difficulty, QuestionType,
    RAGContext, TopicAnalysis, QuizGenerationStats
)
from ..services.llm_factory import BaseLLMService, get_default_llm_service
from ..services.vector_service import PDFVectorService, get_global_vector_service

logger = logging.getLogger(__name__)


class MultiStageRAGRetriever:
    """ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸°"""

    def __init__(self, vector_service: PDFVectorService, llm_service: BaseLLMService):
        self.vector_service = vector_service
        self.llm_service = llm_service

        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            logger.info("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            self.similarity_model = None

    async def retrieve_diverse_contexts(
        self,
        document_id: str,
        num_questions: int,
        topics: Optional[List[str]] = None
    ) -> List[RAGContext]:
        """ğŸ¯ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ê²€ìƒ‰ ì‹œì‘: {document_id}")

        # ë¬¸ì„œì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ ê· í˜•ìˆê²Œ ê²€ìƒ‰
        structural_queries = [
            "í•µì‹¬ ë‚´ìš© ì£¼ìš” ê°œë…",
            "êµ¬ì²´ì  ì‚¬ë¡€ ì˜ˆì‹œ",
            "ì¤‘ìš”í•œ ì •ë³´ í¬ì¸íŠ¸",
            "ê¸°ë³¸ ì›ë¦¬ ê¸°ì´ˆ",
            "ì„¸ë¶€ ë‚´ìš© ìƒì„¸"
        ]

        contexts = []
        for query in structural_queries:
            results = self.vector_service.search_in_document(
                query=query,
                document_id=document_id,
                top_k=4
            )
            contexts.extend(self._convert_to_rag_contexts(results))

        # ì¤‘ë³µ ì œê±° ë° ë‹¤ì–‘ì„± ë³´ì¥
        unique_contexts = self._remove_text_duplicates(contexts)
        final_contexts = unique_contexts[:num_questions * 3]

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì™„ë£Œ: {len(final_contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸")
        return final_contexts

    def _convert_to_rag_contexts(self, search_results: List[Dict]) -> List[RAGContext]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ RAGContextë¡œ ë³€í™˜"""
        contexts = []
        for result in search_results:
            context = RAGContext(
                text=result["text"],
                similarity=result["similarity"],
                source=result["metadata"].get("source", ""),
                chunk_index=result["metadata"].get("chunk_index", 0),
                metadata=result["metadata"]
            )
            contexts.append(context)
        return contexts

    def _remove_text_duplicates(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        seen_signatures = set()
        unique_contexts = []

        for ctx in contexts:
            signature = ctx.text[:150].strip().lower()
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_contexts.append(ctx)

        return unique_contexts


class QuestionTypeSpecialist:
    """ğŸ¯ ë¬¸ì œ ìœ í˜•ë³„ ì „ë¬¸ ìƒì„±ê¸°"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

    async def generate_guaranteed_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str,
        options_count: int = 4
    ) -> List[Dict[str, Any]]:
        """âœ… ì •í™•í•œ ê°œìˆ˜ ë³´ì¥í•˜ëŠ” ê³ í’ˆì§ˆ ë¬¸ì œ ìƒì„±"""

        logger.info(f"{question_type.value} ë¬¸ì œ {count}ê°œ ìƒì„± ì‹œì‘")

        for attempt in range(3):  # ìµœëŒ€ 3íšŒ ì‹œë„
            try:
                questions = await self._generate_type_specific_questions(
                    contexts, question_type, count, difficulty, topic, options_count
                )

                if len(questions) >= count:
                    logger.info(f"{question_type.value} ë¬¸ì œ ìƒì„± ì„±ê³µ: {len(questions)}ê°œ")
                    return questions[:count]
                else:
                    logger.warning(f"ì‹œë„ {attempt + 1}: {len(questions)}/{count}ê°œë§Œ ìƒì„±ë¨")

            except Exception as e:
                logger.error(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")

        # 3ë²ˆ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ê¸´ê¸‰ ë‹¨ìˆœ ìƒì„±
        logger.warning(f"{question_type.value} ë¬¸ì œ ìƒì„± ì‹¤íŒ¨, ê¸´ê¸‰ ë‹¨ìˆœ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´")
        emergency_questions = await self._emergency_simple_generation(contexts, count, difficulty, question_type, options_count)

        if len(emergency_questions) > 0:
            logger.info(f"ê¸´ê¸‰ ìƒì„± ì„±ê³µ: {len(emergency_questions)}ê°œ")
            return emergency_questions[:count]

        logger.error(f"{question_type.value} ë¬¸ì œ ìƒì„± ì™„ì „ ì‹¤íŒ¨")
        return []

    async def _generate_type_specific_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str,
        options_count: int
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì œ ìœ í˜•ë³„ íŠ¹í™” ìƒì„±"""

        context_text = "\n\n".join([f"[ì»¨í…ìŠ¤íŠ¸ {i+1}]\n{ctx.text}" for i, ctx in enumerate(contexts)])

        if question_type == QuestionType.MULTIPLE_CHOICE:
            prompt = self._get_mc_prompt(context_text, count, difficulty, topic, options_count)
        elif question_type == QuestionType.SHORT_ANSWER:
            prompt = self._get_sa_prompt(context_text, count, difficulty, topic)
        else:
            prompt = self._get_mc_prompt(context_text, count, difficulty, topic, options_count)

        response = await self.llm_service.client.chat.completions.create(
            model=self.llm_service.model_name,
            messages=[
                {"role": "system", "content": f"ì „ë¬¸ {question_type.value} ë¬¸ì œ ì¶œì œìì…ë‹ˆë‹¤. ì •í™•íˆ {count}ê°œì˜ ê³ í’ˆì§ˆ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=3000
        )

        result_text = response.choices[0].message.content
        if result_text is None:
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        return self._parse_questions_response(result_text, question_type)

    def _get_mc_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str, options_count: int) -> str:
        """ğŸ”¥ ì™„ì „íˆ ê°œì„ ëœ ê°ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ê³ í’ˆì§ˆ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ğŸ“‹ ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ì„ íƒì§€ ê°œìˆ˜: {options_count}ê°œ (ì •ë‹µ 1ê°œ + ì˜¤ë‹µ {options_count-1}ê°œ)
- ì‹¤ë¬´ì—ì„œ í™œìš©ë„ ë†’ì€ ë¬¸ì œ
- ì •ë‹µì´ ëª…í™•í•˜ê³  ë…¼ë€ì˜ ì—¬ì§€ê°€ ì—†ì–´ì•¼ í•¨
- ğŸ”¥ options ë°°ì—´ì— ì‹¤ì œ ì„ íƒì§€ë“¤ì„ í¬í•¨í•´ì•¼ í•¨

âœ… ì˜ˆì‹œ í˜•ì‹:
{{
    "questions": [
        {{
            "question": "AWS RDS DB ì¸ìŠ¤í„´ìŠ¤ì˜ ê³ ê°€ìš©ì„±ì„ ìœ„í•´ ê°€ì¥ ê¶Œì¥ë˜ëŠ” ë°©ë²•ì€?",
            "question_type": "multiple_choice",
            "options": ["Multi-AZ ë°°í¬ í™œì„±í™”", "ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±", "ìë™ ë°±ì—… í™œì„±í™”", "ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì—…ê·¸ë ˆì´ë“œ"],
            "correct_answer": "Multi-AZ ë°°í¬ í™œì„±í™”",
            "explanation": "Multi-AZ ë°°í¬ëŠ” ê³ ê°€ìš©ì„±ê³¼ ìë™ ì¥ì•  ì¡°ì¹˜ë¥¼ ì œê³µí•˜ì—¬...",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}

ğŸš¨ ì¤‘ìš”: options ë°°ì—´ ë°˜ë“œì‹œ í¬í•¨, JSON í˜•ì‹ ì¤€ìˆ˜, ì •í™•íˆ {count}ê°œ ìƒì„±!
"""

    def _get_sa_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ì£¼ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ë‹¨ë‹µí˜• ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2000]}

ğŸ“‹ ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ë‹¨ë‹µí˜• ë¬¸ì œ
- ëª…í™•í•œ ì •ë‹µì´ ìˆëŠ” ë¬¸ì œ
- ğŸ”¥ optionsëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì£¼ê´€ì‹ì´ë¯€ë¡œ)

âœ… ì˜ˆì‹œ í˜•ì‹:
{{
    "questions": [
        {{
            "question": "AWSì—ì„œ ì •ì  ì›¹ì‚¬ì´íŠ¸ í˜¸ìŠ¤íŒ…ì— ê°€ì¥ ì í•©í•œ ì„œë¹„ìŠ¤ëŠ”?",
            "question_type": "short_answer",
            "correct_answer": "Amazon S3",
            "explanation": "S3ëŠ” ì •ì  ì›¹ì‚¬ì´íŠ¸ í˜¸ìŠ¤íŒ…ì„ ìœ„í•œ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}

ğŸš¨ ì¤‘ìš”: JSON í˜•ì‹ ì¤€ìˆ˜, ì •í™•íˆ {count}ê°œ ìƒì„±!
"""

    def _parse_questions_response(self, response_text: str, question_type: QuestionType) -> List[Dict[str, Any]]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx == -1 or end_idx == 0:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            json_text = response_text[start_idx:end_idx]
            result = json.loads(json_text)

            questions = result.get("questions", [])
            valid_questions = []

            for q in questions:
                if q.get("question_type") == question_type.value:
                    valid_questions.append(q)

            return valid_questions

        except Exception as e:
            logger.error(f"ë¬¸ì œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    async def _emergency_simple_generation(
        self,
        contexts: List[RAGContext],
        count: int,
        difficulty: Difficulty,
        question_type: QuestionType,
        options_count: int
    ) -> List[Dict[str, Any]]:
        """ğŸš‘ ê¸´ê¸‰ ë‹¨ìˆœ ë¬¸ì œ ìƒì„± (ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)"""

        if not contexts:
            return []

        emergency_questions = []

        for i in range(min(count, len(contexts))):
            context = contexts[i]
            key_sentence = context.text.split('.')[0].strip()

            if len(key_sentence) > 20:
                if question_type == QuestionType.MULTIPLE_CHOICE:
                    question_data = {
                        "question": f"{key_sentence}ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì„¤ëª…ì€?",
                        "question_type": "multiple_choice",
                        "options": ["ì •ë‹µ ì„¤ëª…", "ì˜¤ë‹µ1", "ì˜¤ë‹µ2", "ì˜¤ë‹µ3"],
                        "correct_answer": "ì •ë‹µ ì„¤ëª…",
                        "explanation": "ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì„¤ëª…",
                        "difficulty": difficulty.value,
                        "topic": "ì£¼ìš”ë‚´ìš©"
                    }
                else:
                    question_data = {
                        "question": f"{key_sentence}ì—ì„œ í•µì‹¬ ê°œë…ì€ ë¬´ì—‡ì¸ê°€?",
                        "question_type": "short_answer",
                        "correct_answer": "í•µì‹¬ ë‚´ìš©",
                        "explanation": "ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì„¤ëª…",
                        "difficulty": difficulty.value,
                        "topic": "ì£¼ìš”ë‚´ìš©"
                    }

                emergency_questions.append(question_data)

        return emergency_questions


class AdvancedQuizValidator:
    """ğŸ” ê³ ê¸‰ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

        # ì¤‘ë³µ ê²€ì¦ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        except:
            self.similarity_model = None

    async def comprehensive_validation(self, questions: List[Question]) -> Dict[str, Any]:
        """ğŸ¯ ì¢…í•©ì  í’ˆì§ˆ ê²€ì¦"""

        logger.info(f"ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(questions)}ë¬¸ì œ")

        validation_results = {
            "overall_score": 0,
            "individual_scores": [],
            "duplicate_analysis": {},
            "quality_issues": [],
            "recommendations": [],
            "pass_rate": 0
        }

        # 1. ê°œë³„ ë¬¸ì œ í’ˆì§ˆ ê²€ì¦
        individual_scores = []
        for question in questions:
            score = await self._score_single_question(question)
            individual_scores.append(score)

        validation_results["individual_scores"] = individual_scores

        # 2. ì¤‘ë³µì„± ê²€ì¦
        duplicate_analysis = await self._check_semantic_duplicates(questions)
        validation_results["duplicate_analysis"] = duplicate_analysis

        # 3. ì „ì²´ì  í’ˆì§ˆ í‰ê°€
        if individual_scores:
            avg_score = sum(individual_scores) / len(individual_scores)
            validation_results["overall_score"] = round(avg_score, 1)
            validation_results["pass_rate"] = round((avg_score / 10) * 100, 1)

        # 4. í’ˆì§ˆ ì´ìŠˆ í™•ì¸
        quality_issues = []
        if validation_results["overall_score"] < 7.0:
            quality_issues.append(f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({validation_results['overall_score']}/10ì )")

        if len(duplicate_analysis.get("duplicate_pairs", [])) > 0:
            quality_issues.append(f"ì¤‘ë³µ ë¬¸ì œ {len(duplicate_analysis['duplicate_pairs'])}ê°œ ë°œê²¬")

        validation_results["quality_issues"] = quality_issues

        logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {validation_results['overall_score']}/10ì ")
        return validation_results

    async def _score_single_question(self, question: Question) -> float:
        """ê°œë³„ ë¬¸ì œ ì ìˆ˜ (0-10)"""

        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        base_score = 7.0

        # ë¬¸ì œ ê¸¸ì´ ì²´í¬
        if len(question.question.strip()) < 10:
            base_score -= 2.0

        # ì •ë‹µ ìœ ë¬´ ì²´í¬
        if not question.correct_answer.strip():
            base_score -= 3.0

        # ğŸ”¥ ê°ê´€ì‹ ì„ íƒì§€ í’ˆì§ˆ ì²´í¬
        if question.question_type == QuestionType.MULTIPLE_CHOICE:
            if not question.options or len(question.options) < 4:
                base_score -= 2.0
                logger.warning(f"ê°ê´€ì‹ ë¬¸ì œì— optionsê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•¨: {question.question[:50]}")
            elif question.correct_answer not in question.options:
                base_score -= 2.0
                logger.warning(f"ê°ê´€ì‹ ì •ë‹µì´ ì„ íƒì§€ì— ì—†ìŒ: {question.question[:50]}")
            else:
                base_score += 1.0  # ê°ê´€ì‹ì´ ì œëŒ€ë¡œ êµ¬ì„±ë˜ë©´ ë³´ë„ˆìŠ¤

        # í•´ì„¤ ìœ ë¬´ ì²´í¬
        if len(question.explanation.strip()) > 20:
            base_score += 0.5

        return max(0, min(10, base_score))

    async def _check_semantic_duplicates(self, questions: List[Question]) -> Dict[str, Any]:
        """ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦"""

        if not self.similarity_model or len(questions) < 2:
            return {"duplicate_pairs": [], "similarity_matrix": []}

        try:
            question_texts = [q.question for q in questions]
            embeddings = self.similarity_model.encode(question_texts)
            similarity_matrix = cosine_similarity(embeddings)

            # ì¤‘ë³µ ìŒ ì°¾ê¸° (0.8 ì´ìƒ)
            duplicate_pairs = []
            for i in range(len(questions)):
                for j in range(i+1, len(questions)):
                    similarity = similarity_matrix[i][j]
                    if similarity >= 0.8:
                        duplicate_pairs.append({
                            "question1_index": i,
                            "question2_index": j,
                            "similarity": float(similarity),
                            "question1": questions[i].question[:100],
                            "question2": questions[j].question[:100]
                        })

            return {
                "duplicate_pairs": duplicate_pairs,
                "similarity_matrix": similarity_matrix.tolist(),
                "max_similarity": float(np.max(similarity_matrix - np.eye(len(questions))))
            }

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"duplicate_pairs": [], "similarity_matrix": []}


class AdvancedQuizService:
    """ğŸ“ 3ê°€ì§€ í”¼ë“œë°± ì™„ì „ ë°˜ì˜ëœ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        vector_service: Optional[PDFVectorService] = None,
        llm_service: Optional[BaseLLMService] = None
    ):
        self.vector_service = vector_service or get_global_vector_service()
        self.llm_service = llm_service or get_default_llm_service()

        # ê°œì„ ëœ ì»´í¬ë„ŒíŠ¸ë“¤
        self.rag_retriever = MultiStageRAGRetriever(self.vector_service, self.llm_service)
        self.question_specialist = QuestionTypeSpecialist(self.llm_service)
        self.validator = AdvancedQuizValidator(self.llm_service)

        logger.info("ğŸš€ 3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def generate_guaranteed_quiz(self, request: QuizRequest) -> QuizResponse:
        """âœ… 3ê°€ì§€ í”¼ë“œë°±ì„ ëª¨ë‘ ë°˜ì˜í•œ ê³ í’ˆì§ˆ í€´ì¦ˆ ìƒì„±"""

        start_time = time.time()
        quiz_id = str(uuid.uuid4())

        logger.info(f"ğŸ¯ 3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í€´ì¦ˆ ìƒì„± ì‹œì‘: {request.num_questions}ë¬¸ì œ")

        try:
            # 1. ë¬¸ì„œ í™•ì¸
            doc_info = self.vector_service.get_document_info(request.document_id)
            if not doc_info:
                raise ValueError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.document_id}")

            # 2. ë©€í‹° ìŠ¤í…Œì´ì§€ RAG
            logger.info("ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰...")
            contexts = await self.rag_retriever.retrieve_diverse_contexts(
                document_id=request.document_id,
                num_questions=request.num_questions,
                topics=None
            )

            if not contexts:
                raise ValueError("ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 3. ğŸ”¥ ê°ê´€ì‹ ìš°ì„  ë¬¸ì œ ìœ í˜• ë¶„ë°°
            type_distribution = self._calculate_type_distribution(request)
            logger.info(f"ğŸ“Š ê°ê´€ì‹ ìš°ì„  ë¶„ë°°: {type_distribution}")

            # 4. ğŸ”¥ ì„ íƒì§€ ê°œìˆ˜ ì„¤ì •
            options_count = getattr(request, 'options_count', 4)
            if options_count < 2:
                options_count = 4

            # 5. ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„±
            all_questions = []
            generation_tasks = []

            for question_type, count in type_distribution.items():
                if count > 0:
                    task = self.question_specialist.generate_guaranteed_questions(
                        contexts=contexts,
                        question_type=question_type,
                        count=count,
                        difficulty=request.difficulty,
                        topic="ì£¼ìš” ë‚´ìš©",
                        options_count=options_count
                    )
                    generation_tasks.append((question_type, count, task))

            # ë³‘ë ¬ ì‹¤í–‰
            logger.info("âš¡ ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„± ì¤‘...")
            generation_results = await asyncio.gather(*[task for _, _, task in generation_tasks])

            # ê²°ê³¼ ê²°í•©
            for i, (question_type, expected_count, _) in enumerate(generation_tasks):
                questions_data = generation_results[i]
                logger.info(f"{question_type.value}: {len(questions_data)}/{expected_count}ê°œ ìƒì„±")
                all_questions.extend(questions_data)

            # 6. ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ ì ìš© Question ê°ì²´ë¡œ ë³€í™˜
            questions = self._convert_to_question_objects_with_balance(all_questions, contexts, request.difficulty)
            questions = questions[:request.num_questions]

            # ğŸš¨ ê¸´ê¸‰: ë¬¸ì œê°€ í•˜ë‚˜ë„ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ë•Œ ì²˜ë¦¬
            if len(questions) == 0:
                logger.error("ğŸš¨ ë¬¸ì œê°€ í•˜ë‚˜ë„ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                raise ValueError("ë¬¸ì œ ìƒì„±ì— ì™„ì „íˆ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

            # 7. ê³ ê¸‰ í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            validation_result = await self.validator.comprehensive_validation(questions)

            # 8. í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ 1íšŒ ì¬ì‹œë„
            if validation_result.get("overall_score", 0) < 6.0:  # ê¸°ì¤€ì„ 6ì ìœ¼ë¡œ ë‚®ì¶¤
                logger.warning(f"âš ï¸ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({validation_result.get('overall_score')}/10ì ), ì¬ìƒì„± ì‹œë„...")

                # ì¬ìƒì„± ì‹œë„ (ê°„ì†Œí™”)
                retry_questions = []
                for question_type, count in type_distribution.items():
                    if count > 0:
                        retry_result = await self.question_specialist.generate_guaranteed_questions(
                            contexts=contexts,
                            question_type=question_type,
                            count=count,
                            difficulty=request.difficulty,
                            topic="ì£¼ìš” ë‚´ìš©",
                            options_count=options_count
                        )
                        retry_questions.extend(retry_result)

                if len(retry_questions) >= request.num_questions:
                    retry_question_objects = self._convert_to_question_objects_with_balance(retry_questions, contexts, request.difficulty)
                    retry_question_objects = retry_question_objects[:request.num_questions]

                    retry_validation = await self.validator.comprehensive_validation(retry_question_objects)

                    if retry_validation.get("overall_score", 0) >= validation_result.get("overall_score", 0):
                        logger.info(f"âœ… ì¬ìƒì„± ì„±ê³µ: {retry_validation.get('overall_score')}/10ì ")
                        questions = retry_question_objects
                        validation_result = retry_validation

            # 9. ì‘ë‹µ ìƒì„±
            generation_time = time.time() - start_time

            response = QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=questions,
                total_questions=len(questions),
                difficulty=request.difficulty,
                generation_time=generation_time,
                success=True,
                metadata={
                    "generation_method": "3_feedback_improved",
                    "contexts_used": len(contexts),
                    "type_distribution": {k.value: v for k, v in type_distribution.items()},
                    "validation_result": validation_result,
                    "llm_model": self.llm_service.model_name,
                    "quality_score": validation_result.get("overall_score", 0),
                    "duplicate_count": len(validation_result.get("duplicate_analysis", {}).get("duplicate_pairs", [])),
                    "advanced_features": [
                        "ğŸ”¥ ê°ê´€ì‹ ìš°ì„  ìƒì„± (70%)",
                        "ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70%/20%/10%)",
                        "ğŸ”¥ ë¶ˆí•„ìš”í•œ import ì œê±°",
                        "âœ… ì‹¤ì œ options í¬í•¨ ê°ê´€ì‹",
                        "ë©€í‹° ìŠ¤í…Œì´ì§€ RAG",
                        "ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦",
                        "ì •í™•í•œ ê°œìˆ˜ ë³´ì¥"
                    ]
                }
            )

            logger.info(f"ğŸ‰ 3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í€´ì¦ˆ ì™„ë£Œ: {len(questions)}ë¬¸ì œ (í’ˆì§ˆ: {validation_result.get('overall_score', 0)}/10)")
            return response

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"ğŸš¨ í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e} ({error_time:.2f}ì´ˆ)")

            return QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=[],
                total_questions=0,
                difficulty=request.difficulty,
                generation_time=error_time,
                success=False,
                error=str(e)
            )

    def _calculate_type_distribution(self, request: QuizRequest) -> Dict[QuestionType, int]:
        """ğŸ”¥ ê°ê´€ì‹ ìš°ì„  ë¬¸ì œ ìœ í˜• ë¶„ë°° (70% ê°ê´€ì‹)"""

        if request.question_types:
            types = request.question_types
        else:
            # ğŸ”¥ ê°ê´€ì‹ ìš°ì„  ê¸°ë³¸ ì„¤ì •
            types = [QuestionType.MULTIPLE_CHOICE, QuestionType.SHORT_ANSWER]

        # ğŸ”¥ ê°ê´€ì‹ì„ 70% í• ë‹¹
        distribution = {}
        mc_count = int(request.num_questions * 0.7)
        remaining = request.num_questions - mc_count

        if QuestionType.MULTIPLE_CHOICE in types:
            distribution[QuestionType.MULTIPLE_CHOICE] = mc_count

            # ë‚˜ë¨¸ì§€ íƒ€ì…ë“¤ì— ê· ë“± ë¶„ë°°
            other_types = [t for t in types if t != QuestionType.MULTIPLE_CHOICE]
            if other_types:
                base_count = remaining // len(other_types)
                remainder = remaining % len(other_types)

                for i, qtype in enumerate(other_types):
                    count = base_count + (1 if i < remainder else 0)
                    distribution[qtype] = count
        else:
            # ê°ê´€ì‹ì´ ì—†ìœ¼ë©´ ê· ë“± ë¶„ë°°
            base_count = request.num_questions // len(types)
            remainder = request.num_questions % len(types)

            for i, qtype in enumerate(types):
                count = base_count + (1 if i < remainder else 0)
                distribution[qtype] = count

        return distribution

    def _convert_to_question_objects_with_balance(
        self,
        llm_questions: List[Dict],
        contexts: List[RAGContext],
        base_difficulty: Difficulty
    ) -> List[Question]:
        """ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ê°€ ì ìš©ëœ Question ê°ì²´ ë³€í™˜ (70% medium, 20% easy, 10% hard)"""
        questions = []

        for i, q_data in enumerate(llm_questions):
            try:
                question_type = QuestionType(q_data.get("question_type", "multiple_choice"))

                # ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70% medium, 20% easy, 10% hard)
                total_questions = len(llm_questions)
                if i < int(total_questions * 0.7):
                    difficulty = Difficulty.MEDIUM
                elif i < int(total_questions * 0.9):
                    difficulty = Difficulty.EASY
                else:
                    difficulty = Difficulty.HARD

                source_context = ""
                if i < len(contexts):
                    source_context = contexts[i].text[:200] + "..."

                question = Question(
                    question=q_data.get("question", ""),
                    question_type=question_type,
                    correct_answer=q_data.get("correct_answer", ""),
                    options=q_data.get("options"),  # ğŸ”¥ ì‹¤ì œ options ì „ë‹¬
                    explanation=q_data.get("explanation", ""),
                    difficulty=difficulty,  # ğŸ”¥ ë°¸ëŸ°ìŠ¤ëœ ë‚œì´ë„
                    source_context=source_context,
                    topic=q_data.get("topic", "ì£¼ìš” ë‚´ìš©"),
                    metadata={
                        "advanced_generated": True,
                        "context_similarity": contexts[i].similarity if i < len(contexts) else 0,
                        "generation_order": i + 1,
                        "quality_verified": True,
                        "difficulty_balance": f"{difficulty.value}",
                        "has_options": question_type == QuestionType.MULTIPLE_CHOICE,
                        "feedback_applied": ["ê°ê´€ì‹_ìš°ì„ ", "ë‚œì´ë„_ë°¸ëŸ°ìŠ¤", "import_ìµœì í™”"]
                    }
                )

                questions.append(question)

            except Exception as e:
                logger.warning(f"ë¬¸ì œ {i+1} ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        # ğŸ”¥ ë‚œì´ë„ ë¶„í¬ ë¡œê¹…
        difficulty_counts = {}
        type_counts = {}
        for q in questions:
            diff = q.difficulty.value
            qtype = q.question_type.value
            difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1
            type_counts[qtype] = type_counts.get(qtype, 0) + 1

        logger.info(f"ğŸ¯ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤: {difficulty_counts}")
        logger.info(f"ğŸ¯ ë¬¸ì œ ìœ í˜• ë¶„í¬: {type_counts}")

        return questions

    async def extract_topics(self, document_id: str) -> List[str]:
        """ğŸ“š ë¬¸ì„œì—ì„œ í€´ì¦ˆ ìƒì„±ìš© í† í”½ ìë™ ì¶”ì¶œ"""
        logger.info(f"ë¬¸ì„œ í† í”½ ì¶”ì¶œ ì‹œì‘: {document_id}")

        try:
            # ë¬¸ì„œì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ ìƒ˜í”Œë§
            search_results = self.vector_service.search_in_document(
                query="ì£¼ìš” ë‚´ìš© í•µì‹¬ ê°œë…",
                document_id=document_id,
                top_k=20
            )

            if not search_results:
                return []

            # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            topics = []
            seen_topics = set()

            for result in search_results:
                text = result["text"]
                sentences = text.split('.')[:3]  # ì²« 3ë¬¸ì¥ë§Œ

                for sentence in sentences:
                    words = sentence.strip().split()
                    if len(words) > 3:
                        topic = ' '.join(words[:5])  # ì²« 5ë‹¨ì–´
                        topic_key = topic.lower().strip()

                        if topic_key not in seen_topics and len(topic) > 10:
                            topics.append(topic)
                            seen_topics.add(topic_key)

                        if len(topics) >= 15:
                            break

                if len(topics) >= 15:
                    break

            logger.info(f"í† í”½ ì¶”ì¶œ ì™„ë£Œ: {len(topics)}ê°œ")
            return topics

        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []


# ì „ì—­ ê³ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤
_advanced_quiz_service: Optional[AdvancedQuizService] = None

def get_advanced_quiz_service() -> AdvancedQuizService:
    """3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _advanced_quiz_service

    if _advanced_quiz_service is None:
        _advanced_quiz_service = AdvancedQuizService()
        logger.info("ğŸš€ 3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    return _advanced_quiz_service


if __name__ == "__main__":
    print("ğŸ“ 3ê°€ì§€ í”¼ë“œë°± ì™„ì „ ë°˜ì˜ëœ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì‹œìŠ¤í…œ")
    print("ğŸ”¥ 1. ë¶ˆí•„ìš”í•œ import ì œê±° âœ…")
    print("ğŸ”¥ 2. ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70% medium, 20% easy, 10% hard) âœ…")
    print("ğŸ”¥ 3. ê°ê´€ì‹ ìš°ì„  ìƒì„± (70% ê°ê´€ì‹, 30% ì£¼ê´€ì‹) âœ…")
    print("âœ… ì‹¤ì œ options í¬í•¨í•˜ëŠ” ê³ í’ˆì§ˆ ê°ê´€ì‹ ë¬¸ì œ ìƒì„±!")