"""
ğŸ“ í”„ë¡œë•ì…˜ ê¸‰ PDF RAG í€´ì¦ˆ ìƒì„± ì‹œìŠ¤í…œ
ğŸ”¥ ì¤‘ë³µ ì™„ì „ ì œê±° + 2:6:2 ë¹„ìœ¨ ì ìš© ë²„ì „

ğŸ”¥ í•µì‹¬ ê°œì„ ì‚¬í•­:
1. ì¤‘ë³µ ë¬¸ì œ ì™„ì „ ì œê±° ì‹œìŠ¤í…œ
2. OX:ê°ê´€ì‹:ì£¼ê´€ì‹ = 2:6:2 ë¹„ìœ¨ (ê¸°ë³¸)
3. ì‚¬ìš©ì ì„ íƒ ê°€ëŠ¥ (ì „ë¶€ OX, ì „ë¶€ ê°ê´€ì‹, ì „ë¶€ ì£¼ê´€ì‹)
4. ê°•í™”ëœ ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦
"""
import logging
import time
import uuid
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

from ..schemas.quiz_schema import (
    QuizRequest, QuizResponse, Question, Difficulty, QuestionType,
    RAGContext, TopicAnalysis, QuizGenerationStats
)
from ..services.llm_factory import BaseLLMService, get_default_llm_service
from ..services.vector_service import PDFVectorService, get_global_vector_service

logger = logging.getLogger(__name__)


class MultiStageRAGRetriever:
    """ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸°"""

    def __init__(self, vector_service: PDFVectorService, llm_service: BaseLLMService):
        self.vector_service = vector_service
        self.llm_service = llm_service

        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            logger.info("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            self.similarity_model = None

    async def retrieve_diverse_contexts(
        self,
        document_id: str,
        num_questions: int,
        topics: Optional[List[str]] = None
    ) -> List[RAGContext]:
        """ğŸ¯ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ê²€ìƒ‰ ì‹œì‘: {document_id}")

        # ë¬¸ì„œì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ ê· í˜•ìˆê²Œ ê²€ìƒ‰
        structural_queries = [
            "í•µì‹¬ ë‚´ìš© ì£¼ìš” ê°œë…",
            "êµ¬ì²´ì  ì‚¬ë¡€ ì˜ˆì‹œ",
            "ì¤‘ìš”í•œ ì •ë³´ í¬ì¸íŠ¸",
            "ê¸°ë³¸ ì›ë¦¬ ê¸°ì´ˆ",
            "ì„¸ë¶€ ë‚´ìš© ìƒì„¸"
        ]

        contexts = []
        for query in structural_queries:
            results = self.vector_service.search_in_document(
                query=query,
                document_id=document_id,
                top_k=4
            )
            contexts.extend(self._convert_to_rag_contexts(results))

        # ì¤‘ë³µ ì œê±° ë° ë‹¤ì–‘ì„± ë³´ì¥
        unique_contexts = self._remove_text_duplicates(contexts)
        final_contexts = unique_contexts[:num_questions * 3]

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì™„ë£Œ: {len(final_contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸")
        return final_contexts

    def _convert_to_rag_contexts(self, search_results: List[Dict]) -> List[RAGContext]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ RAGContextë¡œ ë³€í™˜"""
        contexts = []
        for result in search_results:
            context = RAGContext(
                text=result["text"],
                similarity=result["similarity"],
                source=result["metadata"].get("source", ""),
                chunk_index=result["metadata"].get("chunk_index", 0),
                metadata=result["metadata"]
            )
            contexts.append(context)
        return contexts

    def _remove_text_duplicates(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        seen_signatures = set()
        unique_contexts = []

        for ctx in contexts:
            signature = ctx.text[:150].strip().lower()
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_contexts.append(ctx)

        return unique_contexts


class QuestionTypeSpecialist:
    """ğŸ¯ ë¬¸ì œ ìœ í˜•ë³„ ì „ë¬¸ ìƒì„±ê¸° (ì¤‘ë³µ ì œê±° ê°•í™”)"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service
        # ğŸ”¥ ìƒì„±ëœ ë¬¸ì œë“¤ ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        self.generated_questions_cache = []

    async def generate_guaranteed_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str,
        options_count: int = 4
    ) -> List[Dict[str, Any]]:
        """âœ… ì¤‘ë³µ ì œê±°ê°€ ê°•í™”ëœ ê³ í’ˆì§ˆ ë¬¸ì œ ìƒì„±"""

        logger.info(f"{question_type.value} ë¬¸ì œ {count}ê°œ ìƒì„± ì‹œì‘ (ì¤‘ë³µ ì œê±° ì ìš©)")

        for attempt in range(3):  # ìµœëŒ€ 3íšŒ ì‹œë„
            try:
                questions = await self._generate_type_specific_questions(
                    contexts, question_type, count, difficulty, topic, options_count
                )

                # ğŸ”¥ ì¤‘ë³µ ì œê±° ì ìš©
                unique_questions = self._remove_duplicates_from_generated(questions)

                if len(unique_questions) >= count:
                    logger.info(f"{question_type.value} ë¬¸ì œ ìƒì„± ì„±ê³µ: {len(unique_questions)}ê°œ (ì¤‘ë³µ ì œê±°ë¨)")
                    final_questions = unique_questions[:count]
                    # ìºì‹œì— ì¶”ê°€
                    self.generated_questions_cache.extend([q.get("question", "") for q in final_questions])
                    return final_questions
                else:
                    logger.warning(f"ì‹œë„ {attempt + 1}: {len(unique_questions)}/{count}ê°œë§Œ ìƒì„±ë¨ (ì¤‘ë³µ ì œê±° í›„)")

            except Exception as e:
                logger.error(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")

        # 3ë²ˆ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ê¸´ê¸‰ ë‹¨ìˆœ ìƒì„±
        logger.warning(f"{question_type.value} ë¬¸ì œ ìƒì„± ì‹¤íŒ¨, ê¸´ê¸‰ ë‹¨ìˆœ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´")
        emergency_questions = await self._emergency_simple_generation(contexts, count, difficulty, question_type, options_count)

        if len(emergency_questions) > 0:
            logger.info(f"ê¸´ê¸‰ ìƒì„± ì„±ê³µ: {len(emergency_questions)}ê°œ")
            return emergency_questions[:count]

        logger.error(f"{question_type.value} ë¬¸ì œ ìƒì„± ì™„ì „ ì‹¤íŒ¨")
        return []

    def _remove_duplicates_from_generated(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ğŸ”¥ ê°•í™”ëœ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ"""
        unique_questions = []
        seen_questions = []

        for q in questions:
            question_text = q.get("question", "").strip()

            # í˜„ì¬ ë¬¸ì œì™€ ì´ë¯¸ ì¶”ê°€ëœ ë¬¸ì œë“¤ ê°„ì˜ ìœ ì‚¬ë„ ê²€ì‚¬
            is_duplicate = False

            for seen_q in seen_questions:
                similarity = self._calculate_text_similarity_advanced(question_text, seen_q)
                if similarity > 0.6:  # 0.6 ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
                    is_duplicate = True
                    logger.info(f"ğŸš« ì¤‘ë³µ ì œê±°: ìœ ì‚¬ë„ {similarity:.3f}")
                    logger.info(f"   ê¸°ì¡´: {seen_q[:50]}...")
                    logger.info(f"   ì‹ ê·œ: {question_text[:50]}...")
                    break

            # ê¸°ì¡´ ìºì‹œì™€ë„ ë¹„êµ
            if not is_duplicate:
                for cached_q in self.generated_questions_cache:
                    similarity = self._calculate_text_similarity_advanced(question_text, cached_q)
                    if similarity > 0.6:
                        is_duplicate = True
                        logger.info(f"ğŸš« ìºì‹œì™€ ì¤‘ë³µ: ìœ ì‚¬ë„ {similarity:.3f}")
                        break

            if not is_duplicate:
                unique_questions.append(q)
                seen_questions.append(question_text)
                logger.debug(f"âœ… ê³ ìœ  ë¬¸ì œ ì¶”ê°€: {question_text[:50]}...")

        logger.info(f"ğŸ” ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(questions)}ê°œ â†’ {len(unique_questions)}ê°œ")
        return unique_questions

    def _calculate_text_similarity_advanced(self, text1: str, text2: str) -> float:
        """ğŸ”¥ ê³ ê¸‰ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (í•œêµ­ì–´ ìµœì í™”)"""
        if not text1 or not text2:
            return 0.0

        # ì •ê·œí™”
        import re

        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
        clean1 = re.sub(r'[^\w\sê°€-í£]', '', text1.lower().strip())
        clean2 = re.sub(r'[^\w\sê°€-í£]', '', text2.lower().strip())

        # ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°
        if clean1 == clean2:
            return 1.0

        # ë‹¨ì–´ ë‹¨ìœ„ ë¹„êµ
        words1 = set(clean1.split())
        words2 = set(clean2.split())

        if not words1 or not words2:
            return 0.0

        # Jaccard ìœ ì‚¬ë„
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        jaccard = len(intersection) / len(union) if union else 0.0

        # ê¸¸ì´ ìœ ì‚¬ë„
        len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))

        # í•µì‹¬ í‚¤ì›Œë“œ ë¹„êµ (AWS, EC2, S3 ë“±)
        keywords1 = set(re.findall(r'[A-Z]{2,}|AWS|EC2|S3|RDS|Lambda', text1))
        keywords2 = set(re.findall(r'[A-Z]{2,}|AWS|EC2|S3|RDS|Lambda', text2))

        keyword_similarity = 0.0
        if keywords1 or keywords2:
            keyword_intersection = keywords1.intersection(keywords2)
            keyword_union = keywords1.union(keywords2)
            keyword_similarity = len(keyword_intersection) / len(keyword_union) if keyword_union else 0.0

        # ìµœì¢… ìœ ì‚¬ë„ (ê°€ì¤‘ í‰ê· )
        final_similarity = (jaccard * 0.5 + len_ratio * 0.2 + keyword_similarity * 0.3)

        return final_similarity

    async def _generate_type_specific_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str,
        options_count: int
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì œ ìœ í˜•ë³„ íŠ¹í™” ìƒì„± (OX ë¬¸ì œ ì¶”ê°€)"""

        context_text = "\n\n".join([f"[ì»¨í…ìŠ¤íŠ¸ {i+1}]\n{ctx.text}" for i, ctx in enumerate(contexts)])

        if question_type == QuestionType.MULTIPLE_CHOICE:
            prompt = self._get_mc_prompt(context_text, count, difficulty, topic, options_count)
        elif question_type == QuestionType.SHORT_ANSWER:
            prompt = self._get_sa_prompt(context_text, count, difficulty, topic)
        elif question_type == QuestionType.TRUE_FALSE:
            prompt = self._get_tf_prompt(context_text, count, difficulty, topic)
        else:
            prompt = self._get_mc_prompt(context_text, count, difficulty, topic, options_count)

        response = await self.llm_service.client.chat.completions.create(
            model=self.llm_service.model_name,
            messages=[
                {"role": "system", "content": f"ì „ë¬¸ {question_type.value} ë¬¸ì œ ì¶œì œìì…ë‹ˆë‹¤. ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ ë¬¸ì œë¥¼ ì •í™•íˆ {count}ê°œ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,  # ğŸ”¥ ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì˜¨ë„ ì¡°ê¸ˆ ë†’ì„
            max_tokens=3000
        )

        result_text = response.choices[0].message.content
        if result_text is None:
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        return self._parse_questions_response(result_text, question_type)

    def _get_mc_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str, options_count: int) -> str:
        """ğŸ”¥ ì™„ì „íˆ ê°œì„ ëœ ê°ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ê³ í’ˆì§ˆ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ğŸ“‹ ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ì„ íƒì§€ ê°œìˆ˜: {options_count}ê°œ (ì •ë‹µ 1ê°œ + ì˜¤ë‹µ {options_count-1}ê°œ)
- ì‹¤ë¬´ì—ì„œ í™œìš©ë„ ë†’ì€ ë¬¸ì œ
- ì •ë‹µì´ ëª…í™•í•˜ê³  ë…¼ë€ì˜ ì—¬ì§€ê°€ ì—†ì–´ì•¼ í•¨
- ğŸ”¥ options ë°°ì—´ì— ì‹¤ì œ ì„ íƒì§€ë“¤ì„ í¬í•¨í•´ì•¼ í•¨

âœ… ì˜ˆì‹œ í˜•ì‹:
{{
    "questions": [
        {{
            "question": "AWS RDS DB ì¸ìŠ¤í„´ìŠ¤ì˜ ê³ ê°€ìš©ì„±ì„ ìœ„í•´ ê°€ì¥ ê¶Œì¥ë˜ëŠ” ë°©ë²•ì€?",
            "question_type": "multiple_choice",
            "options": ["Multi-AZ ë°°í¬ í™œì„±í™”", "ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±", "ìë™ ë°±ì—… í™œì„±í™”", "ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì—…ê·¸ë ˆì´ë“œ"],
            "correct_answer": "Multi-AZ ë°°í¬ í™œì„±í™”",
            "explanation": "Multi-AZ ë°°í¬ëŠ” ê³ ê°€ìš©ì„±ê³¼ ìë™ ì¥ì•  ì¡°ì¹˜ë¥¼ ì œê³µí•˜ì—¬...",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}

ğŸš¨ ì¤‘ìš”: options ë°°ì—´ ë°˜ë“œì‹œ í¬í•¨, JSON í˜•ì‹ ì¤€ìˆ˜, ì •í™•íˆ {count}ê°œ ìƒì„±!
"""

    def _get_sa_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ì£¼ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ë‹¨ë‹µí˜• ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2000]}

ğŸ“‹ ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ë‹¨ë‹µí˜• ë¬¸ì œ
- ëª…í™•í•œ ì •ë‹µì´ ìˆëŠ” ë¬¸ì œ
- ğŸ”¥ optionsëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì£¼ê´€ì‹ì´ë¯€ë¡œ)

âœ… ì˜ˆì‹œ í˜•ì‹:
{{
    "questions": [
        {{
            "question": "AWSì—ì„œ ì •ì  ì›¹ì‚¬ì´íŠ¸ í˜¸ìŠ¤íŒ…ì— ê°€ì¥ ì í•©í•œ ì„œë¹„ìŠ¤ëŠ”?",
            "question_type": "short_answer",
            "correct_answer": "Amazon S3",
            "explanation": "S3ëŠ” ì •ì  ì›¹ì‚¬ì´íŠ¸ í˜¸ìŠ¤íŒ…ì„ ìœ„í•œ ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}

ğŸš¨ ì¤‘ìš”: JSON í˜•ì‹ ì¤€ìˆ˜, ì •í™•íˆ {count}ê°œ ìƒì„±!
"""

    def _get_tf_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: OX ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ê³ í’ˆì§ˆ OX(ì°¸/ê±°ì§“) ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2500]}

ğŸ“‹ ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ëª…í™•í•˜ê²Œ ì°¸ ë˜ëŠ” ê±°ì§“ìœ¼ë¡œ êµ¬ë¶„ ê°€ëŠ¥í•œ ë¬¸ì œ
- ì• ë§¤í•˜ê±°ë‚˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ìˆëŠ” ë‚´ìš© í”¼í•˜ê¸°
- ğŸ”¥ ì •ë‹µì€ ë°˜ë“œì‹œ "True" ë˜ëŠ” "False"

âœ… ì˜ˆì‹œ í˜•ì‹:
{{
    "questions": [
        {{
            "question": "AWS EC2ëŠ” ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ… ì„œë¹„ìŠ¤ì´ë‹¤.",
            "question_type": "true_false",
            "correct_answer": "False",
            "explanation": "AWS EC2ëŠ” ê°€ìƒ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ë¡œ, ì„œë²„ë¦¬ìŠ¤ê°€ ì•„ë‹™ë‹ˆë‹¤. ì„œë²„ë¦¬ìŠ¤ëŠ” AWS Lambdaê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤.",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}

ğŸš¨ ì¤‘ìš”: ì •ë‹µì€ "True" ë˜ëŠ” "False"ë§Œ, JSON í˜•ì‹ ì¤€ìˆ˜, ì •í™•íˆ {count}ê°œ ìƒì„±!
"""

    def _parse_questions_response(self, response_text: str, question_type: QuestionType) -> List[Dict[str, Any]]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx == -1 or end_idx == 0:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            json_text = response_text[start_idx:end_idx]
            result = json.loads(json_text)

            questions = result.get("questions", [])
            valid_questions = []

            for q in questions:
                if q.get("question_type") == question_type.value:
                    valid_questions.append(q)

            return valid_questions

        except Exception as e:
            logger.error(f"ë¬¸ì œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    async def _emergency_simple_generation(
        self,
        contexts: List[RAGContext],
        count: int,
        difficulty: Difficulty,
        question_type: QuestionType,
        options_count: int
    ) -> List[Dict[str, Any]]:
        """ğŸš‘ ê¸´ê¸‰ ë‹¨ìˆœ ë¬¸ì œ ìƒì„± (ì‹¤ì œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)"""

        if not contexts:
            return []

        emergency_questions = []

        for i in range(min(count, len(contexts))):
            context = contexts[i]
            key_sentence = context.text.split('.')[0].strip()

            if len(key_sentence) > 20:
                if question_type == QuestionType.MULTIPLE_CHOICE:
                    question_data = {
                        "question": f"{key_sentence}ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì„¤ëª…ì€?",
                        "question_type": "multiple_choice",
                        "options": ["ì •ë‹µ ì„¤ëª…", "ì˜¤ë‹µ1", "ì˜¤ë‹µ2", "ì˜¤ë‹µ3"],
                        "correct_answer": "ì •ë‹µ ì„¤ëª…",
                        "explanation": "ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì„¤ëª…",
                        "difficulty": difficulty.value,
                        "topic": "ì£¼ìš”ë‚´ìš©"
                    }
                else:
                    question_data = {
                        "question": f"{key_sentence}ì—ì„œ í•µì‹¬ ê°œë…ì€ ë¬´ì—‡ì¸ê°€?",
                        "question_type": "short_answer",
                        "correct_answer": "í•µì‹¬ ë‚´ìš©",
                        "explanation": "ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì„¤ëª…",
                        "difficulty": difficulty.value,
                        "topic": "ì£¼ìš”ë‚´ìš©"
                    }

                emergency_questions.append(question_data)

        return emergency_questions


class AdvancedQuizValidator:
    """ğŸ” ê³ ê¸‰ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

        # ì¤‘ë³µ ê²€ì¦ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        except:
            self.similarity_model = None

    async def comprehensive_validation(self, questions: List[Question]) -> Dict[str, Any]:
        """ğŸ¯ ì¢…í•©ì  í’ˆì§ˆ ê²€ì¦"""

        logger.info(f"ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(questions)}ë¬¸ì œ")

        validation_results = {
            "overall_score": 0,
            "individual_scores": [],
            "duplicate_analysis": {},
            "quality_issues": [],
            "recommendations": [],
            "pass_rate": 0
        }

        # 1. ê°œë³„ ë¬¸ì œ í’ˆì§ˆ ê²€ì¦
        individual_scores = []
        for question in questions:
            score = await self._score_single_question(question)
            individual_scores.append(score)

        validation_results["individual_scores"] = individual_scores

        # 2. ì¤‘ë³µì„± ê²€ì¦
        duplicate_analysis = await self._check_semantic_duplicates(questions)
        validation_results["duplicate_analysis"] = duplicate_analysis

        # 3. ì „ì²´ì  í’ˆì§ˆ í‰ê°€
        if individual_scores:
            avg_score = sum(individual_scores) / len(individual_scores)
            validation_results["overall_score"] = round(avg_score, 1)
            validation_results["pass_rate"] = round((avg_score / 10) * 100, 1)

        # 4. í’ˆì§ˆ ì´ìŠˆ í™•ì¸
        quality_issues = []
        if validation_results["overall_score"] < 7.0:
            quality_issues.append(f"í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({validation_results['overall_score']}/10ì )")

        if len(duplicate_analysis.get("duplicate_pairs", [])) > 0:
            quality_issues.append(f"ì¤‘ë³µ ë¬¸ì œ {len(duplicate_analysis['duplicate_pairs'])}ê°œ ë°œê²¬")

        validation_results["quality_issues"] = quality_issues

        logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {validation_results['overall_score']}/10ì ")
        return validation_results

    async def _score_single_question(self, question: Question) -> float:
        """ê°œë³„ ë¬¸ì œ ì ìˆ˜ (0-10)"""

        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        base_score = 7.0

        # ë¬¸ì œ ê¸¸ì´ ì²´í¬
        if len(question.question.strip()) < 10:
            base_score -= 2.0

        # ì •ë‹µ ìœ ë¬´ ì²´í¬
        if not question.correct_answer.strip():
            base_score -= 3.0

        # ğŸ”¥ ê°ê´€ì‹ ì„ íƒì§€ í’ˆì§ˆ ì²´í¬
        if question.question_type == QuestionType.MULTIPLE_CHOICE:
            if not question.options or len(question.options) < 4:
                base_score -= 2.0
                logger.warning(f"ê°ê´€ì‹ ë¬¸ì œì— optionsê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•¨: {question.question[:50]}")
            elif question.correct_answer not in question.options:
                base_score -= 2.0
                logger.warning(f"ê°ê´€ì‹ ì •ë‹µì´ ì„ íƒì§€ì— ì—†ìŒ: {question.question[:50]}")
            else:
                base_score += 1.0  # ê°ê´€ì‹ì´ ì œëŒ€ë¡œ êµ¬ì„±ë˜ë©´ ë³´ë„ˆìŠ¤

        # í•´ì„¤ ìœ ë¬´ ì²´í¬
        if len(question.explanation.strip()) > 20:
            base_score += 0.5

        return max(0, min(10, base_score))

    async def _check_semantic_duplicates(self, questions: List[Question]) -> Dict[str, Any]:
        """ğŸ”¥ ê°•í™”ëœ ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦ (ì™„ì „ ì œê±° ì‹œìŠ¤í…œ)"""

        if len(questions) < 2:
            return {"duplicate_pairs": [], "similarity_matrix": [], "max_similarity": 0}

        try:
            question_texts = [q.question for q in questions]

            # ğŸ”¥ ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
            if self.similarity_model:
                embeddings = self.similarity_model.encode(question_texts)
                similarity_matrix = cosine_similarity(embeddings)
            else:
                # ì„ë² ë”© ëª¨ë¸ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„
                similarity_matrix = self._calculate_text_similarity_matrix(question_texts)

            # ğŸ”¥ ì¤‘ë³µ ìŒ ì°¾ê¸° (ì„ê³„ê°’ì„ 0.6ìœ¼ë¡œ ë‚®ì¶¤ - ë” ì—„ê²©)
            duplicate_pairs = []
            similar_pairs = []  # ìœ ì‚¬í•œ ë¬¸ì œë“¤ë„ ë³„ë„ ì¶”ì 

            for i in range(len(questions)):
                for j in range(i+1, len(questions)):
                    similarity = similarity_matrix[i][j]

                    if similarity >= 0.6:  # ì¤‘ë³µ ê¸°ì¤€
                        duplicate_pairs.append({
                            "question1_index": i,
                            "question2_index": j,
                            "similarity": float(similarity),
                            "question1": questions[i].question[:100],
                            "question2": questions[j].question[:100],
                            "type": "duplicate"
                        })
                    elif similarity >= 0.4:  # ìœ ì‚¬ ê¸°ì¤€
                        similar_pairs.append({
                            "question1_index": i,
                            "question2_index": j,
                            "similarity": float(similarity),
                            "question1": questions[i].question[:100],
                            "question2": questions[j].question[:100],
                            "type": "similar"
                        })

            max_similarity = float(np.max(similarity_matrix - np.eye(len(questions)))) if len(questions) > 1 else 0

            logger.info(f"ğŸ” ì¤‘ë³µ ê²€ì¦ ì™„ë£Œ: {len(duplicate_pairs)}ê°œ ì¤‘ë³µ, {len(similar_pairs)}ê°œ ìœ ì‚¬")

            return {
                "duplicate_pairs": duplicate_pairs,
                "similar_pairs": similar_pairs,
                "similarity_matrix": similarity_matrix.tolist(),
                "max_similarity": max_similarity,
                "quality_warning": len(duplicate_pairs) > 0 or len(similar_pairs) > 3
            }

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"duplicate_pairs": [], "similarity_matrix": [], "max_similarity": 0}

    def _calculate_text_similarity_matrix(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° (ì„ë² ë”© ëª¨ë¸ ì—†ì„ ë•Œ)"""
        n = len(texts)
        matrix = np.eye(n)

        for i in range(n):
            for j in range(i+1, n):
                similarity = self._calculate_text_similarity_advanced(texts[i], texts[j])
                matrix[i][j] = similarity
                matrix[j][i] = similarity

        return matrix


class AdvancedQuizService:
    """ğŸ“ ì¤‘ë³µ ì™„ì „ ì œê±° + 2:6:2 ë¹„ìœ¨ ì ìš©ëœ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        vector_service: Optional[PDFVectorService] = None,
        llm_service: Optional[BaseLLMService] = None
    ):
        self.vector_service = vector_service or get_global_vector_service()
        self.llm_service = llm_service or get_default_llm_service()

        # ê°œì„ ëœ ì»´í¬ë„ŒíŠ¸ë“¤
        self.rag_retriever = MultiStageRAGRetriever(self.vector_service, self.llm_service)
        self.question_specialist = QuestionTypeSpecialist(self.llm_service)
        self.validator = AdvancedQuizValidator(self.llm_service)

        logger.info("ğŸš€ ì¤‘ë³µ ì œê±° + 2:6:2 ë¹„ìœ¨ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def generate_guaranteed_quiz(self, request: QuizRequest) -> QuizResponse:
        """âœ… 3ê°€ì§€ í”¼ë“œë°±ì„ ëª¨ë‘ ë°˜ì˜í•œ ê³ í’ˆì§ˆ í€´ì¦ˆ ìƒì„±"""

        start_time = time.time()
        quiz_id = str(uuid.uuid4())

        logger.info(f"ğŸ¯ 3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í€´ì¦ˆ ìƒì„± ì‹œì‘: {request.num_questions}ë¬¸ì œ")

        try:
            # 1. ë¬¸ì„œ í™•ì¸
            doc_info = self.vector_service.get_document_info(request.document_id)
            if not doc_info:
                raise ValueError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.document_id}")

            # 2. ë©€í‹° ìŠ¤í…Œì´ì§€ RAG
            logger.info("ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰...")
            contexts = await self.rag_retriever.retrieve_diverse_contexts(
                document_id=request.document_id,
                num_questions=request.num_questions,
                topics=None
            )

            if not contexts:
                raise ValueError("ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 3. ğŸ”¥ ê°ê´€ì‹ ìš°ì„  ë¬¸ì œ ìœ í˜• ë¶„ë°°
            type_distribution = self._calculate_type_distribution(request)
            logger.info(f"ğŸ“Š ê°ê´€ì‹ ìš°ì„  ë¶„ë°°: {type_distribution}")

            # 4. ğŸ”¥ ì„ íƒì§€ ê°œìˆ˜ ì„¤ì •
            options_count = getattr(request, 'options_count', 4)
            if options_count < 2:
                options_count = 4

            # 5. ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„±
            all_questions = []
            generation_tasks = []

            for question_type, count in type_distribution.items():
                if count > 0:
                    task = self.question_specialist.generate_guaranteed_questions(
                        contexts=contexts,
                        question_type=question_type,
                        count=count,
                        difficulty=request.difficulty,
                        topic="ì£¼ìš” ë‚´ìš©",
                        options_count=options_count
                    )
                    generation_tasks.append((question_type, count, task))

            # ë³‘ë ¬ ì‹¤í–‰
            logger.info("âš¡ ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„± ì¤‘...")
            generation_results = await asyncio.gather(*[task for _, _, task in generation_tasks])

            # ê²°ê³¼ ê²°í•©
            for i, (question_type, expected_count, _) in enumerate(generation_tasks):
                questions_data = generation_results[i]
                logger.info(f"{question_type.value}: {len(questions_data)}/{expected_count}ê°œ ìƒì„±")
                all_questions.extend(questions_data)

            # 6. ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ ì ìš© Question ê°ì²´ë¡œ ë³€í™˜
            questions = self._convert_to_question_objects_with_balance(all_questions, contexts, request.difficulty)
            questions = questions[:request.num_questions]

            # ğŸš¨ ê¸´ê¸‰: ë¬¸ì œê°€ í•˜ë‚˜ë„ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ë•Œ ì²˜ë¦¬
            if len(questions) == 0:
                logger.error("ğŸš¨ ë¬¸ì œê°€ í•˜ë‚˜ë„ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                raise ValueError("ë¬¸ì œ ìƒì„±ì— ì™„ì „íˆ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

            # 7. ê³ ê¸‰ í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            validation_result = await self.validator.comprehensive_validation(questions)

            # 8. í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ 1íšŒ ì¬ì‹œë„
            if validation_result.get("overall_score", 0) < 6.0:  # ê¸°ì¤€ì„ 6ì ìœ¼ë¡œ ë‚®ì¶¤
                logger.warning(f"âš ï¸ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ({validation_result.get('overall_score')}/10ì ), ì¬ìƒì„± ì‹œë„...")

                # ì¬ìƒì„± ì‹œë„ (ê°„ì†Œí™”)
                retry_questions = []
                for question_type, count in type_distribution.items():
                    if count > 0:
                        retry_result = await self.question_specialist.generate_guaranteed_questions(
                            contexts=contexts,
                            question_type=question_type,
                            count=count,
                            difficulty=request.difficulty,
                            topic="ì£¼ìš” ë‚´ìš©",
                            options_count=options_count
                        )
                        retry_questions.extend(retry_result)

                if len(retry_questions) >= request.num_questions:
                    retry_question_objects = self._convert_to_question_objects_with_balance(retry_questions, contexts, request.difficulty)
                    retry_question_objects = retry_question_objects[:request.num_questions]

                    retry_validation = await self.validator.comprehensive_validation(retry_question_objects)

                    if retry_validation.get("overall_score", 0) >= validation_result.get("overall_score", 0):
                        logger.info(f"âœ… ì¬ìƒì„± ì„±ê³µ: {retry_validation.get('overall_score')}/10ì ")
                        questions = retry_question_objects
                        validation_result = retry_validation

            # 9. ì‘ë‹µ ìƒì„±
            generation_time = time.time() - start_time

            response = QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=questions,
                total_questions=len(questions),
                difficulty=request.difficulty,
                generation_time=generation_time,
                success=True,
                metadata={
                    "generation_method": "duplicate_free_2_6_2_ratio",
                    "contexts_used": len(contexts),
                    "type_distribution": {k.value: v for k, v in type_distribution.items()},
                    "validation_result": validation_result,
                    "llm_model": self.llm_service.model_name,
                    "quality_score": validation_result.get("overall_score", 0),
                    "duplicate_count": len(validation_result.get("duplicate_analysis", {}).get("duplicate_pairs", [])),
                    "similar_count": len(validation_result.get("duplicate_analysis", {}).get("similar_pairs", [])),
                    "max_similarity": validation_result.get("duplicate_analysis", {}).get("max_similarity", 0),
                    "advanced_features": [
                        "ğŸ”¥ ì™„ì „í•œ ì¤‘ë³µ ì œê±° ì‹œìŠ¤í…œ",
                        "ğŸ”¥ 2:6:2 ë¹„ìœ¨ (OX:ê°ê´€ì‹:ì£¼ê´€ì‹)",
                        "ğŸ”¥ ì‚¬ìš©ì ì„ íƒ ê°€ëŠ¥ (ì „ë¶€ OX/ê°ê´€ì‹/ì£¼ê´€ì‹)",
                        "ğŸ”¥ ê°•í™”ëœ ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦ (0.6 ì„ê³„ê°’)",
                        "âœ… ì‹¤ì œ options í¬í•¨ ê°ê´€ì‹",
                        "âœ… ì •í™•í•œ ê°œìˆ˜ ë³´ì¥",
                        "ë©€í‹° ìŠ¤í…Œì´ì§€ RAG",
                        "ë‚œì´ë„ ë°¸ëŸ°ìŠ¤"
                    ],
                    "ratio_applied": "2:6:2 (OX:ê°ê´€ì‹:ì£¼ê´€ì‹)" if not request.question_types else "ì‚¬ìš©ì ì§€ì •",
                    "duplicate_prevention": "ê°•í™”ëœ ì¤‘ë³µ ì œê±° ì ìš©",
                    "similarity_threshold": 0.6
                }
            )

            logger.info(f"ğŸ‰ ì¤‘ë³µ ì œê±° + 2:6:2 ë¹„ìœ¨ í€´ì¦ˆ ì™„ë£Œ: {len(questions)}ë¬¸ì œ (í’ˆì§ˆ: {validation_result.get('overall_score', 0)}/10)")
            return response

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"ğŸš¨ í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e} ({error_time:.2f}ì´ˆ)")

            return QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=[],
                total_questions=0,
                difficulty=request.difficulty,
                generation_time=error_time,
                success=False,
                error=str(e)
            )

    def _calculate_type_distribution(self, request: QuizRequest) -> Dict[QuestionType, int]:
        """ğŸ”¥ 2:6:2 ë¹„ìœ¨ ë¬¸ì œ ìœ í˜• ë¶„ë°° (OX:ê°ê´€ì‹:ì£¼ê´€ì‹)"""

        if request.question_types:
            types = request.question_types

            # ğŸ”¥ ì‚¬ìš©ìê°€ 1ê°œ íƒ€ì…ë§Œ ì§€ì •í•˜ë©´ 100% ê·¸ íƒ€ì…
            if len(types) == 1:
                return {types[0]: request.num_questions}

            # ğŸ”¥ ì—¬ëŸ¬ íƒ€ì… ì§€ì • ì‹œ ë¹„ìœ¨ ì ìš©
            distribution = {}

            # OX, ê°ê´€ì‹, ì£¼ê´€ì‹ì´ ëª¨ë‘ í¬í•¨ëœ ê²½ìš° 2:6:2 ë¹„ìœ¨
            if (QuestionType.TRUE_FALSE in types and
                QuestionType.MULTIPLE_CHOICE in types and
                QuestionType.SHORT_ANSWER in types):

                total = request.num_questions
                tf_count = max(1, int(total * 0.2))      # 20% OX
                mc_count = max(1, int(total * 0.6))      # 60% ê°ê´€ì‹
                sa_count = total - tf_count - mc_count   # ë‚˜ë¨¸ì§€ ì£¼ê´€ì‹

                distribution[QuestionType.TRUE_FALSE] = tf_count
                distribution[QuestionType.MULTIPLE_CHOICE] = mc_count
                distribution[QuestionType.SHORT_ANSWER] = sa_count

            else:
                # ì¼ë¶€ íƒ€ì…ë§Œ ìˆëŠ” ê²½ìš° ê· ë“± ë¶„ë°°
                base_count = request.num_questions // len(types)
                remainder = request.num_questions % len(types)

                for i, qtype in enumerate(types):
                    count = base_count + (1 if i < remainder else 0)
                    distribution[qtype] = count
        else:
            # ğŸ”¥ ê¸°ë³¸ ì„¤ì •: 2:6:2 ë¹„ìœ¨ (OX:ê°ê´€ì‹:ì£¼ê´€ì‹)
            total = request.num_questions
            tf_count = max(1, int(total * 0.2))      # 20% OX
            mc_count = max(1, int(total * 0.6))      # 60% ê°ê´€ì‹
            sa_count = total - tf_count - mc_count   # ë‚˜ë¨¸ì§€ ì£¼ê´€ì‹

            distribution = {
                QuestionType.TRUE_FALSE: tf_count,
                QuestionType.MULTIPLE_CHOICE: mc_count,
                QuestionType.SHORT_ANSWER: sa_count
            }

        logger.info(f"ğŸ¯ ë¬¸ì œ ìœ í˜• ë¶„ë°° (2:6:2 ê¸°ë³¸): {distribution}")
        return distribution

    def _convert_to_question_objects_with_balance(
        self,
        llm_questions: List[Dict],
        contexts: List[RAGContext],
        base_difficulty: Difficulty
    ) -> List[Question]:
        """ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ê°€ ì ìš©ëœ Question ê°ì²´ ë³€í™˜ (70% medium, 20% easy, 10% hard)"""
        questions = []

        for i, q_data in enumerate(llm_questions):
            try:
                question_type = QuestionType(q_data.get("question_type", "multiple_choice"))

                # ğŸ”¥ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70% medium, 20% easy, 10% hard)
                total_questions = len(llm_questions)
                if i < int(total_questions * 0.7):
                    difficulty = Difficulty.MEDIUM
                elif i < int(total_questions * 0.9):
                    difficulty = Difficulty.EASY
                else:
                    difficulty = Difficulty.HARD

                source_context = ""
                if i < len(contexts):
                    source_context = contexts[i].text[:200] + "..."

                question = Question(
                    question=q_data.get("question", ""),
                    question_type=question_type,
                    correct_answer=q_data.get("correct_answer", ""),
                    options=q_data.get("options"),  # ğŸ”¥ ì‹¤ì œ options ì „ë‹¬
                    explanation=q_data.get("explanation", ""),
                    difficulty=difficulty,  # ğŸ”¥ ë°¸ëŸ°ìŠ¤ëœ ë‚œì´ë„
                    source_context=source_context,
                    topic=q_data.get("topic", "ì£¼ìš” ë‚´ìš©"),
                    metadata={
                        "advanced_generated": True,
                        "context_similarity": contexts[i].similarity if i < len(contexts) else 0,
                        "generation_order": i + 1,
                        "quality_verified": True,
                        "difficulty_balance": f"{difficulty.value}",
                        "has_options": question_type == QuestionType.MULTIPLE_CHOICE,
                        "feedback_applied": ["ê°ê´€ì‹_ìš°ì„ ", "ë‚œì´ë„_ë°¸ëŸ°ìŠ¤", "import_ìµœì í™”"]
                    }
                )

                questions.append(question)

            except Exception as e:
                logger.warning(f"ë¬¸ì œ {i+1} ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        # ğŸ”¥ ë‚œì´ë„ ë¶„í¬ ë¡œê¹…
        difficulty_counts = {}
        type_counts = {}
        for q in questions:
            diff = q.difficulty.value
            qtype = q.question_type.value
            difficulty_counts[diff] = difficulty_counts.get(diff, 0) + 1
            type_counts[qtype] = type_counts.get(qtype, 0) + 1

        logger.info(f"ğŸ¯ ë‚œì´ë„ ë°¸ëŸ°ìŠ¤: {difficulty_counts}")
        logger.info(f"ğŸ¯ ë¬¸ì œ ìœ í˜• ë¶„í¬: {type_counts}")

        return questions

    async def extract_topics(self, document_id: str) -> List[str]:
        """ğŸ“š ë¬¸ì„œì—ì„œ í€´ì¦ˆ ìƒì„±ìš© í† í”½ ìë™ ì¶”ì¶œ"""
        logger.info(f"ë¬¸ì„œ í† í”½ ì¶”ì¶œ ì‹œì‘: {document_id}")

        try:
            # ë¬¸ì„œì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ ìƒ˜í”Œë§
            search_results = self.vector_service.search_in_document(
                query="ì£¼ìš” ë‚´ìš© í•µì‹¬ ê°œë…",
                document_id=document_id,
                top_k=20
            )

            if not search_results:
                return []

            # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            topics = []
            seen_topics = set()

            for result in search_results:
                text = result["text"]
                sentences = text.split('.')[:3]  # ì²« 3ë¬¸ì¥ë§Œ

                for sentence in sentences:
                    words = sentence.strip().split()
                    if len(words) > 3:
                        topic = ' '.join(words[:5])  # ì²« 5ë‹¨ì–´
                        topic_key = topic.lower().strip()

                        if topic_key not in seen_topics and len(topic) > 10:
                            topics.append(topic)
                            seen_topics.add(topic_key)

                        if len(topics) >= 15:
                            break

                if len(topics) >= 15:
                    break

            logger.info(f"í† í”½ ì¶”ì¶œ ì™„ë£Œ: {len(topics)}ê°œ")
            return topics

        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []


# ì „ì—­ ê³ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤
_advanced_quiz_service: Optional[AdvancedQuizService] = None

def get_advanced_quiz_service() -> AdvancedQuizService:
    """3ê°€ì§€ í”¼ë“œë°± ë°˜ì˜ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _advanced_quiz_service

    if _advanced_quiz_service is None:
        _advanced_quiz_service = AdvancedQuizService()
        logger.info("ğŸš€ ì¤‘ë³µ ì œê±° + 2:6:2 ë¹„ìœ¨ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    return _advanced_quiz_service


if __name__ == "__main__":
    print("ğŸ“ 3ê°€ì§€ í”¼ë“œë°± ì™„ì „ ë°˜ì˜ëœ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì‹œìŠ¤í…œ")
    print("ğŸ”¥ 1. ë¶ˆí•„ìš”í•œ import ì œê±° âœ…")
    print("ğŸ”¥ 2. ë‚œì´ë„ ë°¸ëŸ°ìŠ¤ (70% medium, 20% easy, 10% hard) âœ…")
    print("ğŸ”¥ 3. ê°ê´€ì‹ ìš°ì„  ìƒì„± (70% ê°ê´€ì‹, 30% ì£¼ê´€ì‹) âœ…")
    print("âœ… ì‹¤ì œ options í¬í•¨í•˜ëŠ” ê³ í’ˆì§ˆ ê°ê´€ì‹ ë¬¸ì œ ìƒì„±!")