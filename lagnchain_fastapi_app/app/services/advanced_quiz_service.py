"""
ğŸ“ í”„ë¡œë•ì…˜ ê¸‰ PDF RAG í€´ì¦ˆ ìƒì„± ì‹œìŠ¤í…œ
ì‹¤ì œ ëª¨ì˜ê³ ì‚¬/ìê²©ì¦/ì‹œí—˜ ë¬¸ì œ ìƒì„±ì— ìµœì í™”

ì£¼ìš” ê°œì„ ì‚¬í•­:
- ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ ë³´ì¥ (retry ë¡œì§)
- ë©€í‹° ìŠ¤í…Œì´ì§€ RAG (ì‹¬í™” ì»¨í…ìŠ¤íŠ¸ ë¶„ì„)
- ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦ (embedding ê¸°ë°˜)
- ì „ë¬¸ í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸
- ë¬¸ì œ ìœ í˜•ë³„ ì „ìš© ìƒì„±ê¸°
- ì»¨í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„± ë³´ì¥
"""
import logging
import time
import uuid
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import asdict
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

from ..schemas.quiz_schema import (
    QuizRequest, QuizResponse, Question, Difficulty, QuestionType,
    RAGContext, TopicAnalysis, QuizGenerationStats
)
from ..services.llm_factory import BaseLLMService, get_default_llm_service
from ..services.vector_service import PDFVectorService, get_global_vector_service

logger = logging.getLogger(__name__)


class MultiStageRAGRetriever:
    """ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸° (í”„ë¡œë•ì…˜ ê¸‰)"""

    def __init__(self, vector_service: PDFVectorService, llm_service: BaseLLMService):
        self.vector_service = vector_service
        self.llm_service = llm_service

        # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            logger.info("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            self.similarity_model = None

    async def retrieve_diverse_contexts(
        self,
        document_id: str,
        num_questions: int,
        topics: Optional[List[str]] = None
    ) -> List[RAGContext]:
        """ğŸ¯ ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ê²€ìƒ‰ ì‹œì‘: {document_id}")

        all_contexts = []

        # Stage 1: í† í”½ ê¸°ë°˜ ê²€ìƒ‰
        if topics:
            topic_contexts = await self._stage1_topic_search(document_id, topics, num_questions)
            all_contexts.extend(topic_contexts)

        # Stage 2: ë¬¸ì„œ êµ¬ì¡° ê¸°ë°˜ ê²€ìƒ‰ (ì„¹ì…˜ë³„)
        structural_contexts = await self._stage2_structural_search(document_id, num_questions)
        all_contexts.extend(structural_contexts)

        # Stage 3: ë™ì  í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        dynamic_contexts = await self._stage3_dynamic_search(document_id, num_questions)
        all_contexts.extend(dynamic_contexts)

        # Stage 4: í’ˆì§ˆ í•„í„°ë§ ë° ë‹¤ì–‘ì„± ë³´ì¥
        final_contexts = await self._stage4_quality_diversify(all_contexts, num_questions * 3)

        logger.info(f"ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì™„ë£Œ: {len(final_contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸")
        return final_contexts

    async def _stage1_topic_search(self, document_id: str, topics: List[str], num_per_topic: int) -> List[RAGContext]:
        """Stage 1: í† í”½ë³„ ì‹¬í™” ê²€ìƒ‰"""
        contexts = []

        for topic in topics:
            # ê° í† í”½ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_queries = [
                topic,
                f"{topic} ê°œë…",
                f"{topic} ì›ë¦¬",
                f"{topic} ë°©ë²•",
                f"{topic} íŠ¹ì§•"
            ]

            for query in search_queries[:3]:  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ
                results = self.vector_service.search_in_document(
                    query=query,
                    document_id=document_id,
                    top_k=2
                )
                contexts.extend(await self._convert_to_rag_contexts_async(results, topic))

        return contexts

    async def _stage2_structural_search(self, document_id: str, num_questions: int) -> List[RAGContext]:
        """Stage 2: ë¬¸ì„œ êµ¬ì¡°ì  ë‹¤ì–‘ì„± ê²€ìƒ‰"""

        # ë¬¸ì„œì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì—ì„œ ê· í˜•ìˆê²Œ ê²€ìƒ‰
        structural_queries = [
            "í•µì‹¬ ë‚´ìš© ì£¼ìš” ê°œë…",  # ì•ë¶€ë¶„
            "êµ¬ì²´ì  ì‚¬ë¡€ ì˜ˆì‹œ",      # ì¤‘ê°„ë¶€ë¶„
            "ê²°ë¡  ì •ë¦¬ ìš”ì•½",        # ë’·ë¶€ë¶„
            "ì¤‘ìš”í•œ ì •ë³´ í¬ì¸íŠ¸",    # ì „ë°˜ì 
            "ê¸°ë³¸ ì›ë¦¬ ê¸°ì´ˆ"         # ê¸°ë³¸ ê°œë…
        ]

        contexts = []
        for query in structural_queries:
            results = self.vector_service.search_in_document(
                query=query,
                document_id=document_id,
                top_k=2
            )
            contexts.extend(await self._convert_to_rag_contexts_async(results))

        return contexts

    async def _stage3_dynamic_search(self, document_id: str, num_questions: int) -> List[RAGContext]:
        """Stage 3: LLM ê¸°ë°˜ ë™ì  í‚¤ì›Œë“œ ê²€ìƒ‰"""

        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        sample_results = self.vector_service.search_in_document(
            query="ì£¼ìš” ë‚´ìš©",
            document_id=document_id,
            top_k=5
        )

        if not sample_results:
            return []

        sample_text = "\n".join([r["text"][:300] for r in sample_results])

        # LLMìœ¼ë¡œ íŠ¹í™” í‚¤ì›Œë“œ ìƒì„±
        prompt = f"""
ë‹¤ìŒ ë¬¸ì„œì—ì„œ ì‹œí—˜ ë¬¸ì œë¡œ ë§Œë“¤ê¸° ì¢‹ì€ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
- ì•”ê¸°ê°€ ì•„ë‹Œ ì´í•´/ì ìš© ì¤‘ì‹¬
- êµ¬ì²´ì ì´ê³  ì‹œí—˜ ì¶œì œ ê°€ëŠ¥í•œ ê°œë…
- ë¬¸ì„œì— ì‹¤ì œ ì„¤ëª…ëœ ë‚´ìš©ë§Œ

ë¬¸ì„œ ë‚´ìš©:
{sample_text[:2000]}

JSON í˜•ì‹: {{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}
"""

        try:
            response = await self.llm_service.client.chat.completions.create(
                model=self.llm_service.model_name,
                messages=[
                    {"role": "system", "content": "ì‹œí—˜ ì¶œì œ ì „ë¬¸ê°€ë¡œì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=300
            )

            import json
            result_text = response.choices[0].message.content
            start_idx = result_text.find('{')
            end_idx = result_text.rfind('}') + 1

            if start_idx != -1 and end_idx != 0:
                result = json.loads(result_text[start_idx:end_idx])
                keywords = result.get("keywords", [])

                contexts = []
                for keyword in keywords:
                    results = self.vector_service.search_in_document(
                        query=keyword,
                        document_id=document_id,
                        top_k=2
                    )
                    contexts.extend(await self._convert_to_rag_contexts_async(results))

                return contexts

        except Exception as e:
            logger.error(f"ë™ì  í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        return []

    async def _stage4_quality_diversify(self, contexts: List[RAGContext], target_count: int) -> List[RAGContext]:
        """Stage 4: í’ˆì§ˆ í•„í„°ë§ ë° ë‹¤ì–‘ì„± ë³´ì¥ (ë¹„ë™ê¸°)"""

        # 1. ê¸°ë³¸ í’ˆì§ˆ í•„í„°ë§
        quality_contexts = [
            ctx for ctx in contexts
            if len(ctx.text.strip()) >= 100 and ctx.similarity >= 0.1
        ]

        # 2. ì¤‘ë³µ ì œê±° (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        unique_contexts = await self._remove_text_duplicates_async(quality_contexts)

        # 3. ì˜ë¯¸ì  ë‹¤ì–‘ì„± ë³´ì¥
        diverse_contexts = await self._ensure_semantic_diversity_async(unique_contexts, target_count)

        # 4. ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
        diverse_contexts.sort(key=lambda x: x.similarity, reverse=True)

        return diverse_contexts[:target_count]

    async def _remove_text_duplicates_async(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ë¹„ë™ê¸°)"""
        seen_signatures = set()
        unique_contexts = []

        for ctx in contexts:
            # í…ìŠ¤íŠ¸ì˜ ì²« 150ìë¡œ ì‹œê·¸ë‹ˆì²˜ ìƒì„±
            signature = ctx.text[:150].strip().lower()
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_contexts.append(ctx)

        return unique_contexts

    async def _ensure_semantic_diversity_async(self, contexts: List[RAGContext], target_count: int) -> List[RAGContext]:
        """ì˜ë¯¸ì  ë‹¤ì–‘ì„± ë³´ì¥ (ë¹„ë™ê¸°)"""

        if not self.similarity_model or len(contexts) <= target_count:
            return contexts

        try:
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (CPU ì§‘ì•½ì  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ)
            import asyncio
            texts = [ctx.text[:500] for ctx in contexts]

            # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(None, self.similarity_model.encode, texts)

            # ë‹¤ì–‘ì„± ê¸°ë°˜ ì„ íƒ (greedy selection)
            selected_indices = [0]  # ì²« ë²ˆì§¸ëŠ” ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ê²ƒ

            while len(selected_indices) < target_count and len(selected_indices) < len(contexts):
                max_min_distance = -1
                best_candidate = -1

                for i, emb in enumerate(embeddings):
                    if i in selected_indices:
                        continue

                    # ì´ë¯¸ ì„ íƒëœ ê²ƒë“¤ê³¼ì˜ ìµœì†Œ ê±°ë¦¬ ê³„ì‚°
                    min_distance = min([
                        1 - cosine_similarity(np.array([emb]), np.array([embeddings[j]]))[0][0]
                        for j in selected_indices
                    ])

                    if min_distance > max_min_distance:
                        max_min_distance = min_distance
                        best_candidate = i

                if best_candidate != -1:
                    selected_indices.append(best_candidate)
                else:
                    break

            return [contexts[i] for i in selected_indices]

        except Exception as e:
            logger.error(f"ì˜ë¯¸ì  ë‹¤ì–‘ì„± ë³´ì¥ ì‹¤íŒ¨: {e}")
            return contexts[:target_count]

    async def _convert_to_rag_contexts_async(self, search_results: List[Dict], topic: Optional[str] = None) -> List[RAGContext]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ RAGContextë¡œ ë³€í™˜ (ë¹„ë™ê¸°)"""
        contexts = []

        for result in search_results:
            context = RAGContext(
                text=result["text"],
                similarity=result["similarity"],
                source=result["metadata"].get("source", ""),
                chunk_index=result["metadata"].get("chunk_index", 0),
                topic=topic,
                metadata=result["metadata"]
            )
            contexts.append(context)

        return contexts


class QuestionTypeSpecialist:
    """ğŸ¯ ë¬¸ì œ ìœ í˜•ë³„ ì „ë¬¸ ìƒì„±ê¸°"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

    async def generate_guaranteed_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str
    ) -> List[Dict[str, Any]]:
        """âœ… ì •í™•í•œ ê°œìˆ˜ ë³´ì¥í•˜ëŠ” ë¬¸ì œ ìƒì„± (ìµœëŒ€ 3íšŒ ì¬ì‹œë„)"""

        logger.info(f"{question_type.value} ë¬¸ì œ {count}ê°œ ìƒì„± ì‹œì‘")

        for attempt in range(3):  # ìµœëŒ€ 3íšŒ ì‹œë„
            try:
                questions = await self._generate_type_specific_questions(
                    contexts, question_type, count, difficulty, topic
                )

                if len(questions) >= count:
                    logger.info(f"{question_type.value} ë¬¸ì œ ìƒì„± ì„±ê³µ: {len(questions)}ê°œ")
                    return questions[:count]  # ì •í™•í•œ ê°œìˆ˜ë§Œ ë°˜í™˜
                else:
                    logger.warning(f"ì‹œë„ {attempt + 1}: {len(questions)}/{count}ê°œë§Œ ìƒì„±ë¨")

            except Exception as e:
                logger.error(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")

        # 3ë²ˆ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸ì œ ìƒì„±
        logger.error(f"{question_type.value} ë¬¸ì œ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ë¬¸ì œë¡œ ëŒ€ì²´")
        return await self._generate_fallback_questions(count, difficulty, topic)

    async def _generate_type_specific_questions(
        self,
        contexts: List[RAGContext],
        question_type: QuestionType,
        count: int,
        difficulty: Difficulty,
        topic: str
    ) -> List[Dict[str, Any]]:
        """ë¬¸ì œ ìœ í˜•ë³„ íŠ¹í™” ìƒì„±"""

        context_text = "\n\n".join([f"[ì»¨í…ìŠ¤íŠ¸ {i+1}]\n{ctx.text}" for i, ctx in enumerate(contexts)])

        # ë¬¸ì œ ìœ í˜•ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        type_prompts = {
            QuestionType.MULTIPLE_CHOICE: self._get_mc_prompt(context_text, count, difficulty, topic),
            QuestionType.SHORT_ANSWER: self._get_sa_prompt(context_text, count, difficulty, topic),
            QuestionType.FILL_BLANK: self._get_fb_prompt(context_text, count, difficulty, topic),
            QuestionType.TRUE_FALSE: self._get_tf_prompt(context_text, count, difficulty, topic)
        }

        prompt = type_prompts.get(question_type, type_prompts[QuestionType.MULTIPLE_CHOICE])

        response = await self.llm_service.client.chat.completions.create(
            model=self.llm_service.model_name,
            messages=[
                {"role": "system", "content": f"ì „ë¬¸ {question_type.value} ë¬¸ì œ ì¶œì œìì…ë‹ˆë‹¤. ì •í™•íˆ {count}ê°œì˜ ê³ í’ˆì§ˆ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=3000
        )

        result_text = response.choices[0].message.content
        if result_text is None:
            raise ValueError("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

        return self._parse_questions_response(result_text, question_type)

    def _get_mc_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ê°ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ê° ë¬¸ì œë§ˆë‹¤ ì •ë‹µ 1ê°œ + ê·¸ëŸ´ë“¯í•œ ì˜¤ë‹µ 3ê°œ
- ë‹¨ìˆœ ì•”ê¸°ê°€ ì•„ë‹Œ ì´í•´/ì ìš© ë¬¸ì œ
- ì •ë‹µì´ ëª…í™•í•˜ê³  ë…¼ë€ì˜ ì—¬ì§€ê°€ ì—†ì–´ì•¼ í•¨

JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ {count}ê°œ ìƒì„±:
{{
    "questions": [
        {{
            "question": "ë¬¸ì œ ë‚´ìš©",
            "question_type": "multiple_choice",
            "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
            "correct_answer": "ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}
"""

    def _get_sa_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ì£¼ê´€ì‹ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ë‹¨ë‹µí˜• ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ
- ëª…í™•í•œ ì •ë‹µì´ ìˆëŠ” ë¬¸ì œ
- ì„œìˆ í˜•ì´ ì•„ë‹Œ ë‹¨ë‹µí˜•

JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ {count}ê°œ ìƒì„±:
{{
    "questions": [
        {{
            "question": "ë¬¸ì œ ë‚´ìš©",
            "question_type": "short_answer",
            "correct_answer": "ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}
"""

    def _get_fb_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ë¹ˆì¹¸ ì±„ìš°ê¸° ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ë¹ˆì¹¸ ì±„ìš°ê¸° ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ë¬¸ì¥ì—ì„œ í•µì‹¬ ë‹¨ì–´/êµ¬ë¬¸ì„ ë¹ˆì¹¸ìœ¼ë¡œ ì²˜ë¦¬
- ë¹ˆì¹¸ì€ _____ ë¡œ í‘œì‹œ
- ë¬¸ë§¥ìƒ ì •ë‹µì´ ëª…í™•í•´ì•¼ í•¨

JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ {count}ê°œ ìƒì„±:
{{
    "questions": [
        {{
            "question": "ë¹ˆì¹¸ì´ í¬í•¨ëœ ë¬¸ì œ ë‚´ìš© _____",
            "question_type": "fill_blank",
            "correct_answer": "ë¹ˆì¹¸ì— ë“¤ì–´ê°ˆ ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}
"""

    def _get_tf_prompt(self, context: str, count: int, difficulty: Difficulty, topic: str) -> str:
        """ì°¸/ê±°ì§“ ë¬¸ì œ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì •í™•íˆ {count}ê°œ**ì˜ ì°¸/ê±°ì§“ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:3000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ì£¼ì œ: {topic}
- ëª…í™•íˆ ì°¸ ë˜ëŠ” ê±°ì§“ìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥
- ì• ë§¤í•˜ê±°ë‚˜ ë…¼ë€ì˜ ì—¬ì§€ê°€ ì—†ì–´ì•¼ í•¨
- íŠ¸ë¦­ ë¬¸ì œë³´ë‹¤ëŠ” ì •í™•í•œ ì´í•´ë¥¼ ë¬»ëŠ” ë¬¸ì œ

JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ {count}ê°œ ìƒì„±:
{{
    "questions": [
        {{
            "question": "ì°¸ ë˜ëŠ” ê±°ì§“ì„ íŒë‹¨í•  ëª…ì œ",
            "question_type": "true_false",
            "correct_answer": "ì°¸" ë˜ëŠ” "ê±°ì§“",
            "explanation": "ìƒì„¸í•œ í•´ì„¤",
            "difficulty": "{difficulty.value}",
            "topic": "{topic}"
        }}
    ]
}}
"""

    def _parse_questions_response(self, response_text: str, question_type: QuestionType) -> List[Dict[str, Any]]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx == -1 or end_idx == 0:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            json_text = response_text[start_idx:end_idx]
            result = json.loads(json_text)

            questions = result.get("questions", [])

            # ë¬¸ì œ ìœ í˜• ê²€ì¦
            valid_questions = []
            for q in questions:
                if q.get("question_type") == question_type.value:
                    valid_questions.append(q)

            return valid_questions

        except Exception as e:
            logger.error(f"ë¬¸ì œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    async def _generate_fallback_questions(self, count: int, difficulty: Difficulty, topic: str) -> List[Dict[str, Any]]:
        """fallback ê¸°ë³¸ ë¬¸ì œë“¤"""
        fallback_questions = []

        for i in range(count):
            question = {
                "question": f"{topic}ì— ê´€í•œ ê¸°ë³¸ ë¬¸ì œ {i+1}",
                "question_type": "multiple_choice",
                "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
                "correct_answer": "ì„ íƒì§€1",
                "explanation": "ê¸°ë³¸ ì„¤ëª…",
                "difficulty": difficulty.value,
                "topic": topic
            }
            fallback_questions.append(question)

        return fallback_questions


class AdvancedQuizValidator:
    """ğŸ” í”„ë¡œê¸‰ í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸"""

    def __init__(self, llm_service: BaseLLMService):
        self.llm_service = llm_service

        # ì¤‘ë³µ ê²€ì¦ìš© ì„ë² ë”© ëª¨ë¸
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
        except:
            self.similarity_model = None

    async def comprehensive_validation(self, questions: List[Question]) -> Dict[str, Any]:
        """ğŸ¯ ì¢…í•©ì  í’ˆì§ˆ ê²€ì¦"""

        logger.info(f"ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(questions)}ë¬¸ì œ")

        validation_results = {
            "overall_score": 0,
            "individual_scores": [],
            "duplicate_analysis": {},
            "quality_issues": [],
            "recommendations": [],
            "pass_rate": 0
        }

        # 1. ê°œë³„ ë¬¸ì œ í’ˆì§ˆ ê²€ì¦
        individual_results = await self._validate_individual_questions(questions)
        validation_results["individual_scores"] = individual_results

        # 2. ì¤‘ë³µì„± ê²€ì¦
        duplicate_analysis = await self._check_semantic_duplicates_async(questions)
        validation_results["duplicate_analysis"] = duplicate_analysis

        # 3. ì „ì²´ì  í’ˆì§ˆ í‰ê°€
        overall_assessment = await self._overall_quality_assessment(questions)
        validation_results.update(overall_assessment)

        logger.info(f"í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {validation_results['overall_score']}/10ì ")
        return validation_results

    async def _validate_individual_questions(self, questions: List[Question]) -> List[Dict[str, Any]]:
        """ê°œë³„ ë¬¸ì œ ìƒì„¸ ê²€ì¦"""
        results = []

        for i, question in enumerate(questions):
            score = await self._score_single_question(question)
            results.append({
                "question_index": i,
                "score": score,
                "issues": await self._identify_question_issues(question)
            })

        return results

    async def _score_single_question(self, question: Question) -> float:
        """ê°œë³„ ë¬¸ì œ ì ìˆ˜ (0-10)"""

        prompt = f"""
ë‹¤ìŒ í€´ì¦ˆ ë¬¸ì œì˜ í’ˆì§ˆì„ 0-10ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ë¬¸ì œ: {question.question}
ìœ í˜•: {question.question_type.value}
ì •ë‹µ: {question.correct_answer}
{f"ì„ íƒì§€: {question.options}" if question.options else ""}
í•´ì„¤: {question.explanation}

í‰ê°€ ê¸°ì¤€:
- ëª…í™•ì„±: ë¬¸ì œê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
- ì •í™•ì„±: ì •ë‹µì´ ëª…í™•í•˜ê³  ë…¼ë€ì˜ ì—¬ì§€ê°€ ì—†ëŠ”ê°€?
- êµìœ¡ì  ê°€ì¹˜: í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ”ê°€?
- ë‚œì´ë„ ì ì ˆì„±: ì„¤ì •ëœ ë‚œì´ë„ì— ë§ëŠ”ê°€?
- ì„ íƒì§€ í’ˆì§ˆ (ê°ê´€ì‹ì˜ ê²½ìš°): ì˜¤ë‹µì´ ê·¸ëŸ´ë“¯í•œê°€?

JSON í˜•ì‹: {{"score": ìˆ«ì, "reasoning": "í‰ê°€ ê·¼ê±°"}}
"""

        try:
            response = await self.llm_service.client.chat.completions.create(
                model=self.llm_service.model_name,
                messages=[
                    {"role": "system", "content": "ë¬¸ì œ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )

            result_text = response.choices[0].message.content
            if result_text is None:
                return 5.0

            import json
            start_idx = result_text.find('{')
            end_idx = result_text.rfind('}') + 1

            if start_idx != -1 and end_idx != 0:
                result = json.loads(result_text[start_idx:end_idx])
                return float(result.get("score", 5.0))

        except Exception as e:
            logger.error(f"ë¬¸ì œ ì ìˆ˜ í‰ê°€ ì‹¤íŒ¨: {e}")

        return 5.0  # ê¸°ë³¸ê°’

    async def _identify_question_issues(self, question: Question) -> List[str]:
        """ë¬¸ì œì  ì‹ë³„"""
        issues = []

        # ê¸°ë³¸ ê²€ì¦
        if len(question.question.strip()) < 10:
            issues.append("ë¬¸ì œê°€ ë„ˆë¬´ ì§§ìŒ")

        if not question.correct_answer.strip():
            issues.append("ì •ë‹µì´ ë¹„ì–´ìˆìŒ")

        # ê°ê´€ì‹ ì „ìš© ê²€ì¦
        if question.question_type == QuestionType.MULTIPLE_CHOICE:
            if not question.options or len(question.options) < 4:
                issues.append("ì„ íƒì§€ê°€ 4ê°œ ë¯¸ë§Œ")
            elif question.correct_answer not in question.options:
                issues.append("ì •ë‹µì´ ì„ íƒì§€ì— ì—†ìŒ")

        return issues

    async def _check_semantic_duplicates_async(self, questions: List[Question]) -> Dict[str, Any]:
        """ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦ (ë¹„ë™ê¸°)"""

        if not self.similarity_model or len(questions) < 2:
            return {"duplicate_pairs": [], "similarity_matrix": []}

        try:
            # ë¬¸ì œ í…ìŠ¤íŠ¸ ì„ë² ë”© (CPU ì§‘ì•½ì  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ)
            import asyncio
            question_texts = [q.question for q in questions]

            # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            embeddings = await loop.run_in_executor(None, self.similarity_model.encode, question_texts)

            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°ë„ ë¹„ë™ê¸°ë¡œ
            similarity_matrix = await loop.run_in_executor(None, cosine_similarity, embeddings)

            # ì¤‘ë³µ ìŒ ì°¾ê¸° (0.8 ì´ìƒ)
            duplicate_pairs = []
            for i in range(len(questions)):
                for j in range(i+1, len(questions)):
                    similarity = similarity_matrix[i][j]
                    if similarity >= 0.8:
                        duplicate_pairs.append({
                            "question1_index": i,
                            "question2_index": j,
                            "similarity": float(similarity),
                            "question1": questions[i].question[:100],
                            "question2": questions[j].question[:100]
                        })

            return {
                "duplicate_pairs": duplicate_pairs,
                "similarity_matrix": similarity_matrix.tolist(),
                "max_similarity": float(np.max(similarity_matrix - np.eye(len(questions))))
            }

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {"duplicate_pairs": [], "similarity_matrix": []}

    async def _overall_quality_assessment(self, questions: List[Question]) -> Dict[str, Any]:
        """ì „ì²´ì  í’ˆì§ˆ í‰ê°€"""

        # ê¸°ë³¸ í†µê³„
        total_questions = len(questions)
        if total_questions == 0:
            return {"overall_score": 0, "pass_rate": 0, "recommendations": ["ë¬¸ì œê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ"]}

        # ë¬¸ì œ ìœ í˜•ë³„ ë¶„í¬
        type_distribution = {}
        for q in questions:
            qtype = q.question_type.value
            type_distribution[qtype] = type_distribution.get(qtype, 0) + 1

        # ë‚œì´ë„ë³„ ë¶„í¬
        difficulty_distribution = {}
        for q in questions:
            diff = q.difficulty.value
            difficulty_distribution[diff] = difficulty_distribution.get(diff, 0) + 1

        # ì¢…í•© í‰ê°€
        quality_issues = []
        recommendations = []

        # ë‹¤ì–‘ì„± ì²´í¬
        if len(type_distribution) == 1:
            quality_issues.append("ë¬¸ì œ ìœ í˜•ì´ ë‹¨ì¡°ë¡œì›€")
            recommendations.append("ë‹¤ì–‘í•œ ë¬¸ì œ ìœ í˜• ì¶”ê°€ ê¶Œì¥")

        if len(difficulty_distribution) == 1:
            quality_issues.append("ë‚œì´ë„ê°€ ë‹¨ì¡°ë¡œì›€")
            recommendations.append("ë‹¤ì–‘í•œ ë‚œì´ë„ ë¬¸ì œ ì¶”ê°€ ê¶Œì¥")

        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (0-10)
        base_score = min(10, total_questions * 2)  # ê¸°ë³¸ì ìˆ˜
        penalty = len(quality_issues) * 0.5  # ê°ì 
        overall_score = max(0, base_score - penalty)

        pass_rate = min(100, (total_questions / max(1, len(quality_issues))) * 20)

        return {
            "overall_score": round(overall_score, 1),
            "pass_rate": round(pass_rate, 1),
            "quality_issues": quality_issues,
            "recommendations": recommendations,
            "type_distribution": type_distribution,
            "difficulty_distribution": difficulty_distribution
        }


class AdvancedQuizService:
    """ğŸš€ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ìƒì„± ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        vector_service: Optional[PDFVectorService] = None,
        llm_service: Optional[BaseLLMService] = None
    ):
        self.vector_service = vector_service or get_global_vector_service()
        self.llm_service = llm_service or get_default_llm_service()

        # í”„ë¡œê¸‰ ì»´í¬ë„ŒíŠ¸ë“¤
        self.rag_retriever = MultiStageRAGRetriever(self.vector_service, self.llm_service)
        self.question_specialist = QuestionTypeSpecialist(self.llm_service)
        self.validator = AdvancedQuizValidator(self.llm_service)

        logger.info("ğŸš€ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def generate_guaranteed_quiz(self, request: QuizRequest) -> QuizResponse:
        """âœ… ì •í™•í•œ ê°œìˆ˜ì™€ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” í€´ì¦ˆ ìƒì„±"""

        start_time = time.time()
        quiz_id = str(uuid.uuid4())

        logger.info(f"ğŸ¯ í”„ë¡œê¸‰ í€´ì¦ˆ ìƒì„± ì‹œì‘: {request.num_questions}ë¬¸ì œ")

        try:
            # 1. ë¬¸ì„œ í™•ì¸
            doc_info = self.vector_service.get_document_info(request.document_id)
            if not doc_info:
                raise ValueError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.document_id}")

            # 2. ë©€í‹° ìŠ¤í…Œì´ì§€ RAG
            logger.info("ğŸ§  ë©€í‹° ìŠ¤í…Œì´ì§€ RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰...")
            contexts = await self.rag_retriever.retrieve_diverse_contexts(
                document_id=request.document_id,
                num_questions=request.num_questions,
                topics=None  # ìë™ ì¶”ì¶œ
            )

            if not contexts:
                raise ValueError("ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 3. ë¬¸ì œ ìœ í˜•ë³„ ì •í™•í•œ ê°œìˆ˜ ë¶„ë°°
            type_distribution = self._calculate_type_distribution(request)
            logger.info(f"ğŸ“Š ë¬¸ì œ ìœ í˜• ë¶„ë°°: {type_distribution}")

            # 4. ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„±
            all_questions = []
            generation_tasks = []

            for question_type, count in type_distribution.items():
                if count > 0:
                    task = self.question_specialist.generate_guaranteed_questions(
                        contexts=contexts,
                        question_type=question_type,
                        count=count,
                        difficulty=request.difficulty,
                        topic="ì£¼ìš” ë‚´ìš©"
                    )
                    generation_tasks.append((question_type, count, task))

            # ë³‘ë ¬ ì‹¤í–‰
            logger.info("âš¡ ë¬¸ì œ ìœ í˜•ë³„ ë³‘ë ¬ ìƒì„± ì¤‘...")
            generation_results = await asyncio.gather(*[task for _, _, task in generation_tasks])

            # ê²°ê³¼ ê²°í•©
            for i, (question_type, expected_count, _) in enumerate(generation_tasks):
                questions_data = generation_results[i]
                logger.info(f"{question_type.value}: {len(questions_data)}/{expected_count}ê°œ ìƒì„±")
                all_questions.extend(questions_data)

            # 5. Question ê°ì²´ë¡œ ë³€í™˜
            questions = self._convert_to_question_objects(all_questions, contexts, request.difficulty)

            # 6. ì •í™•í•œ ê°œìˆ˜ ë³´ì¥
            if len(questions) < request.num_questions:
                logger.warning(f"ë¶€ì¡±í•œ ë¬¸ì œ ê°œìˆ˜: {len(questions)}/{request.num_questions}")
                # ì¶”ê°€ ìƒì„± ë¡œì§ í•„ìš” ì‹œ ì—¬ê¸°ì„œ ì²˜ë¦¬

            questions = questions[:request.num_questions]  # ì •í™•í•œ ê°œìˆ˜ë§Œ

            # 7. ê³ ê¸‰ í’ˆì§ˆ ê²€ì¦
            logger.info("ğŸ” ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            validation_result = await self.validator.comprehensive_validation(questions)

            # 8. ì‘ë‹µ ìƒì„±
            generation_time = time.time() - start_time

            response = QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=questions,
                total_questions=len(questions),
                difficulty=request.difficulty,
                generation_time=generation_time,
                success=True,
                metadata={
                    "generation_method": "advanced_multi_stage",
                    "contexts_used": len(contexts),
                    "type_distribution": {k.value: v for k, v in type_distribution.items()},
                    "validation_result": validation_result,
                    "llm_model": self.llm_service.model_name,
                    "quality_score": validation_result.get("overall_score", 0),
                    "duplicate_count": len(validation_result.get("duplicate_analysis", {}).get("duplicate_pairs", [])),
                    "advanced_features": [
                        "ë©€í‹° ìŠ¤í…Œì´ì§€ RAG",
                        "ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦",
                        "ë¬¸ì œ ìœ í˜•ë³„ ì „ë¬¸ ìƒì„±",
                        "ì •í™•í•œ ê°œìˆ˜ ë³´ì¥",
                        "í”„ë¡œê¸‰ í’ˆì§ˆ ê²€ì¦"
                    ]
                }
            )

            logger.info(f"ğŸ‰ í”„ë¡œê¸‰ í€´ì¦ˆ ìƒì„± ì™„ë£Œ: {len(questions)}ë¬¸ì œ (í’ˆì§ˆ: {validation_result.get('overall_score', 0)}/10)")
            return response

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"ğŸš¨ í”„ë¡œê¸‰ í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e} ({error_time:.2f}ì´ˆ)")

            return QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=[],
                total_questions=0,
                difficulty=request.difficulty,
                generation_time=error_time,
                success=False,
                error=str(e)
            )

    def _calculate_type_distribution(self, request: QuizRequest) -> Dict[QuestionType, int]:
        """ë¬¸ì œ ìœ í˜•ë³„ ì •í™•í•œ ê°œìˆ˜ ë¶„ë°°"""

        if request.question_types:
            # ì‚¬ìš©ìê°€ ì§€ì •í•œ ìœ í˜•ë“¤
            types = request.question_types
        else:
            # ë‚œì´ë„ë³„ ê¸°ë³¸ ìœ í˜•
            if request.difficulty == Difficulty.EASY:
                types = [QuestionType.MULTIPLE_CHOICE, QuestionType.TRUE_FALSE]
            elif request.difficulty == Difficulty.MEDIUM:
                types = [QuestionType.MULTIPLE_CHOICE, QuestionType.SHORT_ANSWER]
            else:
                types = [QuestionType.MULTIPLE_CHOICE, QuestionType.SHORT_ANSWER, QuestionType.FILL_BLANK]

        # ê· ë“± ë¶„ë°°
        base_count = request.num_questions // len(types)
        remainder = request.num_questions % len(types)

        distribution = {}
        for i, qtype in enumerate(types):
            count = base_count + (1 if i < remainder else 0)
            distribution[qtype] = count

        return distribution

    def _convert_to_question_objects(
        self,
        llm_questions: List[Dict],
        contexts: List[RAGContext],
        base_difficulty: Difficulty
    ) -> List[Question]:
        """Question ê°ì²´ë¡œ ë³€í™˜"""
        questions = []

        for i, q_data in enumerate(llm_questions):
            try:
                question_type = QuestionType(q_data.get("question_type", "multiple_choice"))
                difficulty = Difficulty(q_data.get("difficulty", base_difficulty.value))

                source_context = ""
                if i < len(contexts):
                    source_context = contexts[i].text[:200] + "..."

                question = Question(
                    question=q_data.get("question", ""),
                    question_type=question_type,
                    correct_answer=q_data.get("correct_answer", ""),
                    options=q_data.get("options"),
                    explanation=q_data.get("explanation", ""),
                    difficulty=difficulty,
                    source_context=source_context,
                    topic=q_data.get("topic", "ì£¼ìš” ë‚´ìš©"),
                    metadata={
                        "advanced_generated": True,
                        "context_similarity": contexts[i].similarity if i < len(contexts) else 0,
                        "generation_order": i + 1,
                        "quality_verified": True
                    }
                )

                questions.append(question)

            except Exception as e:
                logger.warning(f"ë¬¸ì œ {i+1} ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        return questions


# ì „ì—­ ê³ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤
_advanced_quiz_service: Optional[AdvancedQuizService] = None

def get_advanced_quiz_service() -> AdvancedQuizService:
    """í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _advanced_quiz_service

    if _advanced_quiz_service is None:
        _advanced_quiz_service = AdvancedQuizService()
        logger.info("ğŸš€ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    return _advanced_quiz_service


if __name__ == "__main__":
    print("ğŸš€ í”„ë¡œë•ì…˜ ê¸‰ í€´ì¦ˆ ìƒì„± ì‹œìŠ¤í…œ")
    print("- ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ ë³´ì¥")
    print("- ë©€í‹° ìŠ¤í…Œì´ì§€ RAG")
    print("- ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì¦")
    print("- ë¬¸ì œ ìœ í˜•ë³„ ì „ë¬¸ ìƒì„±")
    print("- í”„ë¡œê¸‰ í’ˆì§ˆ ê²€ì¦")