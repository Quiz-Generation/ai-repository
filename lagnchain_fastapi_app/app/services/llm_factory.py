"""
LLM íŒ©í† ë¦¬ íŒ¨í„´ êµ¬í˜„
ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ ì¶”ìƒí™”í•˜ì—¬ ì‰½ê²Œ êµì²´í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„
"""
import os
import logging
import json
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from enum import Enum
from lagnchain_fastapi_app.app.core.config import get_settings
from lagnchain_fastapi_app.app.schemas.quiz_schema import LLMConfig, LLMProvider

logger = logging.getLogger(__name__)




class BaseLLMService(ABC):
    """LLM ì„œë¹„ìŠ¤ ì¶”ìƒ í´ë˜ìŠ¤"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.provider = config.provider
        self.model_name = config.model_name

    @abstractmethod
    def generate_quiz(
        self,
        context: str,
        num_questions: int,
        difficulty: str,
        question_types: List[str],
        topics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """í€´ì¦ˆ ìƒì„± (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass

    @abstractmethod
    def extract_topics(self, context: str) -> List[str]:
        """ë¬¸ì„œì—ì„œ ì£¼ìš” í† í”½ ì¶”ì¶œ (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass

    @abstractmethod
    def validate_question_quality(self, question_data: Dict[str, Any]) -> bool:
        """ë¬¸ì œ í’ˆì§ˆ ê²€ì¦ (ì¶”ìƒ ë©”ì„œë“œ)"""
        pass


class OpenAILLMService(BaseLLMService):
    """OpenAI GPT ê¸°ë°˜ LLM ì„œë¹„ìŠ¤"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._setup_client()

    def _setup_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            import openai
            settings = get_settings()
            self.client = openai.OpenAI(
                api_key=self.config.api_key or settings.OPENAI_API_KEY
            )
            logger.info(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.model_name}")
        except ImportError:
            raise ImportError("OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: pip install openai")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def generate_quiz(
        self,
        context: str,
        num_questions: int,
        difficulty: str,
        question_types: List[str],
        topics: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """OpenAIë¥¼ ì‚¬ìš©í•œ í€´ì¦ˆ ìƒì„±"""

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_quiz_generation_prompt(
            context, num_questions, difficulty, question_types, topics
        )

        try:
            logger.info(f"OpenAI í€´ì¦ˆ ìƒì„± ì‹œì‘: {num_questions}ë¬¸ì œ, ë‚œì´ë„: {difficulty}")

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ê¸°ë°˜ í€´ì¦ˆ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )

            # ì‘ë‹µ íŒŒì‹±
            result_text = response.choices[0].message.content
            if result_text is None:
                raise ValueError("OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            quiz_data = self._parse_quiz_response(result_text)

            logger.info(f"OpenAI í€´ì¦ˆ ìƒì„± ì™„ë£Œ: {len(quiz_data.get('questions', []))}ë¬¸ì œ")
            return {
                "questions": quiz_data.get("questions", []),
                "success": True,
                "model_used": self.model_name,
                "provider": "openai"
            }

        except Exception as e:
            logger.error(f"OpenAI í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "questions": [],
                "success": False,
                "error": str(e),
                "model_used": self.model_name
            }

    def extract_topics(self, context: str) -> List[str]:
        """OpenAIë¥¼ ì‚¬ìš©í•œ ì£¼ìš” í† í”½ ì¶”ì¶œ"""

        prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í† í”½ë“¤ì„ ì¶”ì¶œí•˜ì„¸ìš”. í€´ì¦ˆ ë¬¸ì œë¡œ ë§Œë“¤ê¸° ì¢‹ì€ í•µì‹¬ ì£¼ì œë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{context[:3000]}...

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "topics": ["í† í”½1", "í† í”½2", "í† í”½3"],
    "main_subjects": ["ì£¼ìš”ì£¼ì œ1", "ì£¼ìš”ì£¼ì œ2"]
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ í€´ì¦ˆ ë¬¸ì œ ìƒì„±ì— ì í•©í•œ í•µì‹¬ í† í”½ì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=500
            )

            response_content = response.choices[0].message.content
            if response_content is None:
                raise ValueError("OpenAI í† í”½ ì¶”ì¶œ ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

            result = json.loads(response_content)
            topics = result.get("topics", []) + result.get("main_subjects", [])

            logger.info(f"ì¶”ì¶œëœ í† í”½: {topics}")
            return topics[:10]  # ìµœëŒ€ 10ê°œ

        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ["ì¼ë°˜"]

    def validate_question_quality(self, question_data: Dict[str, Any]) -> bool:
        """ë¬¸ì œ í’ˆì§ˆ ê²€ì¦"""

        # ê¸°ë³¸ ê²€ì¦
        if not question_data.get("question") or len(question_data["question"]) < 10:
            return False

        if not question_data.get("correct_answer"):
            return False

        # ê°ê´€ì‹ ë¬¸ì œ ê²€ì¦
        if question_data.get("question_type") == "multiple_choice":
            options = question_data.get("options", [])
            if len(options) < 2 or question_data["correct_answer"] not in options:
                return False

        return True

    def _build_quiz_generation_prompt(
        self,
        context: str,
        num_questions: int,
        difficulty: str,
        question_types: List[str],
        topics: Optional[List[str]] = None
    ) -> str:
        """í€´ì¦ˆ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""

        difficulty_map = {
            "easy": "ì‰¬ì›€ (ê¸°ë³¸ ê°œë… ì´í•´)",
            "medium": "ë³´í†µ (ì‘ìš© ë° ë¶„ì„)",
            "hard": "ì–´ë ¤ì›€ (ì‹¬í™” ë¶„ì„ ë° ì¢…í•©)"
        }

        type_instructions = {
            "multiple_choice": "4ê°œ ì„ íƒì§€ê°€ ìˆëŠ” ê°ê´€ì‹",
            "short_answer": "ê°„ë‹¨í•œ ì£¼ê´€ì‹ (1-2ë¬¸ì¥ ë‹µë³€)",
            "fill_blank": "ë¹ˆì¹¸ ì±„ìš°ê¸°",
            "true_false": "ì°¸/ê±°ì§“"
        }

        prompt = f"""
ë‹¤ìŒ PDF ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {num_questions}ê°œì˜ **ì‹¤ì§ˆì ì´ê³  ê°œë…ì ì¸** í€´ì¦ˆ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ë¬¸ì„œ ë‚´ìš©:**
{context[:4000]}

**ìƒì„± ì¡°ê±´:**
- ë‚œì´ë„: {difficulty_map.get(difficulty, 'ë³´í†µ')}
- ë¬¸ì œ ìˆ˜: {num_questions}ê°œ
- ë¬¸ì œ ìœ í˜•: {', '.join([type_instructions.get(qt, qt) for qt in question_types])}
{f"- ì§‘ì¤‘ í† í”½: {', '.join(topics)}" if topics else ""}

**ğŸš« í”¼í•´ì•¼ í•  ë¬¸ì œ ìœ í˜•:**
- ë‹¨ìˆœ ì•”ê¸°í˜• ë¬¸ì œ (ì˜ˆ: "ì£¼ì–´ì§„ ë™ì „ì˜ ì¢…ë¥˜ê°€ ì•„ë‹Œ ê²ƒì€?")
- ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ êµ¬ì²´ì  ìˆ˜ì¹˜ ë¬»ê¸°
- ì„ íƒì§€ë§Œ ë‹¤ë¥¸ ìœ ì‚¬ ë¬¸ì œ

**âœ… ìƒì„±í•´ì•¼ í•  ë¬¸ì œ ìœ í˜•:**
- ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ì›ë¦¬ ì´í•´
- ê°œë… ì ìš© ë° ì‘ìš©
- ë¬¸ì œ í•´ê²° ê³¼ì • ì„¤ëª…
- í•µì‹¬ ì•„ì´ë””ì–´ì™€ ì ‘ê·¼ë²•
- ì‹œê°„/ê³µê°„ ë³µì¡ë„ ë¶„ì„
- ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ ë¹„êµ

**ì¤‘ìš” ê·œì¹™:**
1. ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ë¬¸ì œ ìƒì„±
2. ê°œë… ì´í•´ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë¬¸ì œ ìš°ì„ 
3. ê°ê´€ì‹ì€ ì •ë‹µ 1ê°œ + ê·¸ëŸ´ë“¯í•œ ì˜¤ë‹µ 3ê°œ
4. ê° ë¬¸ì œë§ˆë‹¤ ìƒì„¸í•œ í•´ì„¤ í¬í•¨
5. ë¬¸ì œ ê°„ ì¤‘ë³µ í”¼í•˜ê¸°
6. ì‹¤ì œ í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ë¬¸ì œ

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "questions": [
        {{
            "question": "ê°œë…ì ì´ê³  ì‹¤ì§ˆì ì¸ ë¬¸ì œ ë‚´ìš©",
            "question_type": "multiple_choice|short_answer|fill_blank|true_false",
            "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],  // ê°ê´€ì‹ë§Œ
            "correct_answer": "ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤ê³¼ ì´ìœ ",
            "difficulty": "{difficulty}",
            "topic": "ê´€ë ¨ í† í”½"
        }}
    ]
}}
"""
        return prompt

    def _parse_quiz_response(self, response_text: str) -> Dict[str, Any]:
        """OpenAI ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx == -1 or end_idx == 0:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            json_text = response_text[start_idx:end_idx]
            return json.loads(json_text)

        except Exception as e:
            logger.error(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {"questions": []}


class AnthropicLLMService(BaseLLMService):
    """Anthropic Claude ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ (ë¯¸ë˜ í™•ì¥ìš©)"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        logger.info("Anthropic LLM ì„œë¹„ìŠ¤ (ì¤€ë¹„ ì¤‘)")

    def generate_quiz(self, context: str, num_questions: int, difficulty: str, question_types: List[str], topics: Optional[List[str]] = None) -> Dict[str, Any]:
        return {"questions": [], "success": False, "error": "Anthropic ë¯¸êµ¬í˜„"}

    def extract_topics(self, context: str) -> List[str]:
        return ["ì¼ë°˜"]

    def validate_question_quality(self, question_data: Dict[str, Any]) -> bool:
        return True


class KoreanLocalLLMService(BaseLLMService):
    """í•œêµ­ì–´ ë¡œì»¬ LLM ì„œë¹„ìŠ¤ (ë¯¸ë˜ í™•ì¥ìš©)"""

    def __init__(self, config: LLMConfig):
        super().__init__(config)
        logger.info("í•œêµ­ì–´ ë¡œì»¬ LLM ì„œë¹„ìŠ¤ (ì¤€ë¹„ ì¤‘)")

    def generate_quiz(self, context: str, num_questions: int, difficulty: str, question_types: List[str], topics: Optional[List[str]] = None) -> Dict[str, Any]:
        return {"questions": [], "success": False, "error": "í•œêµ­ì–´ ë¡œì»¬ ëª¨ë¸ ë¯¸êµ¬í˜„"}

    def extract_topics(self, context: str) -> List[str]:
        return ["ì¼ë°˜"]

    def validate_question_quality(self, question_data: Dict[str, Any]) -> bool:
        return True


class LLMFactory:
    """LLM íŒ©í† ë¦¬ í´ë˜ìŠ¤"""

    _services = {
        LLMProvider.OPENAI: OpenAILLMService,
        LLMProvider.ANTHROPIC: AnthropicLLMService,
        LLMProvider.KOREAN_LOCAL: KoreanLocalLLMService,
    }

    @classmethod
    def create_llm(cls, config: LLMConfig) -> BaseLLMService:
        """LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""

        if config.provider not in cls._services:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì—…ì²´: {config.provider}")

        service_class = cls._services[config.provider]
        return service_class(config)

    @classmethod
    def get_available_providers(cls) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì—…ì²´ ëª©ë¡"""
        return [provider.value for provider in cls._services.keys()]

    @classmethod
    def create_openai_gpt4o_mini(cls, api_key: Optional[str] = None, language: str = "ko") -> BaseLLMService:
        """OpenAI GPT-4o-mini í€µ ìƒì„±"""
        config = LLMConfig(
            provider=LLMProvider.OPENAI,
            model_name="gpt-4o-mini",
            api_key=api_key,
            temperature=0.7,
            max_tokens=2000,
            language=language
        )
        return cls.create_llm(config)

    @classmethod
    def create_korean_local_model(cls, model_name: str = "kullm-polyglot-12.8b-v2") -> BaseLLMService:
        """í•œêµ­ì–´ ë¡œì»¬ ëª¨ë¸ í€µ ìƒì„± (ë¯¸ë˜ìš©)"""
        config = LLMConfig(
            provider=LLMProvider.KOREAN_LOCAL,
            model_name=model_name,
            language="ko"
        )
        return cls.create_llm(config)


# ì „ì—­ ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_default_llm_service: Optional[BaseLLMService] = None


def get_default_llm_service() -> BaseLLMService:
    """ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _default_llm_service

    if _default_llm_service is None:
        settings = get_settings()
        _default_llm_service = LLMFactory.create_openai_gpt4o_mini(
            api_key=settings.OPENAI_API_KEY
        )
        logger.info("ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: OpenAI GPT-4o-mini")

    return _default_llm_service


def set_default_llm_service(service: BaseLLMService):
    """ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ ì„¤ì •"""
    global _default_llm_service
    _default_llm_service = service
    logger.info(f"ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ ë³€ê²½: {service.model_name}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=== LLM Factory í…ŒìŠ¤íŠ¸ ===")

    # OpenAI ì„œë¹„ìŠ¤ ìƒì„±
    try:
        openai_service = LLMFactory.create_openai_gpt4o_mini()
        print(f"OpenAI ì„œë¹„ìŠ¤ ìƒì„± ì„±ê³µ: {openai_service.model_name}")
    except Exception as e:
        print(f"OpenAI ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì—…ì²´ ì¶œë ¥
    providers = LLMFactory.get_available_providers()
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì—…ì²´: {providers}")