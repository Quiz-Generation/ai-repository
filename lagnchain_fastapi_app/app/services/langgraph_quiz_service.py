"""
ğŸ¯ LangGraph ê¸°ë°˜ ì§„ì§œ ì‘ë™í•˜ëŠ” í€´ì¦ˆ ìƒì„± ì‹œìŠ¤í…œ
- Agent ì›Œí¬í”Œë¡œìš°ë¡œ í’ˆì§ˆ ë³´ì¥
- ì‹¤ì œ ì¤‘ë³µ ì œê±° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
- ì§„ì§œ 2:6:2 ë¹„ìœ¨ ì ìš©
- êµ¬ë¦° ë¬¸ì œ ìë™ ì¬ìƒì„±
"""
import logging
import asyncio
import uuid
import time
from typing import List, Dict, Any, Optional, TypedDict
from dataclasses import dataclass
from enum import Enum

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from ..schemas.quiz_schema import (
    QuizRequest, QuizResponse, Question, Difficulty, QuestionType,
    RAGContext
)
from ..services.llm_factory import BaseLLMService, get_default_llm_service
from ..services.vector_service import PDFVectorService, get_global_vector_service

logger = logging.getLogger(__name__)


class WorkflowState(TypedDict):
    """LangGraph ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    request: QuizRequest
    contexts: List[RAGContext]
    generated_questions: List[Dict[str, Any]]
    validated_questions: List[Question]
    quality_score: float
    duplicate_count: int
    type_distribution: Dict[str, int]
    current_attempt: int
    max_attempts: int
    errors: List[str]
    success: bool


@dataclass
class QuestionBatch:
    """ë¬¸ì œ ë°°ì¹˜"""
    question_type: QuestionType
    count: int
    questions: List[Dict[str, Any]]
    quality_score: float
    has_duplicates: bool


class LangGraphQuizService:
    """ğŸš€ LangGraph ê¸°ë°˜ ì§„ì§œ ì‘ë™í•˜ëŠ” í€´ì¦ˆ ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        vector_service: Optional[PDFVectorService] = None,
        llm_service: Optional[BaseLLMService] = None
    ):
        self.vector_service = vector_service or get_global_vector_service()
        self.llm_service = llm_service or get_default_llm_service()

        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.similarity_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
            logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except:
            logger.warning("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            self.similarity_model = None

        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._create_workflow()

        logger.info("ğŸš€ LangGraph í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def _create_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(WorkflowState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("initialize", self._initialize_node)
        workflow.add_node("retrieve_contexts", self._retrieve_contexts_node)
        workflow.add_node("generate_questions", self._generate_questions_node)
        workflow.add_node("validate_quality", self._validate_quality_node)
        workflow.add_node("check_duplicates", self._check_duplicates_node)
        workflow.add_node("regenerate_bad_questions", self._regenerate_bad_questions_node)
        workflow.add_node("finalize", self._finalize_node)

        # ì‹œì‘ì 
        workflow.set_entry_point("initialize")

        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge("initialize", "retrieve_contexts")
        workflow.add_edge("retrieve_contexts", "generate_questions")
        workflow.add_edge("generate_questions", "validate_quality")
        workflow.add_edge("validate_quality", "check_duplicates")

        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "check_duplicates",
            self._should_regenerate,
            {
                "regenerate": "regenerate_bad_questions",
                "finalize": "finalize"
            }
        )

        workflow.add_edge("regenerate_bad_questions", "validate_quality")
        workflow.add_edge("finalize", END)

        return workflow.compile()

    async def generate_quiz(self, request: QuizRequest) -> QuizResponse:
        """ì§„ì§œ ì‘ë™í•˜ëŠ” í€´ì¦ˆ ìƒì„±"""
        start_time = time.time()
        quiz_id = str(uuid.uuid4())

        logger.info(f"ğŸ¯ LangGraph í€´ì¦ˆ ìƒì„± ì‹œì‘: {request.num_questions}ë¬¸ì œ")

        try:
            # ì´ˆê¸° ìƒíƒœ
            initial_state = WorkflowState(
                request=request,
                contexts=[],
                generated_questions=[],
                validated_questions=[],
                quality_score=0.0,
                duplicate_count=0,
                type_distribution={},
                current_attempt=0,
                max_attempts=3,
                errors=[],
                success=False
            )

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self._run_workflow(initial_state)

            generation_time = time.time() - start_time

            if final_state["success"]:
                response = QuizResponse(
                    quiz_id=quiz_id,
                    document_id=request.document_id,
                    questions=final_state["validated_questions"],
                    total_questions=len(final_state["validated_questions"]),
                    difficulty=request.difficulty,
                    generation_time=generation_time,
                    success=True,
                    metadata={
                        "generation_method": "langgraph_agent_workflow",
                        "workflow_attempts": final_state["current_attempt"],
                        "quality_score": final_state["quality_score"],
                        "duplicate_count": final_state["duplicate_count"],
                        "type_distribution": final_state["type_distribution"],
                        "contexts_used": len(final_state["contexts"]),
                        "advanced_features": [
                            "ğŸš€ LangGraph Agent ì›Œí¬í”Œë¡œìš°",
                            "ğŸ”¥ ì‹¤ì œ ì‘ë™í•˜ëŠ” ì¤‘ë³µ ì œê±°",
                            "ğŸ¯ ì§„ì§œ 2:6:2 ë¹„ìœ¨ ì ìš©",
                            "âš¡ êµ¬ë¦° ë¬¸ì œ ìë™ ì¬ìƒì„±",
                            "ğŸ§  Agent ê¸°ë°˜ í’ˆì§ˆ ë³´ì¥"
                        ]
                    }
                )
                logger.info(f"ğŸ‰ LangGraph í€´ì¦ˆ ìƒì„± ì„±ê³µ: í’ˆì§ˆ {final_state['quality_score']:.1f}/10")
                return response
            else:
                raise ValueError(f"ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {final_state['errors']}")

        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"ğŸš¨ LangGraph í€´ì¦ˆ ìƒì„± ì‹¤íŒ¨: {e}")

            return QuizResponse(
                quiz_id=quiz_id,
                document_id=request.document_id,
                questions=[],
                total_questions=0,
                difficulty=request.difficulty,
                generation_time=error_time,
                success=False,
                error=str(e)
            )

    async def _run_workflow(self, initial_state: WorkflowState) -> WorkflowState:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        current_state = initial_state

        try:
            async for step_name, step_output in self.workflow.astream(current_state):
                logger.info(f"ğŸ”„ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„: {step_name}")
                if isinstance(step_output, dict):
                    current_state.update(step_output)

                # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                if current_state.get("errors") and len(current_state["errors"]) > 0:
                    logger.error(f"ì›Œí¬í”Œë¡œìš° ì—ëŸ¬: {current_state['errors']}")
                    break

        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            current_state["errors"].append(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

        return current_state

    async def _initialize_node(self, state: WorkflowState) -> WorkflowState:
        """ì´ˆê¸°í™” ë…¸ë“œ"""
        logger.info("ğŸ“‹ ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”")

        # ë¬¸ì„œ í™•ì¸
        doc_info = self.vector_service.get_document_info(state["request"].document_id)
        if not doc_info:
            state["errors"].append(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {state['request'].document_id}")
            return state

        # íƒ€ì… ë¶„ë°° ê³„ì‚°
        state["type_distribution"] = self._calculate_real_type_distribution(state["request"])
        logger.info(f"ğŸ¯ ì§„ì§œ íƒ€ì… ë¶„ë°°: {state['type_distribution']}")

        return state

    async def _retrieve_contexts_node(self, state: WorkflowState) -> WorkflowState:
        """ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë…¸ë“œ"""
        logger.info("ğŸ§  ë‹¤ì–‘ì„± ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰")

        try:
            # ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ (Fibonaccië§Œ ë‚˜ì˜¤ì§€ ì•Šë„ë¡)
            diverse_queries = [
                "í•µì‹¬ ê°œë…ê³¼ ì›ë¦¬",
                "ì‹¤ì œ ì‚¬ë¡€ì™€ ì˜ˆì‹œ",
                "ì¤‘ìš”í•œ ê¸°ìˆ ê³¼ ë°©ë²•",
                "ë¬¸ì œ í•´ê²° ì „ëµ",
                "ì„±ëŠ¥ê³¼ ìµœì í™”",
                "ì•Œê³ ë¦¬ì¦˜ê³¼ êµ¬ì¡°",
                "ì„¤ê³„ì™€ íŒ¨í„´"
            ]

            all_contexts = []
            for query in diverse_queries:
                results = self.vector_service.search_in_document(
                    query=query,
                    document_id=state["request"].document_id,
                    top_k=3
                )

                for result in results:
                    context = RAGContext(
                        text=result["text"],
                        similarity=result["similarity"],
                        source=result["metadata"].get("source", ""),
                        chunk_index=result["metadata"].get("chunk_index", 0),
                        metadata=result["metadata"]
                    )
                    all_contexts.append(context)

            # ì¤‘ë³µ ì œê±° ë° ë‹¤ì–‘ì„± ë³´ì¥
            unique_contexts = self._diversify_contexts(all_contexts)
            state["contexts"] = unique_contexts[:state["request"].num_questions * 2]

            logger.info(f"âœ… ë‹¤ì–‘ì„± ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ {len(state['contexts'])}ê°œ í™•ë³´")

        except Exception as e:
            state["errors"].append(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        return state

    async def _generate_questions_node(self, state: WorkflowState) -> WorkflowState:
        """ë¬¸ì œ ìƒì„± ë…¸ë“œ"""
        logger.info("âš¡ íƒ€ì…ë³„ ë¬¸ì œ ìƒì„±")

        try:
            all_questions = []

            # íƒ€ì…ë³„ë¡œ ë¬¸ì œ ìƒì„±
            context_offset = 0
            for q_type_str, count in state["type_distribution"].items():
                if count > 0:
                    question_type = QuestionType(q_type_str)

                    # í•´ë‹¹ íƒ€ì…ìš© ì»¨í…ìŠ¤íŠ¸ í• ë‹¹
                    type_contexts = state["contexts"][context_offset:context_offset + count]
                    context_offset += count

                    # ë¬¸ì œ ìƒì„±
                    questions = await self._generate_type_specific_questions(
                        question_type, count, type_contexts, state["request"].difficulty
                    )

                    all_questions.extend(questions)
                    logger.info(f"âœ… {question_type.value} {len(questions)}ê°œ ìƒì„±")

            state["generated_questions"] = all_questions

        except Exception as e:
            state["errors"].append(f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")

        return state

    async def _validate_quality_node(self, state: WorkflowState) -> WorkflowState:
        """í’ˆì§ˆ ê²€ì¦ ë…¸ë“œ"""
        logger.info("ğŸ” í’ˆì§ˆ ê²€ì¦")

        try:
            questions = []
            total_score = 0

            for q_data in state["generated_questions"]:
                question = self._convert_to_question_object(q_data, state["contexts"])
                score = self._score_question_quality(question)

                if score >= 7.0:  # í’ˆì§ˆ ê¸°ì¤€
                    questions.append(question)
                    total_score += score
                else:
                    logger.warning(f"í’ˆì§ˆ ë¯¸ë‹¬ ë¬¸ì œ ì œì™¸: {question.question[:50]}... (ì ìˆ˜: {score})")

            state["validated_questions"] = questions
            state["quality_score"] = total_score / len(questions) if questions else 0

            logger.info(f"âœ… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(questions)}ê°œ í†µê³¼, í‰ê·  {state['quality_score']:.1f}ì ")

        except Exception as e:
            state["errors"].append(f"í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")

        return state

    async def _check_duplicates_node(self, state: WorkflowState) -> WorkflowState:
        """ì¤‘ë³µ ê²€ì‚¬ ë…¸ë“œ"""
        logger.info("ğŸ” ì§„ì§œ ì¤‘ë³µ ê²€ì‚¬")

        try:
            if not state["validated_questions"]:
                state["duplicate_count"] = 0
                return state

            # ì‹¤ì œ ìœ ì‚¬ë„ ê³„ì‚°
            questions_texts = [q.question for q in state["validated_questions"]]

            if self.similarity_model and len(questions_texts) > 1:
                embeddings = self.similarity_model.encode(questions_texts)
                similarity_matrix = cosine_similarity(embeddings)

                duplicate_indices = set()
                for i in range(len(questions_texts)):
                    for j in range(i+1, len(questions_texts)):
                        if similarity_matrix[i][j] > 0.7:  # ì—„ê²©í•œ ê¸°ì¤€
                            duplicate_indices.add(j)  # ë’¤ì˜ ê²ƒ ì œê±°
                            logger.warning(f"ğŸš« ì¤‘ë³µ ë°œê²¬: ìœ ì‚¬ë„ {similarity_matrix[i][j]:.3f}")
                            logger.warning(f"   ë¬¸ì œ1: {questions_texts[i][:50]}...")
                            logger.warning(f"   ë¬¸ì œ2: {questions_texts[j][:50]}...")

                # ì¤‘ë³µ ì œê±°
                filtered_questions = [
                    q for i, q in enumerate(state["validated_questions"])
                    if i not in duplicate_indices
                ]

                state["validated_questions"] = filtered_questions
                state["duplicate_count"] = len(duplicate_indices)

                logger.info(f"ğŸ”¥ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(duplicate_indices)}ê°œ ì œê±°, {len(filtered_questions)}ê°œ ë‚¨ìŒ")

        except Exception as e:
            state["errors"].append(f"ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {e}")

        return state

    def _should_regenerate(self, state: WorkflowState) -> str:
        """ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        state["current_attempt"] += 1

        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
        if state["current_attempt"] >= state["max_attempts"]:
            logger.warning(f"âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {state['current_attempt']}")
            return "finalize"

        # ë¬¸ì œ ìˆ˜ ë¶€ì¡±
        required_count = state["request"].num_questions
        current_count = len(state["validated_questions"])

        if current_count < required_count * 0.8:  # 80% ë¯¸ë§Œì´ë©´ ì¬ìƒì„±
            logger.warning(f"ğŸ“‰ ë¬¸ì œ ìˆ˜ ë¶€ì¡±: {current_count}/{required_count}")
            return "regenerate"

        # í’ˆì§ˆ ì ìˆ˜ ë‚®ìŒ
        if state["quality_score"] < 7.5:
            logger.warning(f"ğŸ“‰ í’ˆì§ˆ ì ìˆ˜ ë‚®ìŒ: {state['quality_score']:.1f}")
            return "regenerate"

        # ì¤‘ë³µì´ ë§ìŒ
        if state["duplicate_count"] > 2:
            logger.warning(f"ğŸ“‰ ì¤‘ë³µ ë§ìŒ: {state['duplicate_count']}ê°œ")
            return "regenerate"

        return "finalize"

    async def _regenerate_bad_questions_node(self, state: WorkflowState) -> WorkflowState:
        """êµ¬ë¦° ë¬¸ì œ ì¬ìƒì„± ë…¸ë“œ"""
        logger.info(f"ğŸ”„ ì¬ìƒì„± ì‹œë„ {state['current_attempt']}")

        try:
            # ë¶€ì¡±í•œ ë¬¸ì œ ìˆ˜ ê³„ì‚°
            required_count = state["request"].num_questions
            current_count = len(state["validated_questions"])
            needed_count = required_count - current_count

            if needed_count > 0:
                # ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€ ìƒì„±
                additional_contexts = state["contexts"][current_count:]

                additional_questions = await self._generate_diverse_questions(
                    needed_count, additional_contexts, state["request"].difficulty
                )

                state["generated_questions"].extend(additional_questions)
                logger.info(f"ğŸ”„ ì¶”ê°€ ìƒì„±: {len(additional_questions)}ê°œ")

        except Exception as e:
            state["errors"].append(f"ì¬ìƒì„± ì‹¤íŒ¨: {e}")

        return state

    async def _finalize_node(self, state: WorkflowState) -> WorkflowState:
        """ìµœì¢…í™” ë…¸ë“œ"""
        logger.info("ğŸ¯ ìµœì¢…í™”")

        # ì •í™•í•œ ê°œìˆ˜ë¡œ ìë¥´ê¸°
        required_count = state["request"].num_questions
        state["validated_questions"] = state["validated_questions"][:required_count]

        # íƒ€ì… ë¶„í¬ ì—…ë°ì´íŠ¸
        actual_distribution = {}
        for question in state["validated_questions"]:
            qtype = question.question_type.value
            actual_distribution[qtype] = actual_distribution.get(qtype, 0) + 1

        state["type_distribution"] = actual_distribution
        state["success"] = True

        logger.info(f"ğŸ‰ ìµœì¢… ì™„ë£Œ: {len(state['validated_questions'])}ë¬¸ì œ, í’ˆì§ˆ {state['quality_score']:.1f}/10")

        return state

    def _calculate_real_type_distribution(self, request: QuizRequest) -> Dict[str, int]:
        """ì§„ì§œ 2:6:2 ë¹„ìœ¨ ê³„ì‚°"""
        if request.question_types and len(request.question_types) == 1:
            # í•˜ë‚˜ë§Œ ì„ íƒí•˜ë©´ 100%
            return {request.question_types[0].value: request.num_questions}

        # ê¸°ë³¸ 2:6:2 ë¹„ìœ¨
        total = request.num_questions
        tf_count = max(1, round(total * 0.2))      # 20% OX
        mc_count = max(1, round(total * 0.6))      # 60% ê°ê´€ì‹
        sa_count = total - tf_count - mc_count     # ë‚˜ë¨¸ì§€ ì£¼ê´€ì‹

        return {
            "true_false": tf_count,
            "multiple_choice": mc_count,
            "short_answer": sa_count
        }

    def _diversify_contexts(self, contexts: List[RAGContext]) -> List[RAGContext]:
        """ì»¨í…ìŠ¤íŠ¸ ë‹¤ì–‘ì„± ë³´ì¥"""
        if not contexts:
            return []

        unique_contexts = []
        seen_signatures = set()

        for ctx in contexts:
            # í…ìŠ¤íŠ¸ ì‹œê·¸ë‹ˆì²˜ ìƒì„± (ì²« 100ì)
            signature = ctx.text[:100].strip().lower()

            # Fibonacci ê°™ì€ íŠ¹ì • í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
            fibonacci_count = sum(1 for existing in unique_contexts if "fibonacci" in existing.text.lower())
            if "fibonacci" in ctx.text.lower() and fibonacci_count >= 2:
                continue

            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_contexts.append(ctx)

        return unique_contexts

    async def _generate_type_specific_questions(
        self,
        question_type: QuestionType,
        count: int,
        contexts: List[RAGContext],
        difficulty: Difficulty
    ) -> List[Dict[str, Any]]:
        """íƒ€ì…ë³„ ë¬¸ì œ ìƒì„±"""

        if not contexts:
            return []

        context_text = "\n\n".join([f"[ì»¨í…ìŠ¤íŠ¸ {i+1}]\n{ctx.text}" for i, ctx in enumerate(contexts)])

        prompt = self._get_type_prompt(question_type, context_text, count, difficulty)

        try:
            response = await self.llm_service.client.chat.completions.create(
                model=self.llm_service.model_name,
                messages=[
                    {"role": "system", "content": f"ì „ë¬¸ {question_type.value} ë¬¸ì œ ì¶œì œì. ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,  # ë‹¤ì–‘ì„± ì¦ê°€
                max_tokens=2000
            )

            result_text = response.choices[0].message.content
            return self._parse_questions_response(result_text, question_type)

        except Exception as e:
            logger.error(f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
            return []

    async def _generate_diverse_questions(
        self,
        count: int,
        contexts: List[RAGContext],
        difficulty: Difficulty
    ) -> List[Dict[str, Any]]:
        """ë‹¤ì–‘í•œ ë¬¸ì œ ìƒì„±"""

        questions = []
        for i, ctx in enumerate(contexts[:count]):
            # ë¬¸ì œ ìœ í˜•ì„ ë²ˆê°ˆì•„ê°€ë©°
            if i % 3 == 0:
                question_type = QuestionType.TRUE_FALSE
            elif i % 3 == 1:
                question_type = QuestionType.MULTIPLE_CHOICE
            else:
                question_type = QuestionType.SHORT_ANSWER

            ctx_questions = await self._generate_type_specific_questions(
                question_type, 1, [ctx], difficulty
            )
            questions.extend(ctx_questions)

        return questions

    def _get_type_prompt(self, question_type: QuestionType, context: str, count: int, difficulty: Difficulty) -> str:
        """íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸"""

        if question_type == QuestionType.TRUE_FALSE:
            return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ {count}ê°œì˜ ê³ í’ˆì§ˆ OX ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ëª…í™•í•˜ê²Œ ì°¸/ê±°ì§“ êµ¬ë¶„ ê°€ëŠ¥
- ì •ë‹µì€ "True" ë˜ëŠ” "False"ë§Œ
- ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ ë¬¸ì œ

JSON í˜•ì‹:
{{
    "questions": [
        {{
            "question": "êµ¬ì²´ì ì¸ OX ë¬¸ì œ",
            "question_type": "true_false",
            "correct_answer": "True",
            "explanation": "ìƒì„¸í•œ í•´ì„¤"
        }}
    ]
}}
"""

        elif question_type == QuestionType.MULTIPLE_CHOICE:
            return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ {count}ê°œì˜ ê³ í’ˆì§ˆ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- 4ê°œ ì„ íƒì§€ (ì •ë‹µ 1ê°œ + ì˜¤ë‹µ 3ê°œ)
- options ë°°ì—´ ë°˜ë“œì‹œ í¬í•¨
- ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ ë¬¸ì œ

JSON í˜•ì‹:
{{
    "questions": [
        {{
            "question": "êµ¬ì²´ì ì¸ ê°ê´€ì‹ ë¬¸ì œ?",
            "question_type": "multiple_choice",
            "options": ["ì •ë‹µ", "ì˜¤ë‹µ1", "ì˜¤ë‹µ2", "ì˜¤ë‹µ3"],
            "correct_answer": "ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤"
        }}
    ]
}}
"""

        else:  # SHORT_ANSWER
            return f"""
ë‹¤ìŒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•íˆ {count}ê°œì˜ ê³ í’ˆì§ˆ ì£¼ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context[:2000]}

ìš”êµ¬ì‚¬í•­:
- ë‚œì´ë„: {difficulty.value}
- ë‹¨ë‹µí˜• (1-2ë¬¸ì¥ ë‹µë³€)
- ëª…í™•í•œ ì •ë‹µ ì¡´ì¬
- ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ ë¬¸ì œ

JSON í˜•ì‹:
{{
    "questions": [
        {{
            "question": "êµ¬ì²´ì ì¸ ì£¼ê´€ì‹ ë¬¸ì œ?",
            "question_type": "short_answer",
            "correct_answer": "ëª…í™•í•œ ì •ë‹µ",
            "explanation": "ìƒì„¸í•œ í•´ì„¤"
        }}
    ]
}}
"""

    def _parse_questions_response(self, response_text: str, question_type: QuestionType) -> List[Dict[str, Any]]:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            import json
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1

            if start_idx == -1 or end_idx == 0:
                return []

            json_text = response_text[start_idx:end_idx]
            result = json.loads(json_text)

            questions = result.get("questions", [])
            valid_questions = []

            for q in questions:
                if q.get("question_type") == question_type.value:
                    valid_questions.append(q)

            return valid_questions

        except Exception as e:
            logger.error(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def _convert_to_question_object(self, q_data: Dict[str, Any], contexts: List[RAGContext]) -> Question:
        """Question ê°ì²´ë¡œ ë³€í™˜"""
        question_type = QuestionType(q_data.get("question_type", "multiple_choice"))

        return Question(
            question=q_data.get("question", ""),
            question_type=question_type,
            correct_answer=q_data.get("correct_answer", ""),
            options=q_data.get("options"),
            explanation=q_data.get("explanation", ""),
            difficulty=Difficulty(q_data.get("difficulty", "medium")),
            source_context=contexts[0].text[:200] if contexts else "",
            topic=q_data.get("topic", "ì£¼ìš” ë‚´ìš©"),
            metadata={
                "langgraph_generated": True,
                "quality_verified": True,
                "duplicate_checked": True
            }
        )

    def _score_question_quality(self, question: Question) -> float:
        """ë¬¸ì œ í’ˆì§ˆ ì ìˆ˜"""
        score = 7.0

        # ê¸°ë³¸ ê²€ì¦
        if len(question.question.strip()) < 10:
            score -= 2.0
        if not question.correct_answer.strip():
            score -= 3.0
        if len(question.explanation.strip()) < 20:
            score -= 1.0

        # ê°ê´€ì‹ íŠ¹ë³„ ê²€ì¦
        if question.question_type == QuestionType.MULTIPLE_CHOICE:
            if not question.options or len(question.options) < 4:
                score -= 3.0
            elif question.correct_answer not in question.options:
                score -= 3.0
            else:
                score += 1.0  # ë³´ë„ˆìŠ¤

        return max(0, min(10, score))


# ì „ì—­ ì„œë¹„ìŠ¤
_langgraph_quiz_service: Optional[LangGraphQuizService] = None

def get_langgraph_quiz_service() -> LangGraphQuizService:
    """LangGraph í€´ì¦ˆ ì„œë¹„ìŠ¤ ë°˜í™˜"""
    global _langgraph_quiz_service

    if _langgraph_quiz_service is None:
        _langgraph_quiz_service = LangGraphQuizService()
        logger.info("ğŸš€ LangGraph í€´ì¦ˆ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    return _langgraph_quiz_service


if __name__ == "__main__":
    print("ğŸš€ LangGraph ê¸°ë°˜ ì§„ì§œ ì‘ë™í•˜ëŠ” í€´ì¦ˆ ì‹œìŠ¤í…œ")
    print("âœ… Agent ì›Œí¬í”Œë¡œìš°ë¡œ í’ˆì§ˆ ë³´ì¥")
    print("âœ… ì‹¤ì œ ì¤‘ë³µ ì œê±° (ë¬´í•œ ë£¨í”„ ë°©ì§€)")
    print("âœ… ì§„ì§œ 2:6:2 ë¹„ìœ¨ ì ìš©")
    print("âœ… êµ¬ë¦° ë¬¸ì œ ìë™ ì¬ìƒì„±")