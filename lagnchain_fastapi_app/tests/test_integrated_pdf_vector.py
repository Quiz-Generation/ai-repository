#!/usr/bin/env python3
"""
í†µí•© PDF ë²¡í„° API í…ŒìŠ¤íŠ¸
"""
import pytest
from pathlib import Path
from io import BytesIO

from lagnchain_fastapi_app.app.api.pdf_service import router
from lagnchain_fastapi_app.app.services.vector_service import PDFVectorService


class TestIntegratedPDFVector:
    """í†µí•© PDF ë²¡í„° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def vector_service(self):
        return PDFVectorService(db_type="weaviate")

    @pytest.fixture
    def sample_pdf_path(self):
        return Path(__file__).parent.parent / "static" / "temp" / "lecture-DynamicProgramming.pdf"

    def test_vector_service_integration(self, vector_service):
        """ë²¡í„° ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        sample_text = "ë™ì ê³„íšë²•ì€ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ê¸°ë²•ì…ë‹ˆë‹¤. " * 20

        # ì²˜ë¦¬ ë° ì €ì¥
        result = vector_service.process_pdf_text(sample_text, "integration_test.pdf")

        assert result["success"] is True
        assert result["total_chunks"] > 0
        assert result["stored_chunks"] > 0
        assert result["db_type"] == "weaviate"
        assert "document_id" in result

        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        search_results = vector_service.search_documents("ë™ì ê³„íšë²•", top_k=3)
        assert len(search_results) > 0
        assert all(result["similarity"] != 0 for result in search_results)

        # í†µê³„ í™•ì¸
        stats = vector_service.get_stats()
        assert stats["total_documents"] > 0
        assert stats["db_type"] == "weaviate"
        assert "weaviate" in stats["supported_dbs"]

        print(f"âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ:")
        print(f"   - ì²˜ë¦¬: {result['stored_chunks']}ê°œ ì²­í¬")
        print(f"   - ë¬¸ì„œ ID: {result['document_id']}")
        print(f"   - ê²€ìƒ‰: {len(search_results)}ê°œ ê²°ê³¼")
        print(f"   - ì´ ë¬¸ì„œ: {stats['total_documents']}ê°œ")

    def test_document_management_workflow(self, vector_service):
        """ğŸ“ ë¬¸ì„œ ê´€ë¦¬ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (RAGìš©)"""
        # 1ë‹¨ê³„: ì—¬ëŸ¬ ë¬¸ì„œ ì—…ë¡œë“œ
        doc1_result = vector_service.process_pdf_text(
            "Pythonì€ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. " * 10,
            "python_guide.pdf"
        )
        doc2_result = vector_service.process_pdf_text(
            "ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. " * 10,
            "database_intro.pdf"
        )
        doc3_result = vector_service.process_pdf_text(
            "ì›¹ ê°œë°œì—ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤. " * 10,
            "web_development.pdf"
        )

        # ë¬¸ì„œ ID í™•ì¸
        doc1_id = doc1_result["document_id"]
        doc2_id = doc2_result["document_id"]
        doc3_id = doc3_result["document_id"]

        assert all([doc1_id, doc2_id, doc3_id])
        print(f"ğŸ“ 3ê°œ ë¬¸ì„œ ì—…ë¡œë“œ: {doc1_id[:8]}..., {doc2_id[:8]}..., {doc3_id[:8]}...")

        # 2ë‹¨ê³„: ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        document_list = vector_service.get_document_list()
        assert len(document_list) >= 3

        uploaded_ids = {doc["document_id"] for doc in document_list}
        assert doc1_id in uploaded_ids
        assert doc2_id in uploaded_ids
        assert doc3_id in uploaded_ids

        print(f"ğŸ“‹ ë¬¸ì„œ ëª©ë¡: {len(document_list)}ê°œ í™•ì¸")

        # 3ë‹¨ê³„: íŠ¹ì • ë¬¸ì„œ ì •ë³´ ì¡°íšŒ
        doc1_info = vector_service.get_document_info(doc1_id)
        assert doc1_info is not None
        assert doc1_info["source_filename"] == "python_guide.pdf"
        assert doc1_info["document_id"] == doc1_id

        print(f"ğŸ“„ ë¬¸ì„œ ì •ë³´: {doc1_info['source_filename']} ({doc1_info['chunk_count']}ê°œ ì²­í¬)")

        # 4ë‹¨ê³„: íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰ (RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œìš©)
        python_results = vector_service.search_in_document("í”„ë¡œê·¸ë˜ë°", doc1_id, top_k=3)
        db_results = vector_service.search_in_document("ë°ì´í„°ë² ì´ìŠ¤", doc2_id, top_k=3)

        assert len(python_results) > 0
        assert len(db_results) > 0

        # ê²€ìƒ‰ ê²°ê³¼ê°€ í•´ë‹¹ ë¬¸ì„œì—ì„œë§Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
        for result in python_results:
            assert result["metadata"]["document_id"] == doc1_id
        for result in db_results:
            assert result["metadata"]["document_id"] == doc2_id

        print(f"ğŸ¯ ë¬¸ì„œë³„ ê²€ìƒ‰: Python({len(python_results)}ê°œ), DB({len(db_results)}ê°œ)")

        # 5ë‹¨ê³„: RAGìš© ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜
        rag_context = ""
        for result in python_results:
            rag_context += result["text"] + "\n\n"

        assert len(rag_context) > 100
        print(f"ğŸ¤– RAG ì»¨í…ìŠ¤íŠ¸: {len(rag_context)}ì ì¶”ì¶œ ì™„ë£Œ")

        print("âœ… ë¬¸ì„œ ê´€ë¦¬ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì„±ê³µ (RAG ì¤€ë¹„ ì™„ë£Œ)")

    def test_rag_context_extraction(self, vector_service):
        """ğŸ¤– RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        # ë‹¤ì–‘í•œ ì£¼ì œì˜ í•™ìŠµ ë¬¸ì„œ ì—…ë¡œë“œ
        topics_and_content = {
            "algorithm": "ë™ì ê³„íšë²•ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. ë©”ëª¨ì´ì œì´ì…˜ì„ í™œìš©í•©ë‹ˆë‹¤. " * 10,
            "database": "ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ëŠ” í…Œì´ë¸” êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. SQLì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬í•©ë‹ˆë‹¤. " * 10,
            "web": "ì›¹ ê°œë°œì—ëŠ” í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ ê¸°ìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤. React, Node.js ë“±ì„ í™œìš©í•©ë‹ˆë‹¤. " * 10
        }

        document_ids = {}
        for topic, content in topics_and_content.items():
            result = vector_service.process_pdf_text(content, f"{topic}_study.pdf")
            document_ids[topic] = result["document_id"]

        # RAGìš© ë¬¸ì„œë³„ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜
        rag_contexts = {}

        # ê° ë¬¸ì„œì—ì„œ í•µì‹¬ ê°œë… ê²€ìƒ‰
        search_queries = {
            "algorithm": ["ë™ì ê³„íšë²•", "ë©”ëª¨ì´ì œì´ì…˜", "íš¨ìœ¨ì "],
            "database": ["ê´€ê³„í˜•", "í…Œì´ë¸”", "SQL"],
            "web": ["í”„ë¡ íŠ¸ì—”ë“œ", "ë°±ì—”ë“œ", "React"]
        }

        for topic, queries in search_queries.items():
            doc_id = document_ids[topic]
            topic_context = ""

            for query in queries:
                results = vector_service.search_in_document(query, doc_id, top_k=2)
                for result in results:
                    topic_context += result["text"] + " "

            rag_contexts[topic] = {
                "document_id": doc_id,
                "context": topic_context.strip(),
                "context_length": len(topic_context),
                "ready_for_rag": len(topic_context) > 100
            }

        # RAG ì¤€ë¹„ë„ ê²€ì¦
        assert len(rag_contexts) == 3
        for topic, context_info in rag_contexts.items():
            assert context_info["ready_for_rag"] is True
            assert context_info["context_length"] > 100
            assert context_info["document_id"] in document_ids.values()

        print(f"ğŸ¤– RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ:")
        for topic, context_info in rag_contexts.items():
            print(f"   - {topic}: {context_info['context_length']}ì (ë¬¸ì„œ ID: {context_info['document_id'][:8]}...)")

        print("âœ… RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")

    def test_database_switching(self, vector_service):
        """ë°ì´í„°ë² ì´ìŠ¤ ì „í™˜ í…ŒìŠ¤íŠ¸"""
        # ì´ˆê¸°: weaviate
        assert vector_service.db_type == "weaviate"

        # chromaë¡œ ì „í™˜
        success = vector_service.switch_database("chroma")
        assert success is True
        assert vector_service.db_type == "chroma"

        # ë‹¤ì‹œ weaviateë¡œ ì „í™˜
        success = vector_service.switch_database("weaviate")
        assert success is True
        assert vector_service.db_type == "weaviate"

        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì „í™˜ í…ŒìŠ¤íŠ¸ ì„±ê³µ")

    def test_real_pdf_workflow(self, vector_service, sample_pdf_path):
        """ì‹¤ì œ PDF ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        if not sample_pdf_path.exists():
            pytest.skip("ìƒ˜í”Œ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

        # PDF í…ìŠ¤íŠ¸ ì§ì ‘ ì¶”ì¶œí•˜ì—¬ í…ŒìŠ¤íŠ¸
        try:
            import fitz
            doc = fitz.open(str(sample_pdf_path))
            pdf_text = ""
            for page in doc:
                pdf_text += page.get_text()
            doc.close()
        except ImportError:
            pytest.skip("PyMuPDFê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ë²¡í„° ì²˜ë¦¬
        process_result = vector_service.process_pdf_text(pdf_text, sample_pdf_path.name)

        assert process_result["success"] is True
        assert process_result["total_chunks"] > 0
        assert "document_id" in process_result

        # ì‹¤ì œ ê°œë… ê²€ìƒ‰
        queries = ["ë™ì ê³„íšë²•", "ë©”ëª¨ì´ì œì´ì…˜", "ìµœì í™”"]

        for query in queries:
            search_results = vector_service.search_documents(query, top_k=3)
            assert len(search_results) > 0

            # ìœ ì‚¬ë„ê°€ ìœ íš¨í•œ ë²”ìœ„ì¸ì§€ í™•ì¸
            for result in search_results:
                assert -1.0 <= result["similarity"] <= 1.0

        print(f"âœ… ì‹¤ì œ PDF ì›Œí¬í”Œë¡œìš° ì„±ê³µ:")
        print(f"   - íŒŒì¼: {sample_pdf_path.name}")
        print(f"   - ë¬¸ì„œ ID: {process_result['document_id'][:8]}...")
        print(f"   - í…ìŠ¤íŠ¸: {len(pdf_text):,}ì")
        print(f"   - ì²­í¬: {process_result['total_chunks']}ê°œ")
        print(f"   - ê²€ìƒ‰ ì¿¼ë¦¬: {len(queries)}ê°œ í…ŒìŠ¤íŠ¸")

    def test_performance_benchmark(self, vector_service):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        import time

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_texts = [
            "ë™ì ê³„íšë²•ì€ ìµœì í™” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. " * 50,
            "FastAPIëŠ” Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. " * 50,
            "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤. " * 50
        ]

        # ì €ì¥ ì„±ëŠ¥ ì¸¡ì •
        store_start = time.time()
        document_ids = []

        for i, text in enumerate(test_texts):
            result = vector_service.process_pdf_text(text, f"benchmark_{i}.pdf")
            assert result["success"] is True
            document_ids.append(result["document_id"])

        store_time = time.time() - store_start

        # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
        search_queries = ["ë™ì ê³„íšë²•", "FastAPI", "ë²¡í„°"]
        search_times = []

        for query in search_queries:
            search_start = time.time()
            results = vector_service.search_documents(query, top_k=5)
            search_time = time.time() - search_start
            search_times.append(search_time)

            assert len(results) > 0

        avg_search_time = sum(search_times) / len(search_times)

        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        assert store_time < 5.0, f"ì €ì¥ ì‹œê°„ ì´ˆê³¼: {store_time:.2f}ì´ˆ"
        assert avg_search_time < 0.1, f"ê²€ìƒ‰ ì‹œê°„ ì´ˆê³¼: {avg_search_time:.3f}ì´ˆ"

        print(f"âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬:")
        print(f"   - ì €ì¥: {store_time:.2f}ì´ˆ ({len(test_texts)}ê°œ ë¬¸ì„œ)")
        print(f"   - ë¬¸ì„œ IDë“¤: {[doc_id[:8] + '...' for doc_id in document_ids]}")
        print(f"   - ê²€ìƒ‰: {avg_search_time:.3f}ì´ˆ (í‰ê· )")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])