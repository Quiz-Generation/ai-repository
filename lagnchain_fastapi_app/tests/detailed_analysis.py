#!/usr/bin/env python3
"""
PDF ì¶”ì¶œê¸° ìƒì„¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- í…ìŠ¤íŠ¸ í’ˆì§ˆ ì„¸ë¶€ ë¶„ì„
- ë¬¸ë‹¨ êµ¬ì¡° ë¶„ì„
- ë¬¸ì ë¶„í¬ ë¶„ì„
- ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¹„êµ
"""
import time
import os
import re
from collections import Counter
from lagnchain_fastapi_app.app.services.pdf_extractor import PDFExtractorFactory


def detailed_analysis():
    """PDF ì¶”ì¶œê¸° ìƒì„¸ ë¶„ì„"""
    print("ğŸ”¬ PDF ì¶”ì¶œê¸° ìƒì„¸ ë¶„ì„")
    print("=" * 80)

    # í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ë“¤
    pdf_files = [
        "static/temp/lecture-DynamicProgramming.pdf",
        "static/temp/AWS Certified Solutions Architect Associate SAA-C03.pdf"
    ]

    extractors = ["pdfminer", "pdfplumber", "pymupdf"]
    all_results = {}

    for pdf_file in pdf_files:
        if not os.path.exists(pdf_file):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_file}")
            continue

        file_size_mb = round(os.path.getsize(pdf_file) / (1024 * 1024), 2)
        print(f"\nğŸ“„ {os.path.basename(pdf_file)} ({file_size_mb}MB)")
        print("=" * 80)

        results = {}

        for extractor_name in extractors:
            print(f"\nğŸ”§ {extractor_name.upper()} ë¶„ì„ ì¤‘...")

            try:
                start_time = time.time()
                extractor = PDFExtractorFactory.create(extractor_name)
                text = extractor.extract_text(pdf_file)
                processing_time = round(time.time() - start_time, 3)

                # ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
                analysis = perform_detailed_analysis(text, extractor_name)
                analysis['processing_time'] = processing_time
                analysis['file_size_mb'] = file_size_mb
                analysis['success'] = True

                results[extractor_name] = analysis
                print(f"  âœ… ë¶„ì„ ì™„ë£Œ: {processing_time}ì´ˆ")

            except Exception as e:
                print(f"  âŒ ì‹¤íŒ¨: {str(e)}")
                results[extractor_name] = {"success": False, "error": str(e)}

        all_results[pdf_file] = results

        # íŒŒì¼ë³„ ìƒì„¸ ë¹„êµ
        print_detailed_comparison(pdf_file, results)

    # ì „ì²´ ì¢…í•© ë¶„ì„
    print_comprehensive_analysis(all_results)

    return all_results


def perform_detailed_analysis(text: str, extractor_name: str) -> dict:
    """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰"""

    # ê¸°ë³¸ í†µê³„
    text_length = len(text)
    lines = text.split('\n')
    non_empty_lines = [line for line in lines if line.strip()]

    # ë¬¸ì ë¶„í¬ ë¶„ì„
    char_analysis = analyze_characters(text)

    # ì¤„ë°”ê¿ˆ ë° ê³µë°± ë¶„ì„
    whitespace_analysis = analyze_whitespace(text)

    # ë¬¸ë‹¨ êµ¬ì¡° ë¶„ì„
    paragraph_analysis = analyze_paragraphs(text)

    # ì–¸ì–´ë³„ ë¶„ì„
    language_analysis = analyze_languages(text)

    # íŠ¹ìˆ˜ ë¬¸ì ë° ê¸°í˜¸ ë¶„ì„
    symbol_analysis = analyze_symbols(text)

    return {
        'text': text,
        'text_length': text_length,
        'line_count': len(lines),
        'non_empty_line_count': len(non_empty_lines),
        'char_analysis': char_analysis,
        'whitespace_analysis': whitespace_analysis,
        'paragraph_analysis': paragraph_analysis,
        'language_analysis': language_analysis,
        'symbol_analysis': symbol_analysis,
        'text_preview': text[:500]
    }


def analyze_characters(text: str) -> dict:
    """ë¬¸ì ë¶„í¬ ë¶„ì„"""
    korean_chars = sum(1 for char in text if 0xAC00 <= ord(char) <= 0xD7A3)
    english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
    digit_chars = sum(1 for char in text if char.isdigit())
    space_chars = sum(1 for char in text if char == ' ')
    newline_chars = text.count('\n')
    tab_chars = text.count('\t')

    total_chars = len(text)

    return {
        'korean_count': korean_chars,
        'korean_ratio': round(korean_chars / total_chars * 100, 2) if total_chars > 0 else 0,
        'english_count': english_chars,
        'english_ratio': round(english_chars / total_chars * 100, 2) if total_chars > 0 else 0,
        'digit_count': digit_chars,
        'digit_ratio': round(digit_chars / total_chars * 100, 2) if total_chars > 0 else 0,
        'space_count': space_chars,
        'space_ratio': round(space_chars / total_chars * 100, 2) if total_chars > 0 else 0,
        'newline_count': newline_chars,
        'newline_ratio': round(newline_chars / total_chars * 100, 2) if total_chars > 0 else 0,
        'tab_count': tab_chars
    }


def analyze_whitespace(text: str) -> dict:
    """ê³µë°± ë° ì¤„ë°”ê¿ˆ ë¶„ì„"""
    single_newlines = text.count('\n') - text.count('\n\n') * 2
    double_newlines = text.count('\n\n')
    triple_newlines = text.count('\n\n\n')

    # ì—°ì†ëœ ê³µë°± ë¶„ì„
    multiple_spaces = len(re.findall(r'  +', text))  # 2ê°œ ì´ìƒ ì—°ì† ê³µë°±

    # ì¤„ ë ê³µë°±
    lines_with_trailing_spaces = len([line for line in text.split('\n') if line.endswith(' ')])

    return {
        'single_newlines': single_newlines,
        'double_newlines': double_newlines,
        'triple_newlines': triple_newlines,
        'multiple_spaces': multiple_spaces,
        'lines_with_trailing_spaces': lines_with_trailing_spaces
    }


def analyze_paragraphs(text: str) -> dict:
    """ë¬¸ë‹¨ êµ¬ì¡° ë¶„ì„"""
    # ë¹ˆ ì¤„ë¡œ êµ¬ë¶„ëœ ë¬¸ë‹¨
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

    # ë¬¸ë‹¨ ê¸¸ì´ ë¶„ì„
    paragraph_lengths = [len(p) for p in paragraphs]
    avg_paragraph_length = sum(paragraph_lengths) / len(paragraph_lengths) if paragraph_lengths else 0

    # ì§§ì€ ë¬¸ë‹¨ (í•œ ì¤„) vs ê¸´ ë¬¸ë‹¨
    short_paragraphs = len([p for p in paragraphs if '\n' not in p])
    long_paragraphs = len([p for p in paragraphs if '\n' in p])

    return {
        'total_paragraphs': len(paragraphs),
        'avg_paragraph_length': round(avg_paragraph_length, 1),
        'short_paragraphs': short_paragraphs,
        'long_paragraphs': long_paragraphs,
        'shortest_paragraph': min(paragraph_lengths) if paragraph_lengths else 0,
        'longest_paragraph': max(paragraph_lengths) if paragraph_lengths else 0
    }


def analyze_languages(text: str) -> dict:
    """ì–¸ì–´ë³„ ë¶„ì„"""
    # í•œê¸€ ë‹¨ì–´ ìˆ˜ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
    korean_words = len(re.findall(r'[ê°€-í£]+', text))

    # ì˜ë¬¸ ë‹¨ì–´ ìˆ˜
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))

    # ìˆ«ì íŒ¨í„´
    numbers = len(re.findall(r'\d+', text))

    return {
        'korean_words': korean_words,
        'english_words': english_words,
        'numbers': numbers,
        'total_words': korean_words + english_words
    }


def analyze_symbols(text: str) -> dict:
    """íŠ¹ìˆ˜ ë¬¸ì ë° ê¸°í˜¸ ë¶„ì„"""
    bullets = text.count('â€¢') + text.count('Â·') + text.count('-')
    parentheses = text.count('(') + text.count(')')
    brackets = text.count('[') + text.count(']')
    quotes = text.count('"') + text.count("'")

    return {
        'bullets': bullets,
        'parentheses': parentheses,
        'brackets': brackets,
        'quotes': quotes
    }


def print_detailed_comparison(pdf_file: str, results: dict):
    """íŒŒì¼ë³„ ìƒì„¸ ë¹„êµ ì¶œë ¥"""
    print(f"\nğŸ“Š {os.path.basename(pdf_file)} ìƒì„¸ ë¶„ì„ ê²°ê³¼")
    print("=" * 80)

    # 1. ê¸°ë³¸ ì„±ëŠ¥ ë¹„êµ
    print("\n1ï¸âƒ£ ì„±ëŠ¥ ë¹„êµ")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            speed = round(result['file_size_mb'] / result['processing_time'], 2) if result['processing_time'] > 0 else 0
            print(f"{extractor_name.upper():<12}: {result['processing_time']:>6.3f}ì´ˆ | {result['text_length']:>8,}ì | {speed:>6.2f} MB/ì´ˆ")

    # 2. ë¬¸ì ë¶„í¬ ë¹„êµ
    print("\n2ï¸âƒ£ ë¬¸ì ë¶„í¬ ë¹„êµ")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            char = result['char_analysis']
            print(f"{extractor_name.upper():<12}: í•œê¸€ {char['korean_ratio']:>5.1f}% | ì˜ë¬¸ {char['english_ratio']:>5.1f}% | ìˆ«ì {char['digit_ratio']:>4.1f}% | ê³µë°± {char['space_ratio']:>5.1f}%")

    # 3. ì¤„ë°”ê¿ˆ ì„¸ë¶€ ë¶„ì„
    print("\n3ï¸âƒ£ ì¤„ë°”ê¿ˆ ì„¸ë¶€ ë¶„ì„")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            ws = result['whitespace_analysis']
            print(f"{extractor_name.upper():<12}: ë‹¨ì¼ {ws['single_newlines']:>4}ê°œ | ì´ì¤‘ {ws['double_newlines']:>3}ê°œ | ì‚¼ì¤‘ {ws['triple_newlines']:>2}ê°œ | ì—°ì†ê³µë°± {ws['multiple_spaces']:>3}ê°œ")

    # 4. ë¬¸ë‹¨ êµ¬ì¡° ë¶„ì„
    print("\n4ï¸âƒ£ ë¬¸ë‹¨ êµ¬ì¡° ë¶„ì„")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            para = result['paragraph_analysis']
            print(f"{extractor_name.upper():<12}: ë¬¸ë‹¨ {para['total_paragraphs']:>3}ê°œ | í‰ê· ê¸¸ì´ {para['avg_paragraph_length']:>6.1f} | ì§§ì€ë¬¸ë‹¨ {para['short_paragraphs']:>3}ê°œ | ê¸´ë¬¸ë‹¨ {para['long_paragraphs']:>3}ê°œ")

    # 5. ì–¸ì–´ë³„ ë‹¨ì–´ ìˆ˜
    print("\n5ï¸âƒ£ ì–¸ì–´ë³„ ë‹¨ì–´ ìˆ˜")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            lang = result['language_analysis']
            print(f"{extractor_name.upper():<12}: í•œê¸€ë‹¨ì–´ {lang['korean_words']:>4}ê°œ | ì˜ë¬¸ë‹¨ì–´ {lang['english_words']:>5}ê°œ | ìˆ«ì {lang['numbers']:>4}ê°œ")

    # 6. í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¹„êµ (ì²˜ìŒ 200ì)
    print("\n6ï¸âƒ£ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ë¹„êµ (ì²˜ìŒ 200ì)")
    print("-" * 50)
    for extractor_name, result in results.items():
        if result["success"]:
            preview = result['text_preview'][:200].replace('\n', '\\n')
            print(f"\n{extractor_name.upper()}:")
            print(f"  {preview}...")


def print_comprehensive_analysis(all_results: dict):
    """ì „ì²´ ì¢…í•© ë¶„ì„ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸ† ì „ì²´ ì¢…í•© ë¶„ì„")
    print("=" * 80)

    # íŒŒì¼ë³„ ì„±ëŠ¥ ìš”ì•½
    print("\nğŸ“ˆ íŒŒì¼ë³„ ì„±ëŠ¥ ìš”ì•½")
    print("-" * 50)
    for pdf_file, results in all_results.items():
        print(f"\nğŸ“„ {os.path.basename(pdf_file)}:")
        performance_ranking = []

        for extractor_name, result in results.items():
            if result["success"]:
                speed = round(result['file_size_mb'] / result['processing_time'], 2) if result['processing_time'] > 0 else 0
                performance_ranking.append((extractor_name, speed, result['processing_time']))

        # ì†ë„ìˆœ ì •ë ¬
        performance_ranking.sort(key=lambda x: x[1], reverse=True)

        for i, (extractor_name, speed, time) in enumerate(performance_ranking, 1):
            print(f"  {i}ìœ„. {extractor_name.upper():<12}: {time:>6.3f}ì´ˆ ({speed:>6.2f} MB/ì´ˆ)")

    # í’ˆì§ˆ ë¶„ì„ ìš”ì•½
    print("\nğŸ¯ í’ˆì§ˆ ë¶„ì„ ìš”ì•½")
    print("-" * 50)

    for pdf_file, results in all_results.items():
        print(f"\nğŸ“„ {os.path.basename(pdf_file)}:")

        # ì¤„ë°”ê¿ˆ í’ˆì§ˆ (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        newline_quality = []
        for extractor_name, result in results.items():
            if result["success"]:
                ws = result['whitespace_analysis']
                total_newlines = ws['single_newlines'] + ws['double_newlines'] * 2 + ws['triple_newlines'] * 3
                newline_quality.append((extractor_name, total_newlines))

        newline_quality.sort(key=lambda x: x[1])

        print("  ì¤„ë°”ê¿ˆ í’ˆì§ˆ (ì ì„ìˆ˜ë¡ ì¢‹ìŒ):")
        for i, (extractor_name, count) in enumerate(newline_quality, 1):
            print(f"    {i}ìœ„. {extractor_name.upper():<12}: {count:>5}ê°œ")


if __name__ == "__main__":
    detailed_analysis()