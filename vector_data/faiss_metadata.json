{
  "documents": {
    "af887d22-6b67-4286-a4c1-93b73e756036_0": {
      "text": "Kookmin University, Seoul, Korea\nJaekoo Lee1\n1 Ph.D., Assistant Professor, School of Software, College of Computer Science\njaekoo@kookmin.ac.kr\n2í•™ê¸°, 2022\nì‹¬ì¸µí•™ìŠµ(deep learning; DL) ìµœì‹ ê¸°ìˆ \n: PyTorch ì¤‘ì‹¬ìœ¼ë¡œ\njaekoo@kookmin.ac.kr\n\nâš«\nIntroduction\nâš«\nFoundation for neural network and deep learning\nï‚„Background for deep learning\nï‚„Representation learning (deep learning)\nï‚„Neural networks\nâš«\nHands-on experience with PYTORCH\nï‚„Frameworks for deep learning\nï‚„On Google Colab\nâš«\nDeep learning for computer vision\nï‚„Convolutional neural networks\nï‚„Object detection\nï‚„Hands-on\nâš«\nDeep learning for sequence data (time series)\nï‚„Recurrent neural networks\nï‚„Transformer\nï‚„Hands-on\nâš«\nConclusion\nOutline\n2\njaekoo@kookmin.ac.kr\n\nâš«About me\nï‚„Jaekoo lee\nâ–ªKookmin university\nâ–ªAssistant professor, School of software, College of computer science\nâ–ªjaekoo@kookmin.ac.kr\nï‚„Machine intelligence (MI) lab. (http://mi.kookmin.ac.kr/)\nâ–ªOur research topics",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 0,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 906
      },
      "faiss_idx": 0
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_1": {
      "text": "aekoo@kookmin.ac.kr\nï‚„Machine intelligence (MI) lab. (http://mi.kookmin.ac.kr/)\nâ–ªOur research topics\n3\nIntroduction\nArtificial intelligence\nMachine (deep) learning\nSystem (Robot) for intelligence\nBio or\nhealth-care analysis\nIoT (e.g. sensor) \nanalysis\nself-driving car\nor drone\nSecurity\nFundamentals\nApps.\njaekoo@kookmin.ac.kr\n\nâš«This tutorial intended to be the first course in deep learning\nâš«Tutorial objectives:\nï‚„understand fundamentals of deep learning\nï‚„have hands-on experience (simple practice) with PyTorch\nï‚„motivate to learn recent breakthroughs in deep learning\n4\nIntroduction\njaekoo@kookmin.ac.kr\n\n5\nIntroduction\njaekoo@kookmin.ac.kr\n\nâš«Artificial intelligence (AI):\nï‚„the simulation of human intelligence processes by machines (computer systems)\nï‚„include (major components of AI)\nâ–ªlearning (the acquisition of information and rules for using the information), \nâ–ªreasoning (using rules to reach approximate or definite conclusions),",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 1,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 938
      },
      "faiss_idx": 1
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_2": {
      "text": "for using the information), \nâ–ªreasoning (using rules to reach approximate or definite conclusions),\nâ–ªknowledge,\nâ–ªlanguage understanding, and\nâ–ªself-correction\nï‚„Goal:\nâ–ªA machine that thinks or acts like a human\n6\nIntroduction\njaekoo@kookmin.ac.kr\n\nâš«AI in production\nï‚„Speech recognition\nï‚„Recommender systems\nï‚„Autonomous driving\nï‚„Real-time object recognition\nï‚„Robotics\nï‚„Real-time language translation\nï‚„Many moreâ€¦\n7\nIntroduction\njaekoo@kookmin.ac.kr\n\nâš«A brief timeline of historical events in AI\nIntroduction\n8\nearly AI stirs excitement\nmachine learning begins to flourish\ndeep learning\nbreakthroughs drive AI boom\njaekoo@kookmin.ac.kr\n\nâš«Start of AI = Acting humanly: the Turing test (1950)\nï‚„â€œCan machines think?â€ (the imitation game)\nâš«Understanding and imitating the brain\nï‚„[1950] one neuron (e.g. perceptron)\nâ†’[1980] several neuron (e.g. multi layer perceptron)\nâ†’[2010] many neuron (e.g. deep neural networks)",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 2,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 906
      },
      "faiss_idx": 2
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_3": {
      "text": "[1980] several neuron (e.g. multi layer perceptron)\nâ†’[2010] many neuron (e.g. deep neural networks)\n9\nIntroduction\njaekoo@kookmin.ac.kr\n\nâš«Reasons for deep learning's success\nâš«Performance with |data|\n10\nIntroduction\nDNN\nGPU\nBIG DATA\njaekoo@kookmin.ac.kr\n\nâš«Background for deep learning\nâš«Representation learning (deep learning)\nâš«Neural networks\n11\nFoundation for neural network and deep learning\njaekoo@kookmin.ac.kr\n\nâš«Field of artificial intelligence\n12\nBackground for deep learning\nâš«Key components of learning\nï‚„Gray box: can be learned from data\njaekoo@kookmin.ac.kr\n\nâš«Definition of machine learning\nï‚„Arthur Samuel (1959): The Machine Learning is a field of study that gives computers \nthe ability to learn without being explicitly programmed\nï‚„Tom Michell (1998): A program is said to learn from experience  â€˜Eâ€™  with respect to \nany task  â€˜Tâ€™  and some measure of performance  â€˜Pâ€™  if their performance in  â€˜Tâ€™",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 3,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 910
      },
      "faiss_idx": 3
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_4": {
      "text": "ith respect to \nany task  â€˜Tâ€™  and some measure of performance  â€˜Pâ€™  if their performance in  â€˜Tâ€™  \nmeasured by  â€˜Pâ€™ , improvement experience  â€˜Eâ€™ \nâš«Basic premise of learning \nâ€œusing a set of observations to uncover an underlying processâ€\nbroad premise â†’many variations\n13\nBackground for deep learning\njaekoo@kookmin.ac.kr\n\nâš«Variations of learning\n14\nBackground for deep learning\nSupervised learning\ngiven (x, y)\nx is data, y is its label\nGoal:\nlearn a function\nto map x â†’ y\nExamples:\nClassification\nRegression\nObject detection\nSegmentation\nImage captioning\nUnsupervised learning\ngiven (x)\njust data, no label\nGoal:\nlearn some underlying hidden \nstructure of the data\nExamples:\nClustering\nDimensionality reduction\nFeature learning\nDensity estimation\nReinforcement learning\ngiven\nProblems involving an agent\ninteracting with an environment which \nprovides numeric reward signals\nGoal:\nLearn how to take actions in order to",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 4,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 921
      },
      "faiss_idx": 4
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_5": {
      "text": "an environment which \nprovides numeric reward signals\nGoal:\nLearn how to take actions in order to \nmaximize reward\nExamples:\nRobotics\nSemi-supervised learning\njaekoo@kookmin.ac.kr\n\nâš«Components of learning\nï‚„metaphor: credit approval\nâ–ªapplicant information:\nï‚„formalization\n15\nBackground for deep learning\n?\ncomponent\nsymbol\nmetaphor of credit approval\ninput\nx\ncustomer application\noutput\nğ‘¦\napprove or deny\ntarget distribution\nğ‘“= ğ‘ƒğ‘¦x\nideal credit approval formula\ndata\n(x1, ğ‘¦1), (x2, ğ‘¦2),â€¦, (xğ‘, ğ‘¦ğ‘)\nhistorical records\nhypothesis\nğ‘”: ğ’³â†’ğ’´\nformula to be used\njaekoo@kookmin.ac.kr\n\nâš«Setup for machine learning\nï‚„supervised learning case,\n16\nBackground for deep learning\ndata\nobjective (=loss=cost) \nfunction\noptimization\nand control\n(learning model)\njaekoo@kookmin.ac.kr\n\nâš«Effect of representation learning in algebraic viewpoint\nï‚„Representation matter\nâ–ªexample of different coordinates\nnot only the mapping from representation (e.",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 5,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 923
      },
      "faiss_idx": 5
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_6": {
      "text": "Representation matter\nâ–ªexample of different coordinates\nnot only the mapping from representation (e.g. input) to output\nbut also the representation itself\ncan find good features more rapidly than human â†’better performance\n17\nRepresentation learning (deep learning)\njaekoo@kookmin.ac.kr\n\nâš«Representation learning\nï‚„Automatically discover the representations \nneeded for feature detection or classification from raw data\nï‚„toy example\n18\nRepresentation learning (deep learning)\ninput\noutput\nhidden \nrepresentation\nclass scores\nfeature representation\njaekoo@kookmin.ac.kr\n\nâš«Deep learning (or representation learning)\nï‚„deep neural network with multiple levels of linear / non-linear operations\nâ–ªintroducing representation that are expressed in terms of other (simpler) representations\nâ–ªeach stage: a kind of trainable feature transform\nâ–ªhierarchy of representations with increasing level of abstraction\nï‚„learning representation â†’data-driven features",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 6,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 943
      },
      "faiss_idx": 6
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_7": {
      "text": "representations with increasing level of abstraction\nï‚„learning representation â†’data-driven features\nï‚„deep neural network == universal approximator\n19\nRepresentation learning (deep learning)\nImage\npixel â†’edge â†’texton â†’motif \nâ†’part â†’object\nText\ncharacter â†’word â†’word group \nâ†’clause â†’sentence â†’story\nSpeech\nsample â†’spectral band â†’\nsound â†’â€¦ â†’phone â†’\nphoneme â†’word â†’\njaekoo@kookmin.ac.kr\n\nâš«Comparisons between traditional machine learning and deep learning\n20\nRepresentation learning (deep learning)\njaekoo@kookmin.ac.kr\n\nâš«Recap: From linear algebra\nï‚„feature vector\nâ–ªtoy example (image â†’vector)\nï‚„inner product (similarity)\n21\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Basic operation in a node of neural networks\nï‚„inner product == linear function\nï‚„activation function == non-linear function\nâ–ªvarious activation functions\n22\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Perceptron\nï‚„combination of linear (inner product)  + nonlinear",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 7,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 917
      },
      "faiss_idx": 7
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_8": {
      "text": "ural networks\njaekoo@kookmin.ac.kr\n\nâš«Perceptron\nï‚„combination of linear (inner product)  + nonlinear\nï‚„started from imitating the brain\nâ–ª1940-1960: cybernetics\n23\nNeural networks\njaekoo@kookmin.ac.kr\n\nï‚„toy example\nâ–ªa perceptron â„(x) = sign(wTx)\nâ–ªthe perceptron implements\nâ„(x) = sign(wTx)\n24\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«XOR problem\nâ–ªmission impossible! linear classifier (perceptron) cannot solve this!\nï‚„another point of view: learning XOR\n25\nNeural networks\nx1 x2\nx1 XOR x2\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\njaekoo@kookmin.ac.kr\n\nï‚„Multi layer perceptron (MLP)\nâ–ª1980-1990: connectionism (or parallel distributed processing) â†’neural networks\nâ–ªtoy example for multi layer perceptron (MLP)\n26\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Linearity in neural networks\nï‚„Neural networks for visual recognition - problem: image classification\nï‚„a toy example with linear classifier\n27\nNeural networks\nâ†’find parameter (W, b) for correct answer!",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 8,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 928
      },
      "faiss_idx": 8
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_9": {
      "text": "ï‚„a toy example with linear classifier\n27\nNeural networks\nâ†’find parameter (W, b) for correct answer!\njaekoo@kookmin.ac.kr\n\nâš«Linearity of neural networks for visual recognition\nï‚„linear classifier\nâ–ªas algebraic viewpoint\nâ–ªas geometric viewpoint\nâ†’extension to deep neural network == multiple levels of linear + nonlinear operations\n28\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«End-to-end structure\n29\nNeural networks\n1. linear transformation by ğ–ğŸ\n2. translation by ğ›ğŸ\n3. point wise application by activation function\njaekoo@kookmin.ac.kr\n\nâš«Zoo of neural networks\n30\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Training and inference of network\n31\nNeural networks\nâ€¦\nâ€œdogâ€\n=?\nerror\nâ€œcatâ€\nlabels\nforward\nbackward\nâ€¦\nforward\npredicting label for unknown data\nlearned model\ntraining\ninference\nğ·\nN\nğ·unknown\njaekoo@kookmin.ac.kr\n\nâš«Computational graph\nï‚„sequential instructions (flowchart)\nï‚„how concepts are related to each other",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 9,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 908
      },
      "faiss_idx": 9
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_10": {
      "text": "âš«Computational graph\nï‚„sequential instructions (flowchart)\nï‚„how concepts are related to each other\nï‚„example\n32\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«What happen? in neural networks\n1.\nforward propagation yields an inferred ğ‘¦for x\n2. loss function used to calculate difference between real ğ‘¦and predicted ğ‘¦\n3. weights are adjusted during backward propagation\n4. repeat the process\n33\nNeural networks\nâ†’decomposition of chain rule\njaekoo@kookmin.ac.kr\n\nâš«Error backpropagation and its chain rule (via computational graph)\n34\nNeural networks\nsubexpressions\njaekoo@kookmin.ac.kr\n\n35\nNeural networks\nâš«Recap: Derivative\nâš«Recap: Gradient descent \njaekoo@kookmin.ac.kr\n\nâš«Chain rule (via computational graph) in neural network \n36\nNeural networks\njaekoo@kookmin.ac.kr\n\n37\nNeural networks\nâš«Toy example for backpropagation\njaekoo@kookmin.ac.kr\n\nâš«Iterative optimization\nï‚„instead of analytically setting\nï‚„gradient descent is a very general algorithm",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 10,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 934
      },
      "faiss_idx": 10
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_11": {
      "text": "erative optimization\nï‚„instead of analytically setting\nï‚„gradient descent is a very general algorithm\nâ–ªgradient == the derivative of vector functions\nâ–ªdirection of greatest increase of a function\nâ–ªrepeatably update weights, iterate to next step\nw ğ‘¡+ 1 = w ğ‘¡âˆ’ğœ‚âˆ‡Ein(w(ğ‘¡))\nâ–ªexample\n38\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Extensions and variants of gradient descent\nï‚„updating time of parameters\nâ–ªfull batch gradient descent\nâ–ªstochastic gradient descent\nâ–ªmini-batch gradient descent\nï‚„optimizer\nâ–ªSGD + momentum\nâ–ªNAG (nesterov accelerated gradient)\nâ–ªAdagrad (adaptive gradient)\nâ–ªAdadelta\nâ–ªRMSProp\nâ–ªAdam (adaptive gradient + moment)\n39\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«A toy example of training (weight updating) principle\n40\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Power of deep neural networks (deep learning)\nï‚„end-to-end learning\nâ–ªlearn data-driven features from data\nâ–ªhierarchical features & abstraction",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 11,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 905
      },
      "faiss_idx": 11
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_12": {
      "text": "ng)\nï‚„end-to-end learning\nâ–ªlearn data-driven features from data\nâ–ªhierarchical features & abstraction\nï‚„importance of the depth of neural networks â†’powerful representation learning \n41\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Regularization\nï‚„toy example\nï‚„ex. constrained optimization - norm penalty\nâ–ªintuition of weight decay\nï‚„other strategies in deep learning\nâ–ªbagging (bootstrap aggregation)\nâ–ªdropout\nâ–ªbatch/layer/instance normlization\nâ–ªetc\n42\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Whole process of deep learning\nï‚„training == exploration of hypothesis (parameters) space\n43\nNeural networks\njaekoo@kookmin.ac.kr\n\nâš«Frameworks for deep learning\nâš«On Google Colab\n44\nHands-on experience with PYTORCH \njaekoo@kookmin.ac.kr\n\n45\nFrameworks for deep learning\nâš«Machine learning tools\njaekoo@kookmin.ac.kr\n\nâš«Implementation of deep neural networks\n== can be assembled like LEGO\n46\nFrameworks for deep learning\njaekoo@kookmin.",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 12,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 911
      },
      "faiss_idx": 12
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_13": {
      "text": "f deep neural networks\n== can be assembled like LEGO\n46\nFrameworks for deep learning\njaekoo@kookmin.ac.kr\n\n47\nFrameworks for deep learning\nSoftware\nCreator\nSoftware license\nOpen\nsource\nPlatform\nWritten in\nInterface\nOpenMP\nsupport\nOpenCL support\nCUDA support\nParallel\nexecution\n(multi\nnode)\nAutomatic differentiation\nHas\npretrained\nmodels\nRecurrent nets Convolutional nets\nRBM/DBNs\nApache MXNet\nApache Software Foundation Apache 2.0\nYes\nLinux, macOS,\nWindows, AWS,\nAndroid, iOS,\nJavaScript\nSmall C++ core library\nC++, Python,\nJulia, Matlab,\nJavaScript,\nGo, R, Scala,\nPerl\nYes\nOn roadmap\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nCaffe\nBerkeley Vision and Learning\nCenter\nBSD license\nYes\nLinux, macOS,\nWindows\nC++\nPython,\nMATLAB,\nC++\nYes\nUnder development\nYes\nYes\nYes\nYes\nYes\nYes\nChainer\nPreferred Networks\nMIT license\nYes\nLinux, macOS,\nWindows\nPython\nNo\nNo\nYes\nYes\nYes\nYes\nYes\nKeras\nFranÃ§ois Chollet\nMIT license\nYes",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 13,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 900
      },
      "faiss_idx": 13
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_14": {
      "text": "e\nYes\nLinux, macOS,\nWindows\nPython\nNo\nNo\nYes\nYes\nYes\nYes\nYes\nKeras\nFranÃ§ois Chollet\nMIT license\nYes\nLinux, macOS,\nWindows\nPython\nPython, R\nOnly if using\nTheano as\nbackend\nUnder development for\nthe Theano backend (and\non roadmap for the\nTensorFlow backend)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nMicrosoft\nCognitive Toolkit\nMicrosoft Research\nMIT license\nYes\nWindows, Linux,\n(macOS via Docker\non roadmap)\nC++\nPython\n(Keras), C++,\nCommand\nline,\nBrainScript,\n(.NET on\nroadmap\n[)\nYes\nNo\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nPyTorch\nAdam Paszke, Sam Gross,\nSoumith Chintala, Gregory\nChanan\nBSD license\nYes\nLinux, macOS,\nWindows\nPython, C, CUDA\nPython\nYes\nVia separately\nmaintained package\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nTensorFlow\nGoogle Brain team\nApache 2.0\nYes\nLinux, macOS,\nWindows, Android\nC++, Python, CUDA\nPython\n(Keras),\nC/C++, Java,\nGo, R, Julia\nNo\nOn roadmap but already\nwith SYCL support\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nTheano",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 14,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 905
      },
      "faiss_idx": 14
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_15": {
      "text": ", Java,\nGo, R, Julia\nNo\nOn roadmap but already\nwith SYCL support\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nTheano\nUniversitÃ© de MontrÃ©al\nBSD license\nYes\nCross-platform Python\nPython\n(Keras)\nYes\nUnder development\nYes\nYes\nYes\nYes\nYes\nYes\nTorch\nRonan Collobert, Koray\nKavukcuoglu, Clement\nFarabet\nBSD license\nYes\nLinux, macOS,\nWindows, Android,\niOS\nC, Lua\nLua, LuaJIT,\nC, utility\nlibrary for\nC++/OpenCL\nYes\nThird party\nimplementations\nYes\nThrough\nTwitter's\nAutograd\nYes\nYes\nYes\nYes\nYes\njaekoo@kookmin.ac.kr\n\n48\nFrameworks for deep learning\nâš«Overview of deep learning tools\njaekoo@kookmin.ac.kr\n\nâš«PyTorch (https://pytorch.org/) â€“ version 1.0 released Dec., 2018\nï‚„enables fast, flexible experimentation and efficient production\nï‚„provides automatic differentiation for all operations on tensors\nï‚„run it all efficiently on GPU \nâ–ªwrap cuDNN, cuBLAS, etc\n49\nFrameworks for deep learning\njaekoo@kookmin.ac.kr\n\nâš«A replacement for numpy",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 15,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 910
      },
      "faiss_idx": 15
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_16": {
      "text": "p cuDNN, cuBLAS, etc\n49\nFrameworks for deep learning\njaekoo@kookmin.ac.kr\n\nâš«A replacement for numpy\nâ‘ looks exactly like numpy\nâ‘¡PyTorch handles gradients for your model\nâ‘¢trivial to run on GPU\nâ–ªjust construct arrays on a different device\n50\nFrameworks for deep learning\njaekoo@kookmin.ac.kr\n\nâš«Example of a dynamic neural network\nï‚„building the graph\nï‚„computing the graph happen at the same time (dynamic)\nâ†”TensorFlow builds graph once, then run many times (static)\nbut TensorFlow 2.0 default dynamic graph\n51\nFrameworks for deep learning\n# back-propagation uses the dynamically created graph\n# a graph is created on the fly\n# create random Tensors for data and weights\ntensor\nforward pass\nbackward pass\njaekoo@kookmin.ac.kr\n\nâš«TensorFlow vs. PyTorch\n52\nFrameworks for deep learning\n# build graph\n# run each iteration\n# graph each iteration\n# TensorFlow (1.x)\n# PyTorch\njaekoo@kookmin.ac.kr\n\nâš«Example of own neural network",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 16,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 917
      },
      "faiss_idx": 16
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_17": {
      "text": "ph each iteration\n# TensorFlow (1.x)\n# PyTorch\njaekoo@kookmin.ac.kr\n\nâš«Example of own neural network\n53\nFrameworks for deep learning\ncomputation graph\ncode for own neural network\njaekoo@kookmin.ac.kr\n\nâš«Convolutional layer in PyTorch\n54\nFrameworks for deep learning\njaekoo@kookmin.ac.kr\n\nâš«Example of using pretrained models\nï‚„with torchvision\n55\nFrameworks for deep learning\nAlexNet\nVGG16\nResNet\njaekoo@kookmin.ac.kr\n\nâš«Google Colab\nï‚„is a Googleâ€™s free cloud service which will let you run your deep learning or machine \nlearning models in cloud\nï‚„accessing Colab\nâ–ªhttps://colab.research.google.com/\n56\nOn Google Colab\njaekoo@kookmin.ac.kr\n\nâš«Using a free GPU for up to 12 hours at a time\nï‚„select \"Runtime,\" \"Change runtime type,\" and this is the pop-up you see:\nâš«Installing libraries\nï‚„currently, software installations within Google Colab are not persistent, in that you \nmust reinstall libraries every time you (re-)connect to an instance",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 17,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 934
      },
      "faiss_idx": 17
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_18": {
      "text": "re not persistent, in that you \nmust reinstall libraries every time you (re-)connect to an instance\nï‚„Colab supports both the pip and apt package managers\n57\nOn Google Colab\njaekoo@kookmin.ac.kr\n\nâš«Uploading and using own data files\nâš«Now execute your codes of a neural network!\n58\nOn Google Colab\njaekoo@kookmin.ac.kr\n\nâš«PyTorch getting started\nï‚„https://pytorch.org/\n59\nOn Google Colab\njaekoo@kookmin.ac.kr\n\nâš«Task: neural networks\n60\nOn Google Colab\ntorch.nn í™œìš©í•œì‹ ê²½ë§ë§Œë“¤ê¸°\njaekoo@kookmin.ac.kr\n\nâš«Task: neural networks\n61\nOn Google Colab\nì‹ ê²½ë§ì •ì˜\nConv2d(in, out, filter)\nLinear(in, out)\njaekoo@kookmin.ac.kr\n\nâš«Task: neural networks\n62\nOn Google Colab\nì„ì˜tensor ì…ë ¥, ê²°ê³¼í™•ì¸\njaekoo@kookmin.ac.kr\n\nâš«Task: neural networks\n63\nOn Google Colab\nì†ì‹¤í•¨ìˆ˜\nì‹ ê²½ë§ì˜ì—°ì‚°ê·¸ë˜í”„\njaekoo@kookmin.ac.kr\n\nâš«Task: neural networks\n64\nOn Google Colab\nì˜¤ë¥˜ì—­ì „íŒŒ\nê°€ì¤‘ì¹˜ê°±ì‹ â€“ ê²½ì‚¬í•˜ê°•ë²•\njaekoo@kookmin.ac.kr\n\nâš«Task: image classification\n65\nOn Google Colab\njaekoo@kookmin.",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 18,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 904
      },
      "faiss_idx": 18
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_19": {
      "text": "ì „íŒŒ\nê°€ì¤‘ì¹˜ê°±ì‹ â€“ ê²½ì‚¬í•˜ê°•ë²•\njaekoo@kookmin.ac.kr\n\nâš«Task: image classification\n65\nOn Google Colab\njaekoo@kookmin.ac.kr\n\nï‚„about data\n66\nOn Google Colab\nì‚¬ì§„â†’ë‹¤ì°¨ì›í–‰ë ¬í˜•íƒœì˜ë°ì´í„°í‘œí˜„\nCIFAR10: ë¹„í–‰ê¸°, ìë™ì°¨, ìƒˆ, ê³ ì–‘ì´, ì‚¬ìŠ´, ê°œ, ê°œêµ¬ë¦¬, ë§, ë°°, íŠ¸ëŸ­ìœ¼ë¡œêµ¬ì„±ëœì‚¬ì§„ì§‘í•©\nê°ì‚¬ì§„ì€R/G/B ì‚¼ìƒ‰ìœ¼ë¡œêµ¬ì„±ëœ32*32 í¬ê¸°ë¥¼ê°€ì§\nâ†’3*32*32\njaekoo@kookmin.ac.kr\n\nï‚„training steps\n67\nOn Google Colab\nCIFAR10 ì‚¬ì§„ë°ì´í„°ì§‘í•©(í›ˆë ¨ìš©, ì¶”ë¡ ìš©) ë¶€ë¥´ê³ , ì •ê·œí™”ì „ì²˜ë¦¬ìˆ˜í–‰\nCNN ì‹ ê²½ë§ì •ì˜\nCNN ì†ì‹¤í•¨ìˆ˜(í›ˆë ¨íŒë‹¨ê·¼ê±°) ì •ì˜\ní›ˆë ¨ìš©ë°ì´í„°ì§‘í•©ìœ¼ë¡œì‹ ê²½ë§í›ˆë ¨\nì¶”ë¡ ìš©ë°ì´í„°ì§‘í•©ìœ¼ë¡œì‹ ê²½ë§ì‹œí—˜\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 1. loading and normalizing CIFAR10\n68\nOn Google Colab\nê´€ë ¨íŒ¨í‚¤ì§€ì„ ì–¸\në°ì´í„°ë¶€ë¥´ê³ ,\në°ì´í„°[-1,1] ì •ê·œí™”\nê²°ê³¼í™•ì¸\njaekoo@kookmin.ac.kr\n\n69\nOn Google Colab\në°ì´í„°ì§‘í•©ì˜ì„ì˜ì‚¬ì§„ë“¤ê³¼ì •ë‹µì¶œë ¥\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 2. define a CNN\n70\nOn Google Colab\nì‹ ê²½ë§êµ¬ì¡°ì •ì˜\nì‹ ê²½ë§ì˜ì „ë°©ì—°ì‚°ì •ì˜\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 3. define a loss function and optimizer\n71\nOn Google Colab\nì†ì‹¤í•¨ìˆ˜ì •ì˜\nìµœì í™”ì •ì˜\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 4. train the network\n72\nOn Google Colab\nì‹ ê²½ë§í›ˆë ¨ê³¼ì†ì‹¤í•¨ìˆ˜ê²°ê³¼ì¶œë ¥\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 5. test the network",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 19,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 906
      },
      "faiss_idx": 19
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_20": {
      "text": "ain the network\n72\nOn Google Colab\nì‹ ê²½ë§í›ˆë ¨ê³¼ì†ì‹¤í•¨ìˆ˜ê²°ê³¼ì¶œë ¥\njaekoo@kookmin.ac.kr\n\nï‚„step â€“ 5. test the network\n73\nOn Google Colab\nì¶”ë¡ ë°ì´í„°ì˜ì„ì˜ì‚¬ì§„ë“¤í™•ì¸\njaekoo@kookmin.ac.kr\n\n74\nOn Google Colab\ní›ˆë ¨ëœì‹ ê²½ë§ì—ë™ì¼í•œì‚¬ì§„ë“¤ì¶”ë¡ ìˆ˜í–‰\nì¶”ë¡ ê²°ê³¼ì¶œë ¥\njaekoo@kookmin.ac.kr\n\n75\nOn Google Colab\nì „ì²´ì¶”ë¡ ì‚¬ì§„ì§‘í•©ì—ì‹œí—˜í•˜ê³ , ì •í™•ë„ì¶œë ¥\njaekoo@kookmin.ac.kr\n\n76\nOn Google Colab\nê°ì¢…ë¥˜ë³„ì •í™•ë„ì¶œë ¥\njaekoo@kookmin.ac.kr\n\nâš«With programming materials\n77\nHands-on\njaekoo@kookmin.ac.kr\n\nâš«Convolutional neural networks\nâš«Object detection\nâš«Hands-on\n78\nDeep learning for computer vision\njaekoo@kookmin.ac.kr\n\nâš«Visual recognition problems\nï‚„image classification\nï‚„object detection\nï‚„shape estimation\nï‚„action/event recognition\nï‚„emotion classification\nï‚„image captioning\nï‚„video summarization\n79\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Human vs. computer in visual recognition\nï‚„human: represents increasingly more complex visual features along the ventral steam\nï‚„computer: the very same phenomenon emerges in deep neural networks",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 20,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 939
      },
      "faiss_idx": 20
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_21": {
      "text": "atures along the ventral steam\nï‚„computer: the very same phenomenon emerges in deep neural networks \n80\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Convolutional neural networks (CNNs): \nï‚„layers have width, height, and depth \nï‚„started from statistical assumptions (locality, stationary)\nâ†’fundamental components of CNNs\nâ–ªconvolutional layer (CONV)\nâ–ªpooling layer (POOL)\nâ–ªfully connected layer (FC)\nï‚„example\n81\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Convolutional (partial connected) layer \nï‚„fully connected layer vs. convolutional layer \nâ–ªmaintain a shape of input / output of each layer\nâ–ªsummarize and enhance the features of input\nâ–ªfewer # of parameters ïƒŸweight (parameter) sharing\nï‚„if input is 256*256,\nâ–ªfully connected layer:\nâ€“\nunknown 256*256*10=655360 weights\nâ–ªconvolutional layer:\nâ€“\nconvolving a 5*5 filter\nâ€“\nunknown 5*5=25 weights (parameter sharing)\n82\nConvolutional neural networks",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 21,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 912
      },
      "faiss_idx": 21
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_22": {
      "text": "nvolving a 5*5 filter\nâ€“\nunknown 5*5=25 weights (parameter sharing)\n82\nConvolutional neural networks\nreceptive\nfield\nparallel\ndistributed structure\njaekoo@kookmin.ac.kr\n\nâš«Convolutional layer = convolutional filter + activation function\nï‚„convolutional filter (kernel) == self-designed filter\nâ–ª(spatial) filter and its feature map\nâ€“ a toy example of convolutional filer in 2D and 3D\n83\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâ–ªfilterâ€™s hyperparameters\nâ€“ width*height*depth (e.g., 5*5*3, 3*3*3)\nâ€“ stride\nâ€“ padding (e.g., zero padding)\n84\nConvolutional neural networks\nfilters always extend the full depth of the input volume\njaekoo@kookmin.ac.kr\n\nï‚„For your information, parameters != hyperparameters\n85\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nï‚„result\nâ–ªconvolutional filter\nâ–ªactivation function\n86\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Pooling layer\nï‚„makes the representations smaller and more manageable â†’down-sampling",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 22,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 953
      },
      "faiss_idx": 22
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_23": {
      "text": "kookmin.ac.kr\n\nâš«Pooling layer\nï‚„makes the representations smaller and more manageable â†’down-sampling\nï‚„operates over each activation map independently\nï‚„examples\nâ–ªaverage (sum) pooling\nâ–ªmax pooling\n87\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Building block\n88\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«Full architectural description\nï‚„stacking CONV, POOL, FC layers\nï‚„Trend:\nâ–ªsmaller filters and deeper architectures\nâ–ªgetting rid of POOL/FC layers (just COVN layers)\nï‚„Typical architectures\nâ–ª[(CONV-RELU)*N - POOL]*M - (FC-RELU)*K, SOFTMAX\nwhere N is usually up to 5, M is large, K < 3\nï‚„Examples\n89\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«CNN visualization: https://poloclub.github.io/cnn-explainer/\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«A bit of history\nï‚„winners in ILSVRC (ImageNet large scale visual recognition competition)\nâ–ªLeNet (1998)\nâ–ªAlexNet (2012)",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 23,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 902
      },
      "faiss_idx": 23
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_24": {
      "text": "nners in ILSVRC (ImageNet large scale visual recognition competition)\nâ–ªLeNet (1998)\nâ–ªAlexNet (2012)\nâ–ªVGGNet (2014)\nâ–ªGoogLeNet (2014)\nâ–ªResNet (2015)\nâ–ªâ€¦\n91\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«LeNet-5\nï‚„initial convolutional neural networks\nï‚„5*5 filter with stride 1\nï‚„2*2 pooling with stride 2\n92\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«AlexNet\nï‚„[(CONV-RELU) - POOL]*5 â€“ FC*3 ïƒŸtrained by ImageNet\nâ–ª# of parameters in [(CONV-RELU) - POOL]*5 = 2M << # of parameters in FC*3 = 65M\nï‚„first use of ReLU\nâ–ªsolving the problem of vanishing gradient\nï‚„apply methods of regularization\nâ–ªdata augmentation\n93\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâ–ªdropout\nï‚„GPU 1 + GPU 2\nï‚„ensemble method on test dataset\n94\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«VGGNet\nï‚„small filters\nâ–ªonly 3*3 filter with stride 1\nâ–ª2*2 max pooling with stride 2\nâ–ªexample: stack of two 3*3 filters has same effective receptive field as one 5*5 filter",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 24,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 957
      },
      "faiss_idx": 24
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_25": {
      "text": "th stride 2\nâ–ªexample: stack of two 3*3 filters has same effective receptive field as one 5*5 filter\nï‚„deeper networks\nâ–ª8 layers (AlexNet) â†’16 / 19 layers (VGG)\n95\nConvolutional neural networks\n#of weight\none 5*5 filter, 25\nstack of two 3*3 filters, 9+9 =18\njaekoo@kookmin.ac.kr\n\nâš«GoogLeNet\nï‚„deeper networks with computational efficiency\nâ–ª22 layers\nâ–ªefficient inception module (inspired by network in network)\nâ€“ parallel filter operations (1*1, 3*3, 5*5)\nâ€“ 1*1 convolutions â†’reduce depth â†’dimension reduction\nâ–ªreduce FC layers â†’only 5M parameters (12* less than AlexNet)\nâ–ªauxiliary outputs to inject additional gradient at lower layer\n96\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«ResNet\nï‚„very deep networks\nâ–ªdeeper networks are hard to optimize\nâ†’residual connections (skip connections)\nâ–ªusing global average pooling\nâ–ªbeyond ResNet: DenseNet\nâ–ªbatch normalization (without dropout)\n97\nConvolutional neural networks",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 25,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 921
      },
      "faiss_idx": 25
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_26": {
      "text": "ng\nâ–ªbeyond ResNet: DenseNet\nâ–ªbatch normalization (without dropout)\n97\nConvolutional neural networks\njaekoo@kookmin.ac.kr\n\nâš«ëŒ€í‘œì ì¸ì»´í“¨í„°ë¹„ì „ë¬¸ì œ\nï‚„ì‚¬ì§„ë¶„ë¥˜ì˜ˆì‹œ\nObject detection\nì‚¬ì§„ë‹¨ìœ„ì¸ì‹\ní”½ì…€ë‹¨ìœ„ì¸ì‹\në‹¤ìˆ˜ì˜ê°ì²´ì¸ì‹\njaekoo@kookmin.ac.kr\n\nâš«ììœ¨ì£¼í–‰ì—ì„œì˜ê°ì²´ì¸ì‹ê¸°ìˆ ì‚¬ë¡€\nObject detection\njaekoo@kookmin.ac.kr\n\nâš«ê°ì²´ì¸ì‹í˜¹ì€ê°ì²´ê²€ì¶œ\nï‚„ë¶„ë¥˜ì™€ìœ„ì¹˜ë¥¼ë™ì‹œê³ ë ¤\nï‚„ë‹¨ì¼ê°ì²´ì˜ê²½ìš°\nâ–ªë¶„ë¥˜â€“ ê¸°ì¡´ì‚¬ì§„ë¶„ë¥˜ì™€ë™ì¼í•˜ê²Œí•´ê²°\nâ–ªìœ„ì¹˜â€“ ìœ„ì¹˜ì¢Œí‘œë¥¼íšŒê·€ë¬¸ì œë¡œì·¨ê¸‰í•˜ì—¬í•´ê²°\nObject detection\njaekoo@kookmin.ac.kr\n\nâ†’ê°ì²´ì¸ì‹= ë¶„ë¥˜ì™€ìœ„ì¹˜ì¶”ì •ì„ë™ì‹œì—ê³ ë ¤í•˜ì—¬ì†ì‹¤í•¨ìˆ˜ì„¤ì •\nïƒŸCNNsì„í†µí•´ë¶„ë¥˜ì™€ìœ„ì¹˜ì¶”ì •ì—ê´€ë ¨ëœíŠ¹ì§•ì¶”ì¶œí•¨\nObject detection\njaekoo@kookmin.ac.kr\n\nï‚„ë‹¤ìˆ˜ì˜ê°ì²´ì¸ê²½ìš°\nâ–ªì‚¬ì§„ë§ˆë‹¤ê°ì²´ìˆ˜ì—ë¹„ë¡€í•˜ì—¬ì¶œë ¥ì˜ìˆ˜ê°€ë‹¤ë¦„\nObject detection\njaekoo@kookmin.ac.kr\n\nâš«ë‹¤ìˆ˜ì˜ê°ì²´ì¸ì‹ì˜ì–´ë ¤ì›€\nï‚„ë‹¨ìˆœí•˜ê²Œì£¼ì–´ì§„ì‚¬ì§„ì„ì—¬ëŸ¬ê°œì¡°ê°ìœ¼ë¡œë§Œë“¤ê³ ,\nCNNì—ì…ë ¥í•˜ì—¬ë‹¤ìˆ˜ì˜ê°ì²´ì™€ë°°ê²½ì„ë¶„ë¥˜í•œë‹¤ë©´â€¦\nObject detection\nê²°ê³¼ì ìœ¼ë¡œ\në„ˆë¬´ë§ê³ ë‹¤ì–‘í•œìœ„ì¹˜ì™€í¬ê¸°(ë¹„ìœ¨)ë¥¼ê³ ë ¤í•˜ì—¬\nCNN ì…ë ¥ì„ì¤˜ì•¼í•˜ëŠ”ì—°ì‚°ë³µì¡ë„ë¬¸ì œê°€ì¡´ì¬\nìƒˆë¡œìš´ì ‘ê·¼ì´í•„ìš”í•¨!\njaekoo@kookmin.ac.kr\n\nâš«ë¶€ë¶„ì œì•ˆregion proposalsì¸ì„ íƒì ì¸íƒìƒ‰selective search\nï‚„ê°ì²´ê²€ì¶œì„ìœ„í•œê°ì²´ì˜ì˜ì—­í›„ë³´ë¥¼êµ¬í•˜ëŠ”ë°©ë²•ì¤‘í•˜ë‚˜\nâ–ªì‹¤ì œ, R-CNN, Fast R-CNN, SPPNets ë“±ê°ì²´ì¸ì‹í•™ìŠµëª¨ë¸ì—ì„œí™œìš©ë¨\nï‚„ìƒëŒ€ì ìœ¼ë¡œë¹ ë¥´ê²Œìˆ˜í–‰ë¨\nâ–ªCPUë¡œ2000ê°œë¶€ë¶„ì„ì œì•ˆí•˜ëŠ”ë°ìˆ˜ì´ˆì •ë„ë§Œê±¸ë¦¼\nObject detection - Deep learning for object detection",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 26,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 905
      },
      "faiss_idx": 26
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_27": {
      "text": "ë¸ì—ì„œí™œìš©ë¨\nï‚„ìƒëŒ€ì ìœ¼ë¡œë¹ ë¥´ê²Œìˆ˜í–‰ë¨\nâ–ªCPUë¡œ2000ê°œë¶€ë¶„ì„ì œì•ˆí•˜ëŠ”ë°ìˆ˜ì´ˆì •ë„ë§Œê±¸ë¦¼\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«R-CNN\nï‚„ë¬¸ì œ: ë§¤ìš°ëŠë¦¼(ì‚¬ì§„ë§ˆë‹¤2000ê°œì œì•ˆì˜ì—­ì˜ë…ë¦½ì ì¸CNN ì „ë°©ì „íŒŒì—°ì‚°ë“¤ìˆ˜í–‰í•„ìš”)\nïƒŸCNNì˜ì „ë°©ì—°ì‚°ê³¼ì˜ì—­ì„ ì •êµí™˜\nObject detection - Deep learning for object detection\nê´€ì‹¬ì˜ì—­ì„ ì •\nì˜ì—­í¬ê¸°í†µì¼\nê°ì˜ì—­ì˜\nCNN ì „ë°©ì „íŒŒì—°ì‚°\n(íŠ¹ì§•ì¶”ì¶œ)\nì˜ì—­ì˜\nSVM ë¶„ë¥˜ì™€ìœ„ì¹˜íšŒê·€\njaekoo@kookmin.ac.kr\n\nâš«Fast R-CNN\nObject detection - Deep learning for object detection\nR-CNN: \në§¤ìš°ëŠë¦°ë¬¸ì œ\nâ†’ê³¼ì •ì˜êµ¬ì¡°ì êµí™˜ìœ¼ë¡œí•´ê²°\nImageNet í›ˆë ¨ëœ\nCNN í™œìš©\nì˜ì—­ì œì•ˆë°©ë²•ì—ì˜í•œ\nê´€ì‹¬ì˜ì—­ì„ ì •\n(RoI POOL)\njaekoo@kookmin.ac.kr\n\nâš«RoI ì¶”ì¶œ\nï‚„RoI POOL \nâ–ªì…ë ¥ì˜ê°ì²´ìœ„ì¹˜ì™€ìƒëŒ€ì ìœ¼ë¡œëŒ€ì‘ë˜ëŠ”íŠ¹ì§•ê³µê°„ì˜ì˜ì—­ì„¤ì •\nâ–ªê²©ìí˜•íƒœì˜íŠ¹ì§•ê³µê°„ìœ¼ë¡œì˜ì—­ì¬ì„¤ì •\nâ–ªì¬ì„¤ì •ëœì˜ì—­ì„ë¶€ë¶„í™”í•˜ê³ , í†µê³„ìµœëŒ€ê°’ìœ¼ë¡œì¹˜í™˜(max-pool)\nâ†’ë™ì¼í•œí¬ê¸°ì˜íŠ¹ì§•ì˜ì—­ì¶”ì¶œ\nâ–ªíŠ¹ì§•ì˜ì—­ë“¤ì´ì•½ê°„ì–´ê¸‹ë‚ ìˆ˜ìˆìŒâ†’RoI ì •ë ¬ìˆ˜í–‰\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«RoI ì •ë ¬\nï‚„ì˜ì—­ì˜ìœ„ì¹˜ê°€ìƒëŒ€ì ìœ¼ë¡œì¼ì¹˜í•˜ì§€ì•Šì€ê²½ìš°\nâ–ªê²©ìë‹¨ìœ„ê°ìš”ì†Œê°’ë“¤(íšŒìƒ‰)ì˜ì„ í˜•ë³´ê°„ë²•ì„ì´ìš©í•˜ì—¬ì¤‘ê°„ê°’ì¶”ì •(ë…¹ìƒ‰)\nâ–ªì¤‘ê°„ê°’ë“¤ì„ê¸°ì¤€ìœ¼ë¡œë¶€ë¶„í™”í•˜ê³ í†µê³„ì ìµœëŒ€ê°’ìœ¼ë¡œì¹˜í™˜\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«R-CNNê³¼Fast R-CNN ë¹„êµ\nObject detection - Deep learning for object detection",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 27,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 940
      },
      "faiss_idx": 27
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_28": {
      "text": "on\njaekoo@kookmin.ac.kr\n\nâš«R-CNNê³¼Fast R-CNN ë¹„êµ\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«Faster R-CNN\nï‚„CNNìœ¼ë¡œì˜ì—­ì œì•ˆí•˜ëŠ”êµ¬ì¡°ì¶”ê°€\nâ–ªíŠ¹ì§•ë“¤ì—ì„œì˜ì—­ì„ì œì•ˆí•˜ëŠ”ì˜ì—­ì œì•ˆë§region proposal network (RPN) ì‚½ì…\nâ€“ ì„ íƒì íƒìƒ‰â†’ì˜ì—­ì œì•ˆë§\nï‚„ê·¸ì™¸ì˜ë¶€ë¶„ë“¤ì€Fast R-CNNê³¼ê°™ìŒ\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«ì˜ì—­ì œì•ˆë§region proposal network (RPN)\nï‚„íŠ¹ì§•ë§µì˜ê°ì ë§ˆë‹¤ê³ ì •í¬ê¸°ì˜í•˜ë‚˜ì˜ì•µì»¤ë°•ìŠ¤anchor box í™œìš©\nï‚„ê°ì ë§ˆë‹¤ì•µì»¤ë°•ìŠ¤ì—ê°ì²´ê°€ìˆëŠ”ì§€ë¥¼ì˜ˆì¸¡(í”½ì…€ë‹¨ìœ„ë¡œì§€ìŠ¤í‹±íšŒê·€per-pixel logistic regression)\nï‚„ê°ì²´ê°€ìˆë‹¤ê³ íŒë‹¨ë˜ëŠ”ì•µì»¤ë°•ìŠ¤ì—ëŒ€ì‘ë˜ëŠ”ì ì—ëŒ€í•´ì„œëŠ”\nê°ì²´ì˜ì •í™•í•œí¬ê¸°ì™€ì•µì»¤ë°•ìŠ¤ê°„ì˜ë³€í™˜ì„ì˜ˆì¸¡\nï‚„í•˜ë‚˜ì˜ì•µì»¤ë°•ìŠ¤ë¥¼ë‹¤ìˆ˜ë¡œí™•ì¥ê°€ëŠ¥\nObject detection - Deep learning for object detection\nâ‘ classify object/not-object\nâ‘¡regress box location\njaekoo@kookmin.ac.kr\n\nâš«Faster R-CNNì˜ì†ì‹¤í•¨ìˆ˜\nï‚„4ê°€ì§€ê³ ë ¤\nâ‘ ì˜ì—­ì œì•ˆë§ì˜ê°ì²´ìœ ë¬´ì†ì‹¤\nâ‘¡ì˜ì—­ì œì•ˆë§ì˜ìœ„ì¹˜ë°•ìŠ¤íšŒê·€ì†ì‹¤\nâ‘¢ê°ê°ì²´ë“¤ì˜ìµœì¢…ë¶„ë¥˜ì†ì‹¤\nâ‘£ìµœì¢…ê°ì²´ìœ„ì¹˜ë°•ìŠ¤íšŒê·€ì†ì‹¤\nObject detection - Deep learning for object detection\nâ‘ \nâ‘¡\nâ‘¢\nâ‘£\njaekoo@kookmin.ac.kr\n\nâš«Faster R-CNN ë™ì‘\nï‚„ë‘ë‹¨ê³„ìˆ˜í–‰\nâ–ª1ë‹¨ê³„(íŒŒë€ìƒ‰): ì‚¬ì§„ë‹¨ìœ„ìˆ˜í–‰ë¨\nâ€“ CNN\nâ€“ ì˜ì—­ì œì•ˆë§\nâ–ª2ë‹¨ê³„(ë…¹ìƒ‰): ì˜ì—­ë‹¨ìœ„ìˆ˜í–‰ë¨\nâ€“ RoI ìš”ì•½ë°ì •ë ¬\nâ€“ ê°ì²´ë¶„ë¥˜ì˜ˆì¸¡",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 28,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 901
      },
      "faiss_idx": 28
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_29": {
      "text": "kr\n\nâš«Faster R-CNN ë™ì‘\nï‚„ë‘ë‹¨ê³„ìˆ˜í–‰\nâ–ª1ë‹¨ê³„(íŒŒë€ìƒ‰): ì‚¬ì§„ë‹¨ìœ„ìˆ˜í–‰ë¨\nâ€“ CNN\nâ€“ ì˜ì—­ì œì•ˆë§\nâ–ª2ë‹¨ê³„(ë…¹ìƒ‰): ì˜ì—­ë‹¨ìœ„ìˆ˜í–‰ë¨\nâ€“ RoI ìš”ì•½ë°ì •ë ¬\nâ€“ ê°ì²´ë¶„ë¥˜ì˜ˆì¸¡\nâ€“ ê°ì²´ìœ„ì¹˜ì˜ˆì¸¡\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«R-CNN ê³„ì—´ê°ì²´ì¸ì‹ëª¨ë¸ë¹„êµ\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«Faster R-CNN: ë‘ë‹¨ê³„ê°ì²´ì¸ì‹â†’YOLO/SSD/RetinaNet: ë‹¨ì¼ë‹¨ê³„ê°ì²´ì¸ì‹\nObject detection - Deep learning for object detection\nâœ“ê°ê²©ìë§ˆë‹¤ìœ„ì¹˜ë°•ìŠ¤ì¶”ì •(5ìš”ì†Œ)\nâœ“(ë°°ê²½í¬í•¨ëª¨ë“ ë¶€ë¥˜) ê°ë¶„ë¥˜ì˜ˆì¸¡ê²°ê³¼\nâ†’ì˜ì—­ì œì•ˆë§ê³¼ìœ ì‚¬í•˜ì§€ë§Œë³´ë‹¤ë¶€ë¥˜ì—íŠ¹ì •ë¨\njaekoo@kookmin.ac.kr\n\nâš«ê°ì²´ì¸ì‹ì„ìœ„í•œê¹Šì€ì¸ê³µì‹ ê²½ë§ì •ë¦¬\nï‚„ê·¼ê°„ì´ë˜ëŠ”ì¸ê³µì‹ ê²½ë§\nâ–ªVGG16\nâ–ªResNet/ResNet101\nâ–ªInception/Inception V2/Inception V3\nâ–ªMobileNet\nâ†’ê·¼ê°„ì´ë˜ëŠ”ì¸ê³µì‹ ê²½ë§ì´ê¹Šê³ í´ìˆ˜ë¡ì¢‹ì€ì„±ëŠ¥ì„ê°€ì§\nï‚„ì¸ê³µì‹ ê²½ë§ì˜êµ¬ì¡°ì ë‹¨ê³„ë‹¤ì–‘ì„±\nâ–ªë‘ë‹¨ê³„: Faster R-CNN\nâ–ªë‹¨ì¼ë‹¨ê³„: YOLO/SSD/RetinaNet\nâ–ªí˜¼í•©ë‹¨ê³„: R-FCN\nï‚„ëŒ€í‘œì ì¸ì¸ê³µì‹ ê²½ë§ë¹„êµ\nâ–ªFaster R-CNN: ëŠë¦¬ì§€ë§Œë†’ì€ì •í™•ë„(ì‚¬ì§„)\nâ–ªSSD: ë¹ ë¥´ì§€ë§Œë‚®ì€ì •í™•ë„(ë™ì˜ìƒ)\nObject detection - Deep learning for object detection\njaekoo@kookmin.ac.kr\n\nâš«Detectron2: a PyTorch-based modular object detection library\nï‚„Facebook AI Research (FAIR)â€™s open-source projects",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 29,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 920
      },
      "faiss_idx": 29
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_30": {
      "text": "PyTorch-based modular object detection library\nï‚„Facebook AI Research (FAIR)â€™s open-source projects\nâ–ªwill continue to accelerate progress in the area of object detection\nâ–ªable to rapidly move research ideas into production models\nâ€“ e.g., AI camera system in Facebookâ€™s Portal video-calling devices\nï‚„next-generation platform for object detection and segmentation \nâ–ªDetectron: release in 2018 (https://github.com/facebookresearch/Detectron)\nâ–ªDetectron2: release in 2019, Oct. (https://github.com/facebookresearch/detectron2)\n117\nObject detection - Detectron2\njaekoo@kookmin.ac.kr\n\nâš«Detectron2: a PyTorch-based modular object detection library\nï‚„implementation\nâ–ªimplemented in PyTorch (Detectron: Caffe2)\nâ–ªmodular, extensible design (more flexible)\nâ–ªfast training on single or multiple GPU servers\nâ€“ much simpler to scale training to very large data sets\nâ–ªincluding state-of-the-art object detection algorithms (high-quality reference)",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 30,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 930
      },
      "faiss_idx": 30
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_31": {
      "text": "ry large data sets\nâ–ªincluding state-of-the-art object detection algorithms (high-quality reference)\nâ€“ e.g., DensePose, panoptic feature pyramid networks, and variants of Mask R-CNN model family\n118\nObject detection - Detectron2\njaekoo@kookmin.ac.kr\n\n119\nObject detection - Detectron2\nâš«Example of ResNet+FPN (backbone) with Fast R-CNN (heads)\njaekoo@kookmin.ac.kr\n\nâš«https://google.github.io/mediapipe/\n120\nObject detection - MediaPipe\njaekoo@kookmin.ac.kr\n\nâš«Applications\n121\nObject detection - MediaPipe\njaekoo@kookmin.ac.kr\n\nâš«With programming materials\n122\nHands-on\njaekoo@kookmin.ac.kr\n\nâš«Recurrent neural networks\nâš«Transformer\nâš«Hands-on\n123\nDeep learning for sequence data (time series)\njaekoo@kookmin.ac.kr\n\nâš«Recurrent neural networks (RNNs) process sequences\nï‚„sequences == a variable-length input having temporal features (e.g., context dependence)\nâ–ªe.g., time series data\nâ†’recurrent connections (recurrent edges):",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 31,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 917
      },
      "faiss_idx": 31
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_32": {
      "text": "tures (e.g., context dependence)\nâ–ªe.g., time series data\nâ†’recurrent connections (recurrent edges): \nconnections between neurons that are located in same layer \n124\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nâš«A single neuron, fully connected recurrent layer\nï‚„computational graph (structure) in RNNs\nâ–ªforward network\nâ€“ a toy example\n125\nRecurrent neural networks\nâ„ğ‘¡= ğ‘“ğ‘¤(â„ğ‘¡âˆ’1, xğ‘¡) = tanh(ğ‘Šâ„â„â„ğ‘¡âˆ’1 + ğ‘Šxâ„xğ‘¡)\nğ‘¦ğ‘¡= ğ‘Šâ„ğ‘¦â„ğ‘¡\nunfold\nxğ‘¡\nğ‘¦ğ‘¡\nâ„ğ‘¡\nğ‘Šâ„â„\nğ‘Šxâ„\nğ‘Šâ„ğ‘¦\njaekoo@kookmin.ac.kr\n\nâš«Variation of RNNs\n126\nRecurrent neural networks\ne.g., speech to text\ne.g., translation\ne.g., image captioning\ne.g., text classification\njaekoo@kookmin.ac.kr\n\nâš«Details of feedforward\nï‚„sequential input\nï‚„tanh function\nï‚„propagating and updating hidden state \n127\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nâš«A single neuron, fully connected recurrent layer\nï‚„computational graph in RNNs\nâ–ªbackward network\nâ€“ backpropagation through time (BPTT): computing gradient through RNN",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 32,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 941
      },
      "faiss_idx": 32
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_33": {
      "text": "aph in RNNs\nâ–ªbackward network\nâ€“ backpropagation through time (BPTT): computing gradient through RNN\nâ€“ vanishing gradients: sensitivity decay exponentially over time\n128\nRecurrent neural networks\ntruncated BPTT\njaekoo@kookmin.ac.kr\n\nâš«RNNs\n1.\nprocesses information from input by incorporating it into hidden state that is \npassed forward through time\n2. forward through entire sequence to compute loss then backward through entire \nsequence to compute gradient\n129\nRecurrent neural networks\npredicted value: black\nerror: yellow\ngradient: orange\njaekoo@kookmin.ac.kr\n\nâš«Long short-term memory (LSTM)\nï‚„allow the networks to accumulate information over a long duration\nâ–ªf: forget (keep) gate\nâ–ªi: input (write) gate\nâ–ªo: output gate\nâ–ªà·¤c: memory cell\n130\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nï‚„in LSTM, f: forget (keep) gate\n131\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nï‚„in LSTM, i: input (write) gate",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 33,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 909
      },
      "faiss_idx": 33
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_34": {
      "text": "get (keep) gate\n131\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nï‚„in LSTM, i: input (write) gate\nRecurrent neural networks\n132\njaekoo@kookmin.ac.kr\n\nï‚„in LSTM, à·¤c: memory cell\n133\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nï‚„in LSTM, o: output gate\nRecurrent neural networks\n134\njaekoo@kookmin.ac.kr\n\nâš«Flow of gradient in LSTM\n135\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nâš«Variation of LSTMs\n136\nRecurrent neural networks\nstacked LSTM\ndensely connected LSTM\njaekoo@kookmin.ac.kr\n\nâš«in LSTM, example of image captioning\n137\nRecurrent neural networks\njaekoo@kookmin.ac.kr\n\nâš«LSTM vs. RNN\nâš«Comparison between CNNs and RNNs\n138\nRecurrent neural networks\nCNNs\nRNNs\nwhat for\ngrid of values\nsequence of values\nextracting \nspatial features\ntemporal features\nsharing parameters\nsame filter across local regions\nsame weights across time steps\njaekoo@kookmin.ac.kr\n\nTransformer\njaekoo@kookmin.ac.kr\n\nTransformer",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 34,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 911
      },
      "faiss_idx": 34
    },
    "af887d22-6b67-4286-a4c1-93b73e756036_35": {
      "text": "same weights across time steps\njaekoo@kookmin.ac.kr\n\nTransformer\njaekoo@kookmin.ac.kr\n\nTransformer\njaekoo@kookmin.ac.kr\n\nâš«With programming materials\n141\nHands-on\njaekoo@kookmin.ac.kr\n\nâš«Deep (representation) learning have achieved remarkable improvement\nï‚„e.g., computer vision, language understanding, â€¦\nâš«Deep change is started by deep learning!\nï‚„deep neural network == (linear + nonlinear)*M == universal approximator\nï‚„change of paradigm â†’the deep learning revolution\nâš«Deep learning has power and limits\nï‚„rapidly changing, must stay in tune!!\nâš«Deep learning \nenablerOther fields\nConclusion\n142\njaekoo@kookmin.ac.kr\n\nâš«If you have any comments, \nsuggestions or questions then please do let me know!\nâš«For more information, contact me \njaekoo@kookmin.ac.kr\nData Science Laboratory (http://data.snu.ac.kr)\nElectrical and Computer Engineering\nSeoul National University\nQuestion and Answer\n143\nThank you â˜º\njaekoo@kookmin.ac.kr",
      "metadata": {
        "document_id": "af887d22-6b67-4286-a4c1-93b73e756036",
        "source": "iÌ€__iÌ__iÌ_Â¸iÌ€_Â¨iÌ€_Â´iÌ€_ÂµiÌ_Â©iÌ€Âµ_iÌ€_Â eÌ‚Â¸Â°iÌ€_Â  eÌ‚Â°_iÌ€__iÌ€__eÌˆÂ£_.pdf",
        "chunk_index": 35,
        "total_chunks": 37,
        "upload_timestamp": "2025-06-09T17:19:56.816019",
        "text_length": 919
      },
      "faiss_idx": 35
    }
  },
  "id_map": {
    "0": "af887d22-6b67-4286-a4c1-93b73e756036_0",
    "1": "af887d22-6b67-4286-a4c1-93b73e756036_1",
    "2": "af887d22-6b67-4286-a4c1-93b73e756036_2",
    "3": "af887d22-6b67-4286-a4c1-93b73e756036_3",
    "4": "af887d22-6b67-4286-a4c1-93b73e756036_4",
    "5": "af887d22-6b67-4286-a4c1-93b73e756036_5",
    "6": "af887d22-6b67-4286-a4c1-93b73e756036_6",
    "7": "af887d22-6b67-4286-a4c1-93b73e756036_7",
    "8": "af887d22-6b67-4286-a4c1-93b73e756036_8",
    "9": "af887d22-6b67-4286-a4c1-93b73e756036_9",
    "10": "af887d22-6b67-4286-a4c1-93b73e756036_10",
    "11": "af887d22-6b67-4286-a4c1-93b73e756036_11",
    "12": "af887d22-6b67-4286-a4c1-93b73e756036_12",
    "13": "af887d22-6b67-4286-a4c1-93b73e756036_13",
    "14": "af887d22-6b67-4286-a4c1-93b73e756036_14",
    "15": "af887d22-6b67-4286-a4c1-93b73e756036_15",
    "16": "af887d22-6b67-4286-a4c1-93b73e756036_16",
    "17": "af887d22-6b67-4286-a4c1-93b73e756036_17",
    "18": "af887d22-6b67-4286-a4c1-93b73e756036_18",
    "19": "af887d22-6b67-4286-a4c1-93b73e756036_19",
    "20": "af887d22-6b67-4286-a4c1-93b73e756036_20",
    "21": "af887d22-6b67-4286-a4c1-93b73e756036_21",
    "22": "af887d22-6b67-4286-a4c1-93b73e756036_22",
    "23": "af887d22-6b67-4286-a4c1-93b73e756036_23",
    "24": "af887d22-6b67-4286-a4c1-93b73e756036_24",
    "25": "af887d22-6b67-4286-a4c1-93b73e756036_25",
    "26": "af887d22-6b67-4286-a4c1-93b73e756036_26",
    "27": "af887d22-6b67-4286-a4c1-93b73e756036_27",
    "28": "af887d22-6b67-4286-a4c1-93b73e756036_28",
    "29": "af887d22-6b67-4286-a4c1-93b73e756036_29",
    "30": "af887d22-6b67-4286-a4c1-93b73e756036_30",
    "31": "af887d22-6b67-4286-a4c1-93b73e756036_31",
    "32": "af887d22-6b67-4286-a4c1-93b73e756036_32",
    "33": "af887d22-6b67-4286-a4c1-93b73e756036_33",
    "34": "af887d22-6b67-4286-a4c1-93b73e756036_34",
    "35": "af887d22-6b67-4286-a4c1-93b73e756036_35"
  },
  "next_id": 36
}