"""
ğŸ¤– Quiz Generation AI Agent using LangGraph
"""
import logging
import os
from typing import Dict, List, Any, Optional, TypedDict
from dataclasses import dataclass
from enum import Enum

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain.prompts import ChatPromptTemplate

# ğŸ”¥ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì ì„í¬íŠ¸
from .prompt import QuizPromptManager
from .prompt.quiz_prompt_manager import DifficultyLevel, QuestionType

logger = logging.getLogger(__name__)


@dataclass
class QuizRequest:
    """ë¬¸ì œ ìƒì„± ìš”ì²­"""
    file_ids: List[str]                 # ëŒ€ìƒ íŒŒì¼ IDë“¤
    num_questions: int = 5              # ìƒì„±í•  ë¬¸ì œ ìˆ˜
    difficulty: DifficultyLevel = DifficultyLevel.MEDIUM
    question_type: QuestionType = QuestionType.MULTIPLE_CHOICE
    custom_topic: Optional[str] = None  # íŠ¹ì • ì£¼ì œ ì§€ì •
    additional_instructions: Optional[List[str]] = None  # ì¶”ê°€ ì§€ì‹œì‚¬í•­


class QuizState(TypedDict):
    """LangGraph ìƒíƒœ ê´€ë¦¬"""
    # ì…ë ¥
    request: QuizRequest
    documents: List[Dict[str, Any]]

    # ì›Œí¬í”Œë¡œìš° ìƒíƒœ
    summary: str
    core_topics: List[str]
    keywords: List[str]
    generated_questions: List[Dict[str, Any]]

    # ë©”íƒ€ë°ì´í„°
    current_step: str
    errors: List[str]
    domain_context: Dict[str, Any]


class QuizGeneratorAgent:
    """ë¬¸ì œ ìƒì„± AI ì—ì´ì „íŠ¸"""

    def __init__(self, openai_api_key: Optional[str] = None):
        """
        ì´ˆê¸°í™”
        Args:
            openai_api_key: OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ ê°€ëŠ¥)
        """
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì§ì ‘ ì „ë‹¬í•˜ì„¸ìš”.")

        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸
            temperature=0.7,      # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
            api_key=self.openai_api_key
        )

        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._create_workflow()

        # ğŸ¯ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”
        self.prompt_manager = QuizPromptManager()

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self.summary_template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì»¨í…ì¸  ë¶„ì„ê°€ì…ë‹ˆë‹¤."),
            ("human", "{prompt}")
        ])

        self.topic_template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ê³¼ì • ì„¤ê³„ìì…ë‹ˆë‹¤."),
            ("human", "{prompt}")
        ])

        self.keyword_template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ ì‹œí—˜ ì¶œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            ("human", "{prompt}")
        ])

        self.question_template = ChatPromptTemplate.from_messages([
            ("system", "{system_message}"),
            ("human", "{prompt}")
        ])

    def _create_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        workflow = StateGraph(QuizState)

        # ë³‘ë ¬ ì²˜ë¦¬ ë…¸ë“œ ì¶”ê°€
        workflow.add_node("parallel_processor", self._parallel_process)
        workflow.add_node("question_generator", self._generate_questions)
        workflow.add_node("quality_validator", self._validate_questions)

        # ì›Œí¬í”Œë¡œìš° ìˆœì„œ ì •ì˜
        workflow.set_entry_point("parallel_processor")
        workflow.add_edge("parallel_processor", "question_generator")
        workflow.add_edge("question_generator", "quality_validator")
        workflow.add_edge("quality_validator", END)

        return workflow.compile()

    async def _parallel_process(self, state: QuizState) -> QuizState:
        """ğŸ“„ ë³‘ë ¬ ì²˜ë¦¬: ë¬¸ì„œ ìš”ì•½, í•µì‹¬ ì£¼ì œ ì¶”ì¶œ, í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            logger.info("ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘")

            # ë¬¸ì„œ ë‚´ìš© ê²°í•©
            combined_content = ""
            domain_info = {}
            for doc in state["documents"]:
                filename = doc.get("filename", "Unknown")
                content = doc.get("content", "")
                combined_content += f"\n\n=== {filename} ===\n{content[:2000]}"
                domain_info[filename] = {
                    "language": doc.get("language", "unknown"),
                    "file_size": doc.get("file_size", 0),
                    "chunk_count": doc.get("total_chunks", 0)
                }

            # ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ì •ì˜
            async def summarize_documents():
                summary_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì»¨í…ì¸  ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸ“‹ **ë¶„ì„ ëŒ€ìƒ ë¬¸ì„œë“¤:**
{combined_content}

ğŸ¯ **ìš”ì•½ ì§€ì¹¨:**
1. ê° ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•˜ê³  ì£¼ìš” ê°œë…ì„ ì¶”ì¶œí•˜ì„¸ìš”
2. ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ ë¬¸ì„œë¼ë©´ ê°ê°ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì„¸ìš”
3. êµìœ¡/í•™ìŠµ ëª©ì ì— ì í•©í•œ í•µì‹¬ ì§€ì‹ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”
4. ë¬¸ì œ ì¶œì œê°€ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤, ê°œë…, ì ˆì°¨ë¥¼ í¬í•¨í•˜ì„¸ìš”

**ìš”ì•½ ê¸¸ì´:** 500-800ì
**ì¶œë ¥ í˜•ì‹:** ê° ë¬¸ì„œë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ìš”ì•½í•œ í›„ ì „ì²´ ì¢…í•© ìš”ì•½
"""
                chain = self.summary_template | self.llm
                response = await chain.ainvoke({"prompt": summary_prompt})
                return response.content

            async def extract_topics():
                topic_prompt = f"""
ë¬¸ì„œ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ì£¼ì œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ğŸ“‹ **ë¬¸ì„œ ìš”ì•½:**
{combined_content}

ğŸ¯ **ì¶”ì¶œ ì¡°ê±´:**
- ë‚œì´ë„: {state["request"].difficulty.value}
- ëª©í‘œ ë¬¸ì œ ìˆ˜: {state["request"].num_questions}ê°œ
- ë¬¸ì œ ìœ í˜•: {state["request"].question_type.value}

**ì£¼ì œ ì¶”ì¶œ ì§€ì¹¨:**
1. êµìœ¡ì  ê°€ì¹˜ê°€ ë†’ì€ í•µì‹¬ ê°œë…ë“¤ì„ ì„ ë³„í•˜ì„¸ìš”
2. ì„ íƒëœ ë‚œì´ë„ì— ì í•©í•œ ì£¼ì œë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ í•˜ì„¸ìš”
3. ê° ë„ë©”ì¸ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•˜ì„¸ìš”
4. ë¬¸ì œ ì¶œì œê°€ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ì£¼ì œë¥¼ í¬í•¨í•˜ì„¸ìš”

**ì¶œë ¥ í˜•ì‹:**
- ì£¼ì œ1: [ì£¼ì œëª…] - [ê°„ë‹¨í•œ ì„¤ëª…]
- ì£¼ì œ2: [ì£¼ì œëª…] - [ê°„ë‹¨í•œ ì„¤ëª…]
...

**ì£¼ì œ ê°œìˆ˜:** 5-8ê°œ (ë¬¸ì œ ìˆ˜ë³´ë‹¤ ë§ê²Œ)
"""
                chain = self.topic_template | self.llm
                response = await chain.ainvoke({"prompt": topic_prompt})
                topics_text = response.content
                topics = []
                for line in topics_text.split('\n'):
                    if line.strip().startswith('-') or line.strip().startswith('â€¢'):
                        topic = line.strip().lstrip('- â€¢').strip()
                        if topic:
                            topics.append(topic)
                return topics

            async def extract_keywords():
                keyword_prompt = f"""
ì¶”ì¶œëœ í•µì‹¬ ì£¼ì œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œ ì¶œì œìš© í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ğŸ“‹ **í•µì‹¬ ì£¼ì œë“¤:**
{combined_content}

ğŸ¯ **í‚¤ì›Œë“œ ì¶”ì¶œ ì¡°ê±´:**
- ë‚œì´ë„: {state["request"].difficulty.value}
- ë¬¸ì œ ìœ í˜•: {state["request"].question_type.value}

**í‚¤ì›Œë“œ ì¶”ì¶œ ì§€ì¹¨:**
1. ê° ì£¼ì œë³„ë¡œ í•µì‹¬ í‚¤ì›Œë“œ 2-3ê°œì”© ì¶”ì¶œ
2. ë‚œì´ë„ë³„ íŠ¹ì„±:
   - EASY: ê¸°ë³¸ ìš©ì–´, ì •ì˜, ë‹¨ìˆœ ì‚¬ì‹¤
   - MEDIUM: ê°œë… ê´€ê³„, ì›ë¦¬, ì ˆì°¨
   - HARD: ì‘ìš© ìƒí™©, ë³µí•© ê°œë…, ë¶„ì„ ìš”ì†Œ
3. ë¬¸ì œ ì¶œì œê°€ ì§ì ‘ì ìœ¼ë¡œ ê°€ëŠ¥í•œ êµ¬ì²´ì  í‚¤ì›Œë“œ
4. ë„ë©”ì¸ë³„ ì „ë¬¸ ìš©ì–´ì™€ ì¼ë°˜ ê°œë…ì˜ ê· í˜•

**ì¶œë ¥ í˜•ì‹:**
í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, ...

**í‚¤ì›Œë“œ ê°œìˆ˜:** 15-25ê°œ
"""
                chain = self.keyword_template | self.llm
                response = await chain.ainvoke({"prompt": keyword_prompt})
                keywords_text = response.content
                keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
                return keywords

            # ë³‘ë ¬ ì‹¤í–‰
            import asyncio
            summary, topics, keywords = await asyncio.gather(
                summarize_documents(),
                extract_topics(),
                extract_keywords()
            )

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["summary"] = summary
            state["core_topics"] = topics
            state["keywords"] = keywords
            state["domain_context"] = domain_info
            state["current_step"] = "parallel_processor"

            logger.info("SUCCESS ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")
            return state

        except Exception as e:
            logger.error(f"ERROR ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            state["errors"].append(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return state

    async def _generate_questions(self, state: QuizState) -> QuizState:
        """â“ 4ë‹¨ê³„: ê· í˜• ì¡íŒ ë¬¸ì œ ìƒì„± (90% ì¼ë°˜ + 10% ì‘ìš©)"""
        try:
            logger.info("STEP4 ê· í˜• ì¡íŒ ë¬¸ì œ ìƒì„± ì‹œì‘")

            request = state["request"]
            summary = state["summary"]
            topics = state["core_topics"]
            keywords = state["keywords"]

            # ì¶”ê°€ ì§€ì‹œì‚¬í•­ì´ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
            additional_guide = ""
            if request.additional_instructions:
                additional_guide = "\n\nğŸ“ **ì¶”ê°€ ì§€ì‹œì‚¬í•­**:\n" + "\n".join(f"- {instruction}" for instruction in request.additional_instructions)

            # ğŸ¯ 1ë‹¨ê³„: PDF ê¸°ë°˜ ë¬¸ì œ ìƒì„±
            pdf_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì»¨í…ì¸  ê°œë°œìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ğŸ“š **ì»¨í…ì¸  ìš”ì•½**:
{summary}

ğŸ¯ **í•µì‹¬ ì£¼ì œë“¤**:
{chr(10).join(f"- {topic}" for topic in topics)}

ğŸ”‘ **í•µì‹¬ í‚¤ì›Œë“œë“¤**:
{chr(10).join(f"- {keyword}" for keyword in keywords)}

ğŸ“ **ë¬¸ì œ ìƒì„± ì¡°ê±´**:
- ìƒì„±í•  ë¬¸ì œ ìˆ˜: {request.num_questions}ê°œ
- ë‚œì´ë„: {request.difficulty.value}
- ë¬¸ì œ ìœ í˜•: {request.question_type.value}

ğŸ¯ **ë¬¸ì œ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­**:
1. ê° ë¬¸ì œëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì‹¤ì œ ì‚¬ë¡€ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤
2. ì¤‘ë³µë˜ëŠ” ê°œë…ì˜ ë¬¸ì œëŠ” í”¼í•˜ê³ , ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤
3. ë¬¸ì œëŠ” ì´ë¡ ì  ê°œë…ê³¼ ì‹¤ì œ êµ¬í˜„ì„ ê· í˜•ìˆê²Œ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
4. ê° ë¬¸ì œëŠ” ëª…í™•í•œ í•™ìŠµ ëª©í‘œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
5. ë¬¸ì œì˜ ë‚œì´ë„ëŠ” ì§€ì •ëœ ìˆ˜ì¤€ì— ë§ê²Œ ì¡°ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
6. ì„ íƒì§€ëŠ” ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
7. ì •ë‹µ í•´ì„¤ì€ ìƒì„¸í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ê°€ì¹˜ìˆì–´ì•¼ í•©ë‹ˆë‹¤

{additional_guide}

**ì¶œë ¥ í˜•ì‹**:
```json
{{
  "questions": [
    {{
      "id": 1,
      "question": "ë¬¸ì œ ë‚´ìš©",
      "type": "{request.question_type.value}",
      "difficulty": "{request.difficulty.value}",
      "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
      "correct_answer": "ì •ë‹µ",
      "explanation": "ì •ë‹µ í•´ì„¤",
      "learning_objective": "í•™ìŠµ ëª©í‘œ",
      "problem_level": "basic ë˜ëŠ” application",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
      "source": "pdf_based",
      "example": "ê´€ë ¨ ì˜ˆì‹œë‚˜ ì‹¤ì œ ì‚¬ë¡€",
      "implementation": "ì‹¤ì œ êµ¬í˜„ ë°©ë²• (í•´ë‹¹ë˜ëŠ” ê²½ìš°)"
    }}
  ]
}}
```

ì •í™•íˆ {request.num_questions}ê°œì˜ ê³ í’ˆì§ˆ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
"""

            messages = [
                SystemMessage(content=self.prompt_manager.get_system_message()),
                HumanMessage(content=pdf_prompt)
            ]

            response = await self.llm.ainvoke(messages)
            pdf_questions = self._parse_questions(response.content)

            # ğŸ“Š PDF ê¸°ë°˜ ë¬¸ì œ ìˆ˜ í™•ì¸
            if len(pdf_questions) >= request.num_questions:
                state["generated_questions"] = pdf_questions[:request.num_questions]
                logger.info(f"SUCCESS PDF ê¸°ë°˜ ë¬¸ì œ ìƒì„± ì™„ë£Œ: {len(pdf_questions)}ê°œ")
                return state

            # ğŸ¯ 2ë‹¨ê³„: AI ê¸°ë°˜ ì¶”ê°€ ë¬¸ì œ ìƒì„±
            remaining_count = request.num_questions - len(pdf_questions)
            logger.info(f"PDF ê¸°ë°˜ ë¬¸ì œ ë¶€ì¡±: {remaining_count}ê°œ ì¶”ê°€ ìƒì„± í•„ìš”")

            # AI ê¸°ë°˜ ë¬¸ì œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            ai_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì»¨í…ì¸  ê°œë°œìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì™€ í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ğŸ“š **ê¸°ì¡´ ì»¨í…ì¸  ìš”ì•½**:
{summary}

ğŸ¯ **í•µì‹¬ ì£¼ì œë“¤**:
{chr(10).join(f"- {topic}" for topic in topics)}

ğŸ”‘ **í•µì‹¬ í‚¤ì›Œë“œë“¤**:
{chr(10).join(f"- {keyword}" for keyword in keywords)}

ğŸ“ **ë¬¸ì œ ìƒì„± ì¡°ê±´**:
- ì¶”ê°€ ìƒì„± í•„ìš” ìˆ˜ëŸ‰: {remaining_count}ê°œ
- ë‚œì´ë„: {request.difficulty.value}
- ë¬¸ì œ ìœ í˜•: {request.question_type.value}

ğŸ¯ **ë¬¸ì œ í’ˆì§ˆ ìš”êµ¬ì‚¬í•­**:
1. ê° ë¬¸ì œëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì‹¤ì œ ì‚¬ë¡€ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤
2. ì¤‘ë³µë˜ëŠ” ê°œë…ì˜ ë¬¸ì œëŠ” í”¼í•˜ê³ , ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤
3. ë¬¸ì œëŠ” ì´ë¡ ì  ê°œë…ê³¼ ì‹¤ì œ êµ¬í˜„ì„ ê· í˜•ìˆê²Œ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
4. ê° ë¬¸ì œëŠ” ëª…í™•í•œ í•™ìŠµ ëª©í‘œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
5. ë¬¸ì œì˜ ë‚œì´ë„ëŠ” ì§€ì •ëœ ìˆ˜ì¤€ì— ë§ê²Œ ì¡°ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
6. ì„ íƒì§€ëŠ” ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤
7. ì •ë‹µ í•´ì„¤ì€ ìƒì„¸í•˜ê³  êµìœ¡ì ìœ¼ë¡œ ê°€ì¹˜ìˆì–´ì•¼ í•©ë‹ˆë‹¤

{additional_guide}

**ì¶œë ¥ í˜•ì‹**:
```json
{{
  "questions": [
    {{
      "id": 1,
      "question": "ë¬¸ì œ ë‚´ìš©",
      "type": "{request.question_type.value}",
      "difficulty": "{request.difficulty.value}",
      "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4"],
      "correct_answer": "ì •ë‹µ",
      "explanation": "ì •ë‹µ í•´ì„¤",
      "learning_objective": "í•™ìŠµ ëª©í‘œ",
      "problem_level": "basic ë˜ëŠ” application",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
      "source": "ai_generated",
      "example": "ê´€ë ¨ ì˜ˆì‹œë‚˜ ì‹¤ì œ ì‚¬ë¡€",
      "implementation": "ì‹¤ì œ êµ¬í˜„ ë°©ë²• (í•´ë‹¹ë˜ëŠ” ê²½ìš°)"
    }}
  ]
}}
```

ì •í™•íˆ {remaining_count}ê°œì˜ ê³ í’ˆì§ˆ ì¶”ê°€ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
"""

            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ì „ë¬¸ êµìœ¡ ì»¨í…ì¸  ê°œë°œìì…ë‹ˆë‹¤."),
                HumanMessage(content=ai_prompt)
            ]

            response = await self.llm.ainvoke(messages)
            ai_questions = self._parse_questions(response.content)

            # ğŸ“Š ìµœì¢… ë¬¸ì œ ëª©ë¡ ìƒì„±
            final_questions = pdf_questions + ai_questions[:remaining_count]

            # ğŸ”„ ë¬¸ì œ ìˆœì„œ ì„ê¸°
            import random
            random.shuffle(final_questions)

            state["generated_questions"] = final_questions
            state["current_step"] = "question_generator"

            # ğŸ“Š ë¶„í¬ í™•ì¸ ë¡œê¹…
            basic_count = sum(1 for q in final_questions if q.get("problem_level") == "basic")
            app_count = sum(1 for q in final_questions if q.get("problem_level") == "application")
            pdf_count = sum(1 for q in final_questions if q.get("source") != "ai_generated")
            ai_count = sum(1 for q in final_questions if q.get("source") == "ai_generated")

            logger.info(f"SUCCESS ë¬¸ì œ ìƒì„± ì™„ë£Œ: ì´ {len(final_questions)}ê°œ")
            logger.info(f"- PDF ê¸°ë°˜: {pdf_count}ê°œ")
            logger.info(f"- AI ê¸°ë°˜: {ai_count}ê°œ")
            logger.info(f"- ì¼ë°˜ ë¬¸ì œ: {basic_count}ê°œ")
            logger.info(f"- ì‘ìš© ë¬¸ì œ: {app_count}ê°œ")

            return state

        except Exception as e:
            logger.error(f"ERROR ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
            state["errors"].append(f"ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return state

    def _parse_questions(self, content: str) -> List[Dict]:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            import json

            if "```json" in content:
                json_start = content.find("```json") + 7
                json_end = content.find("```", json_start)
                json_content = content[json_start:json_end].strip()
            elif "```" in content:
                json_start = content.find("```") + 3
                json_end = content.find("```", json_start)
                json_content = content[json_start:json_end].strip()
            else:
                json_content = content.strip()

            questions_data = json.loads(json_content)
            return questions_data.get("questions", [])

        except json.JSONDecodeError as e:
            logger.error(f"ERROR JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.error(f"LLM ì‘ë‹µ ë‚´ìš©: {content[:500]}...")
            return []

    async def _validate_questions(self, state: QuizState) -> QuizState:
        """âœ… 5ë‹¨ê³„: ë¬¸ì œ í’ˆì§ˆ ê²€ì¦"""
        try:
            logger.info("STEP5 ë¬¸ì œ ê²€ì¦ ì‹œì‘")

            questions = state["generated_questions"]
            request = state["request"]

            # ê¸°ë³¸ ê²€ì¦
            validated_questions = []

            for i, q in enumerate(questions):
                if isinstance(q, dict) and "question" in q:
                    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                    if q.get("question") and q.get("correct_answer"):
                        validated_questions.append(q)
                    else:
                        logger.warning(f"WARNING ë¬¸ì œ {i+1} í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                else:
                    logger.warning(f"WARNING ë¬¸ì œ {i+1} í˜•ì‹ ì˜¤ë¥˜")

            # ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸
            state["generated_questions"] = validated_questions
            state["current_step"] = "quality_validator"

            logger.info(f"SUCCESS ë¬¸ì œ ê²€ì¦ ì™„ë£Œ: {len(validated_questions)}ê°œ ë¬¸ì œ í™•ì •")
            return state

        except Exception as e:
            logger.error(f"ERROR ë¬¸ì œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            state["errors"].append(f"ë¬¸ì œ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return state

    async def generate_quiz(self, request: QuizRequest, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ë¬¸ì œ ìƒì„± ë©”ì¸ ë©”ì„œë“œ

        Args:
            request: ë¬¸ì œ ìƒì„± ìš”ì²­
            documents: ëŒ€ìƒ ë¬¸ì„œë“¤

        Returns:
            ìƒì„±ëœ ë¬¸ì œ ë°ì´í„°
        """
        try:
            logger.info("ğŸš€ ë¬¸ì œ ìƒì„± AI ì—ì´ì „íŠ¸ ì‹œì‘")

            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state: QuizState = {
                "request": request,
                "documents": documents,
                "summary": "",
                "core_topics": [],
                "keywords": [],
                "generated_questions": [],
                "current_step": "init",
                "errors": [],
                "domain_context": {}
            }

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)

            # ê²°ê³¼ ì •ë¦¬
            result = {
                "success": True,
                "request": {
                    "file_ids": request.file_ids,
                    "num_questions": request.num_questions,
                    "difficulty": request.difficulty.value,
                    "question_type": request.question_type.value
                },
                "process_info": {
                    "summary": final_state["summary"],
                    "core_topics": final_state["core_topics"],
                    "keywords": final_state["keywords"],
                    "domain_context": final_state["domain_context"]
                },
                "questions": final_state["generated_questions"],
                "meta": {
                    "generated_count": len(final_state["generated_questions"]),
                    "errors": final_state["errors"],
                    "final_step": final_state["current_step"]
                }
            }

            logger.info("ğŸ‰ SUCCESS ë¬¸ì œ ìƒì„± ì™„ë£Œ")
            return result

        except Exception as e:
            logger.error(f"ERROR ë¬¸ì œ ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "request": {
                    "file_ids": request.file_ids,
                    "num_questions": request.num_questions,
                    "difficulty": request.difficulty.value,
                    "question_type": request.question_type.value
                }
            }