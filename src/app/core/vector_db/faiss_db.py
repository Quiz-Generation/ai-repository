"""
ğŸ“Š FAISS Vector Database Implementation (3ìˆœìœ„)
"""
import os
import json
import pickle
import logging
import numpy as np
from typing import List, Dict, Any, Optional
from .base import VectorDatabase, VectorDocument, SearchResult

logger = logging.getLogger(__name__)


class FaissDB(VectorDatabase):
    """FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„ì²´ (3ìˆœìœ„ - ë¡œì»¬)"""

    def __init__(self, db_path: str):
        super().__init__(db_path)
        self.index = None
        self.documents = {}  # id -> VectorDocument ë§¤í•‘
        self.dimension = 384  # sentence-transformers all-MiniLM-L6-v2 ê¸°ë³¸ ì°¨ì›
        self.index_path = os.path.join(db_path, "faiss.index")
        self.metadata_path = os.path.join(db_path, "metadata.json")
        self.documents_path = os.path.join(db_path, "documents.pkl")

    async def initialize(self) -> None:
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        try:
            import faiss

            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.db_path, exist_ok=True)

            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
            if os.path.exists(self.index_path) and os.path.exists(self.documents_path):
                logger.info("STEP_VECTOR ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
                self.index = faiss.read_index(self.index_path)

                with open(self.documents_path, 'rb') as f:
                    self.documents = pickle.load(f)

                logger.info(f"STEP_VECTOR FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
            else:
                # ìƒˆ ì¸ë±ìŠ¤ ìƒì„± (L2 ê±°ë¦¬ ê¸°ì¤€ Flat ì¸ë±ìŠ¤)
                logger.info("STEP_VECTOR ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„±")
                self.index = faiss.IndexFlatL2(self.dimension)

            logger.info("SUCCESS FAISS ì´ˆê¸°í™” ì™„ë£Œ")

        except ImportError:
            logger.error("ERROR FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install faiss-cpu' ì‹¤í–‰í•˜ì„¸ìš”.")
            raise ImportError("FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        except Exception as e:
            logger.error(f"ERROR FAISS ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def add_documents(self, documents: List[VectorDocument]) -> List[str]:
        """ë¬¸ì„œë“¤ì„ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€"""
        try:
            if not self.index:
                await self.initialize()

            import faiss

            # ì„ë² ë”© ë²¡í„° ì¤€ë¹„
            embeddings = []
            doc_ids = []

            for doc in documents:
                embeddings.append(doc.embedding)
                doc_ids.append(doc.id)
                self.documents[doc.id] = doc

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(embeddings, dtype=np.float32)

            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings_array)

            # ë””ìŠ¤í¬ì— ì €ì¥
            await self._save_to_disk()

            logger.info(f"SUCCESS FAISSì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            return doc_ids

        except Exception as e:
            logger.error(f"ERROR FAISS ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise

    async def search(
        self,
        query_embedding: List[float],
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """FAISSì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            if not self.index or self.index.ntotal == 0:
                logger.warning("WARNING FAISS ì¸ë±ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return []

            # ì¿¼ë¦¬ ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            query_array = np.array([query_embedding], dtype=np.float32)

            # FAISS ê²€ìƒ‰ ìˆ˜í–‰
            distances, indices = self.index.search(query_array, min(top_k, self.index.ntotal))

            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx >= 0:  # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ ê²½ìš°
                    # ë¬¸ì„œ ID ì°¾ê¸° (ìˆœì„œëŒ€ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
                    doc_id = list(self.documents.keys())[idx]
                    doc = self.documents[doc_id]

                    # í•„í„° ì ìš©
                    if filters:
                        if not self._apply_filters(doc, filters):
                            continue

                    # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ê±°ë¦¬ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜)
                    score = 1.0 / (1.0 + distance)

                    results.append(SearchResult(
                        document=doc,
                        score=score,
                        distance=float(distance)
                    ))

            logger.info(f"SUCCESS FAISS ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results

        except Exception as e:
            logger.error(f"ERROR FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def delete_document(self, document_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ (FAISSëŠ” ì§ì ‘ ì‚­ì œ ë¶ˆê°€ëŠ¥, ì¬êµ¬ì„± í•„ìš”)"""
        try:
            if document_id not in self.documents:
                logger.warning(f"WARNING ë¬¸ì„œ {document_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return False

            # ë¬¸ì„œ ì œê±°
            del self.documents[document_id]

            # ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            await self._rebuild_index()

            logger.info(f"SUCCESS ë¬¸ì„œ {document_id} ì‚­ì œ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ERROR ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False

    async def get_document_count(self) -> int:
        """ì´ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ"""
        return len(self.documents)

    async def get_all_documents(self, limit: Optional[int] = None) -> List[VectorDocument]:
        """ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ"""
        try:
            all_docs = list(self.documents.values())

            if limit:
                all_docs = all_docs[:limit]

            logger.info(f"SUCCESS FAISSì—ì„œ {len(all_docs)}ê°œ ë¬¸ì„œ ì¡°íšŒ ì™„ë£Œ")
            return all_docs

        except Exception as e:
            logger.error(f"ERROR FAISS ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    async def health_check(self) -> Dict[str, Any]:
        """FAISS í—¬ìŠ¤ì²´í¬"""
        try:
            import faiss

            return {
                "status": "healthy",
                "db_type": "faiss",
                "priority": 3,
                "document_count": len(self.documents),
                "index_size": self.index.ntotal if self.index else 0,
                "dimension": self.dimension,
                "library_available": True,
                "index_type": "IndexFlatL2",
                "storage": "local_disk"
            }

        except ImportError:
            return {
                "status": "unhealthy",
                "db_type": "faiss",
                "error": "FAISS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ",
                "library_available": False
            }
        except Exception as e:
            logger.error(f"ERROR FAISS í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "status": "unhealthy",
                "db_type": "faiss",
                "error": str(e),
                "library_available": False
            }

    async def _save_to_disk(self) -> None:
        """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            import faiss

            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, self.index_path)

            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.documents_path, 'wb') as f:
                pickle.dump(self.documents, f)

            # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
            metadata = {
                "document_count": len(self.documents),
                "dimension": self.dimension,
                "index_type": "IndexFlatL2"
            }

            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"ERROR FAISS ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    async def _rebuild_index(self) -> None:
        """ì¸ë±ìŠ¤ ì¬êµ¬ì„± (ì‚­ì œ í›„)"""
        try:
            import faiss

            if not self.documents:
                # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
                self.index = faiss.IndexFlatL2(self.dimension)
            else:
                # ë‚¨ì€ ë¬¸ì„œë“¤ë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì„±
                embeddings = [doc.embedding for doc in self.documents.values()]
                embeddings_array = np.array(embeddings, dtype=np.float32)

                self.index = faiss.IndexFlatL2(self.dimension)
                self.index.add(embeddings_array)

            await self._save_to_disk()

        except Exception as e:
            logger.error(f"ERROR FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
            raise

    def _apply_filters(self, doc: VectorDocument, filters: Dict[str, Any]) -> bool:
        """í•„í„° ì¡°ê±´ í™•ì¸"""
        for key, value in filters.items():
            if key not in doc.metadata:
                return False
            if doc.metadata[key] != value:
                return False
        return True