#!/usr/bin/env python3
"""
ğŸš€ M1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import asyncio
import time
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_m1_performance():
    """M1 MPS ê°€ì† ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    print("ğŸ M1 ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    # 1. PyTorch MPS í™•ì¸
    try:
        import torch
        print(f"âœ… PyTorch ë²„ì „: {torch.__version__}")
        print(f"âœ… MPS ì‚¬ìš© ê°€ëŠ¥: {torch.backends.mps.is_available()}")

        if torch.backends.mps.is_available():
            print("ğŸ‰ M1 MPS ê°€ì† í™œì„±í™”ë¨!")
        else:
            print("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€ëŠ¥ - CPU ì‚¬ìš©")

    except ImportError:
        print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return

    # 2. SentenceTransformer í…ŒìŠ¤íŠ¸
    try:
        from sentence_transformers import SentenceTransformer

        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
        test_texts = [
            "ì´ê²ƒì€ ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
            "ë‘ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
            "ì„¸ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
            "ë„¤ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
            "ë‹¤ì„¯ ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
        ] * 20  # 100ê°œ ë¬¸ì¥ìœ¼ë¡œ í™•ì¥

        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {len(test_texts)}ê°œ ë¬¸ì¥")

        # CPU vs MPS ì„±ëŠ¥ ë¹„êµ
        for device_name in ["cpu", "mps"]:
            if device_name == "mps" and not torch.backends.mps.is_available():
                continue

            print(f"\nğŸ”§ {device_name.upper()} í…ŒìŠ¤íŠ¸ ì‹œì‘...")

            # ëª¨ë¸ ë¡œë“œ
            start_time = time.time()
            model = SentenceTransformer('all-MiniLM-L6-v2', device=device_name)
            load_time = time.time() - start_time
            print(f"   ëª¨ë¸ ë¡œë“œ ì‹œê°„: {load_time:.2f}ì´ˆ")

            # ì„ë² ë”© ìƒì„±
            start_time = time.time()
            embeddings = model.encode(test_texts, show_progress_bar=False)
            encode_time = time.time() - start_time
            print(f"   ì„ë² ë”© ìƒì„± ì‹œê°„: {encode_time:.2f}ì´ˆ")
            print(f"   ì²˜ë¦¬ ì†ë„: {len(test_texts)/encode_time:.1f} ë¬¸ì¥/ì´ˆ")

            # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            print(f"   ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
            batch_size = 32 if device_name == "mps" else 8
            start_time = time.time()

            all_embeddings = []
            for i in range(0, len(test_texts), batch_size):
                batch = test_texts[i:i + batch_size]
                batch_embeddings = model.encode(batch, show_progress_bar=False)
                all_embeddings.extend(batch_embeddings)

            batch_time = time.time() - start_time
            print(f"   ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {batch_time:.2f}ì´ˆ")
            print(f"   ë°°ì¹˜ ì²˜ë¦¬ ì†ë„: {len(test_texts)/batch_time:.1f} ë¬¸ì¥/ì´ˆ")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            if device_name == "mps":
                torch.mps.empty_cache()
            elif device_name == "cuda":
                torch.cuda.empty_cache()

    except ImportError:
        print("âŒ sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return

    print("\n" + "=" * 50)
    print("ğŸ¯ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ’¡ ê²°ê³¼ í•´ì„:")
    print("- MPSê°€ CPUë³´ë‹¤ ë¹ ë¥´ë©´ ì„±ëŠ¥ ê°œì„  ì„±ê³µ")
    print("- ë°°ì¹˜ ì²˜ë¦¬ê°€ ë‹¨ì¼ ì²˜ë¦¬ë³´ë‹¤ íš¨ìœ¨ì ì´ë©´ ìµœì í™” ì„±ê³µ")

if __name__ == "__main__":
    asyncio.run(test_m1_performance())